{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Welcome to the Hoplab Wiki","text":"<p>Welcome to the Hoplab Wiki repository. This Wiki is a work in progress and an ongoing effort to migrate all the Hoplab wisdom into a more user-friendly format. This process is currently managed by @costantinoai and @kschevenels. For any questions, feel free to ping me.</p> <p>The Hoplab is part of the larger NeuroSPACE consortium funded by the Methusalem Program that in addition to Hoplab also includes the teams of C\u00e9line Gillebert, Kobe Desender, and Bert De Smedt. Much of the research information on the Hoplab wiki will also be very useful to members of these teams. Likewise, where appropriate, this Hoplab wiki contains links to online information provided by these other teams.</p> <p>Leave a feedback</p> <p>The wiki is our collective knowledge base - your feedback and contributions help keep it accurate, complete, and useful for everyone in the lab.</p> <p>For small updates to the Wiki, click on the  icon at the top right of each page. Use the feedback buttons ( ) at the bottom of each page to help improve the wiki. </p> <p>Use the search bar</p> <p>The search bar is your friend! Just type in a keyword and you will be presented with a list of relevant pages.</p> <ul> <li> <p> Research Tools &amp; Methods</p> <p>Access guides and procedures for conducting research at Hoplab</p> <p> Explore methods</p> </li> <li> <p> Get Started</p> <p>Essential information for new members to hit the ground running</p> <p> Start here</p> </li> <li> <p> Contribute</p> <p>Learn how you can help improve and expand our knowledge base</p> <p> How to contribute</p> </li> </ul>"},{"location":"index.html#research-tools-methods","title":"Research Tools &amp; Methods","text":"<ul> <li> <p> Behaviour</p> <p>Methodologies for behavioral experiments</p> <p> Learn more</p> </li> <li> <p> Coding</p> <p>Best practices and resources for research coding</p> <p> Learn more</p> </li> <li> <p> Deep Neural Networks</p> <p>Guides for implementing and using DNNs in research</p> <p> Learn more</p> </li> <li> <p> EEG</p> <p>Procedures for EEG data collection and analysis</p> <p> Learn more</p> </li> <li> <p> fMRI</p> <p>Guidelines for fMRI studies and data analysis</p> <p> Learn more</p> </li> </ul>"},{"location":"index.html#essential-resources","title":"Essential Resources","text":"<ul> <li> <p> Student Starter Pack</p> <p>Essential information for new students</p> <p> Get started</p> </li> <li> <p> Computer Setup</p> <p>Set up your work environment</p> <p> Set up</p> </li> <li> <p> Practical Setup</p> <p>Day-to-day operational guidelines</p> <p> Learn more</p> </li> <li> <p> Mailing Lists</p> <p>Stay informed with our mailing lists</p> <p> Subscribe</p> </li> <li> <p> Research Ethics</p> <p>Ethical guidelines for research</p> <p> Learn more</p> </li> <li> <p> Outreach</p> <p>Engage with the wider community</p> <p> Explore</p> </li> </ul>"},{"location":"index.html#quick-links","title":"Quick Links","text":"<ul> <li> Lab Calendar (Requires permission)</li> <li> Hoplab Teams Directory (Requires permission)</li> <li> Hoplab Website</li> <li> Hoplab Publications</li> </ul>"},{"location":"index.html#need-help","title":"Need Help?","text":"<p>Can't find what you're looking for? Have questions or suggestions? Don't hesitate to reach out: </p> <ul> <li>Wiki Managers: @costantinoai and @kschevenels</li> <li>Contact: Andrea Costantino</li> </ul>"},{"location":"contribute.html","title":"Contribute to the Hoplab Wiki","text":"<p>Welcome to the Hoplab Wiki repository. This Wiki is a work in progress and an ongoing effort to migrate all the Hoplab knowledge and procedures into a more user-friendly format. This process is currently managed by @costantinoai and @kschevenels. For any questions, feel free to ping me. </p> <p>This guide will help you set up, update, and maintain the Wiki both locally and online. Follow the instructions if you want to make changes to the wiki.</p> <p>Suggest or make quick changes</p> <p>For most cases, if you want to suggest some changes you can do so by opening a new Issue. If you want to make quick changes to any page, you can do so by clicking on the pencil icon (\u270f\ufe0f) at the top right of the page to begin editing the file.</p>"},{"location":"contribute.html#table-of-contents","title":"Table of Contents","text":"<ol> <li>Getting Started</li> <li>Editing the Wiki<ul> <li>Adding a New Page</li> <li>Creating Child Pages</li> <li>Adding Tags</li> <li>Common Formatting Syntax</li> <li>Linking and Referencing</li> </ul> </li> <li>How to Contribute<ul> <li>Easy Workflow (Quick Changes)</li> <li>Advanced Workflow (Extensive Changes)</li> </ul> </li> <li>Reviewing and Accepting Pull Requests (for Admins)</li> <li>Troubleshooting</li> </ol>"},{"location":"contribute.html#getting-started","title":"Getting Started","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>A GitHub account (click here to sign up).</li> <li>Be part of the <code>HOPLAB-LBP</code> organization (contact Andrea if you need to be added).</li> <li>If you plan on following the Advanced Workflow (encouraged for more complex changes), also make sure that you have Conda or Miniconda, and git or GitHub Desktop (strongly encouraged) installed.</li> </ul>"},{"location":"contribute.html#editing-the-wiki","title":"Editing the Wiki","text":"<p>We welcome contributions from all members. All the content of the wiki is written in Markdown files located in the <code>docs</code> directory. You can edit these files in your browser (if you follow the Easy Workflow) or locally using any text editor or IDE (e.g., VSCode, Sublime Text) if you follow the Advanced Workflow.</p>"},{"location":"contribute.html#adding-a-new-page","title":"Adding a New Page","text":"<ol> <li>Create a new Markdown file in the <code>docs</code> directory (e.g., <code>docs/new-page.md</code>).</li> <li>Add the new page to the <code>nav</code> section of <code>mkdocs.yml</code>:  <pre><code>nav:\n  - Home: index.md\n  - Guide: guide.md\n  - New Page: new-page.md\n</code></pre></li> </ol>"},{"location":"contribute.html#creating-child-pages","title":"Creating child pages","text":"<p>To create a child page, place the Markdown file in a subdirectory and update the <code>nav</code> section in <code>mkdocs.yml</code> accordingly:</p> <ol> <li>Create a new subdirectory in the <code>docs</code> directory (e.g., <code>docs/subdir</code>).</li> <li>Create a new Markdown file in the subdirectory (e.g., <code>docs/subdir/child-page.md</code>).</li> <li>Update the <code>nav</code> section in <code>mkdocs.yml</code>:  <pre><code>nav:\n  - Home: index.md\n  - Guide: guide.md\n  - Subdir:\n      - Child Page: subdir/child-page.md\n</code></pre></li> </ol>"},{"location":"contribute.html#adding-note-todo-and-placeholder-tags","title":"Adding <code>NOTE</code>, <code>TODO</code>, and <code>PLACEHOLDER</code> tags","text":"<p>For ease of collaboration, we keep track of all the tasks in our documentation in the Issues page. Tasks are organized by file, and each file with tags will automatically be listed as an Issue.</p> <p>To add a new task to this list, write <code>NOTE</code>, <code>TODO</code>, or <code>PLACEHOLDER</code> in any document in the <code>docs/</code> folder. This will be added to the Issue for that page during the workflow run. It is a good practice to write the name of the author in square brackets. Example:</p> <p><code>TODO: [Andrea] fix hyperlinks</code></p> <p>Please remember to delete the source tag from the original file once the task is resolved. This ensures that the Issue page includes only unresolved tasks.</p>"},{"location":"contribute.html#how-task-counters-in-titles-work","title":"How task counters in titles work","text":"<ul> <li>Each Issue aggregates tasks for a single file and shows a counter in the title: <code>(open/total open) Tags in &lt;file&gt;</code>.</li> <li>Example: <code>(3/12 open) Tags in docs/get-started/index.md</code>.</li> <li>Tasks created from file tags are marked \u201cAdded from file\u201d. If a tag is removed from the file, the corresponding task is automatically marked complete as \u201cResolved from file\u201d.</li> <li>Tasks added via comments on the Issue body are preserved and counted; the title counter is updated automatically.</li> </ul>"},{"location":"contribute.html#common-formatting-syntax","title":"Common formatting syntax","text":"<p>Here are some common Markdown elements:</p> <ul> <li>Headers: <code># Header 1</code>, <code>## Header 2</code>, <code>### Header 3</code>, etc.</li> <li>Bold text: <code>**bold text**</code></li> <li>Italic text: <code>*italic text*</code></li> <li>Links: <code>[link text](URL)</code></li> <li>Lists: <ul> <li>Unordered list: <code>- Item 1</code></li> <li>Ordered list: <code>1. Item 1</code></li> </ul> </li> <li>Images: <code>![Alt text](path/to/image.png)</code> (see this section for instructions on how to link images.)</li> </ul> <p>For more advanced formatting options, refer to the MkDocs Material Reference Guide.</p>"},{"location":"contribute.html#linking-and-referencing","title":"Linking and Referencing","text":"<p>When creating or editing content, you may want to reference or link to other sections within the wiki, external resources, or images. Here's how to do it:</p>"},{"location":"contribute.html#internal-links-within-the-wiki","title":"Internal Links (Within the Wiki)","text":"<p>Use relative paths for internal links. The general format is:</p> <pre><code>[Link Text](path/to/file.md)\n</code></pre> <p>Examples:</p> <ol> <li> <p>Linking to a page in the same directory:    <pre><code>[Getting Started](getting-started.md)\n</code></pre></p> </li> <li> <p>Linking to a page in a subdirectory:    <pre><code>[fMRI Analysis](research/fmri/fmri-analysis.md)\n</code></pre></p> </li> <li> <p>Linking to a specific section on another page:    <pre><code>[Ethics Guidelines](research/ethics/index.md#ethical-guidelines)\n</code></pre></p> </li> <li> <p>Linking to a parent directory:    <pre><code>[Back to Research](../index.md)\n</code></pre></p> </li> </ol>"},{"location":"contribute.html#external-links-outside-the-wiki","title":"External Links (Outside the Wiki)","text":"<p>For external links, use the full URL:</p> <pre><code>[Hoplab Website](https://www.hoplab.be/)\n</code></pre>"},{"location":"contribute.html#adding-and-linking-images","title":"Adding and Linking Images","text":"<p>When adding images to the Wiki:</p> <ol> <li>Store all images in the <code>docs/assets</code> folder.</li> <li>Use descriptive, lowercase names for images, separating words with hyphens (e.g., <code>fmri-analysis-workflow.png</code>).</li> <li> <p>Use relative links to reference images. The path depends on the location of your Markdown file:</p> <ul> <li> <p>If your Markdown file is in the main <code>docs</code> folder:  <pre><code>![fMRI Analysis Workflow](../assets/fmri-analysis-workflow.png)\n</code></pre></p> </li> <li> <p>If your file is in a subdirectory of <code>docs</code> (e.g., <code>docs/research/</code>):  <pre><code>![fMRI Analysis Workflow](../../assets/fmri-analysis-workflow.png)\n</code></pre></p> </li> <li> <p>If your file is in a sub-subdirectory (e.g., <code>docs/research/fmri/</code>):  <pre><code>![fMRI Analysis Workflow](../../../assets/fmri-analysis-workflow.png)\n</code></pre></p> </li> </ul> </li> <li> <p>Always include descriptive alt text for accessibility:    <pre><code>![Diagram showing steps of fMRI analysis](../assets/fmri-analysis-workflow.png)\n</code></pre></p> </li> <li> <p>Optionally, specify image dimensions using HTML:    <pre><code>&lt;img src=\"../assets/fmri-analysis-workflow.png\" alt=\"fMRI Analysis Workflow\" width=\"500\"&gt;\n</code></pre></p> </li> </ol>"},{"location":"contribute.html#best-practices-for-linking","title":"Best Practices for Linking","text":"<ol> <li>Use descriptive link text that gives users an idea of where the link will take them.</li> <li>Check your links after creating them to ensure they work correctly.</li> <li>For external links, consider opening them in a new tab:    <pre><code>[Hoplab Website](https://www.hoplab.be/){target=\"_blank\"}\n</code></pre></li> <li>When linking to specific sections within long documents, use anchor links to improve user experience.</li> <li>For images, always use relative links and store images in the <code>docs/assets</code> folder to maintain a self-contained Wiki.</li> </ol>"},{"location":"contribute.html#how-to-contribute","title":"How to Contribute","text":"<p>We offer two workflows for contributing to the Hoplab Wiki: an Easy Workflow for quick changes to single files, and an Advanced Workflow for more extensive changes to multiple files.</p>"},{"location":"contribute.html#easy-workflow-for-quick-changes","title":"Easy Workflow (for Quick Changes)","text":"<p>This workflow is ideal for making small, quick changes to a single file. It can be done entirely through your web browser and doesn't require any local setup.</p> Edit directly from this page! <p>Existing pages can be edited directly through the Wiki! If you need to edit or add information to any page, look for the paper and pencil symbol  at the top-right of the page, next to the page title. This will let you edit the page and open a PR either by creating a new branch on the main repo (if you are part of the Hoplab organization on GitHub) or by forking your own copy of the repo (if you are an external contributor). Make sure to submit a PR after your changes are made.</p>"},{"location":"contribute.html#step-1-make-your-changes","title":"Step 1: Make your changes","text":"<ol> <li> <p>To edit an existing page:</p> <ol> <li>Navigate to the <code>HOPLAB-LBP/hoplab-wiki</code> repository.</li> <li>Click on the file you want to edit (usually, in <code>docs/</code>).</li> <li>Click on the pencil icon (\u270f\ufe0f) at the top right to edit the file.</li> </ol> </li> <li> <p>To create a new page: </p> <ol> <li>Navigate to the <code>mkdocs.yml</code> file.</li> <li>Click on the pencil icon (\u270f\ufe0f) at the top right to edit the file.</li> <li>Add the new page (e.g., <code>docs/new-page.md</code>) to the <code>nav</code> section and commit (follow the steps in the section 2 below).</li> <li>In the <code>docs</code> folder, click on \"Add file\" &gt; \"Create new file\".</li> <li>Enter a name for your file in the <code>docs</code> directory (the same you used before, e.g., <code>docs/new-page.md</code>).</li> </ol> </li> </ol> <p>You can then add/edit your content in Markdown format (see Editing the Wiki for more info), and click on \"Preview\" next to the \"Edit\" tab to see how your changes will look like.</p>"},{"location":"contribute.html#step-2-commit-changes-to-a-temporary-branch","title":"Step 2: Commit changes to a temporary branch","text":"<ol> <li>Click on \"Commit changes\" after any necessary adjustments.</li> <li>In the pop-up window, add a commit message and description for your changes.</li> <li>Select \"Create a new branch for this commit and start a pull request\".</li> <li>Click on \"Propose changes\".</li> </ol>"},{"location":"contribute.html#step-3-submit-a-pr-with-your-proposed-changes","title":"Step 3: Submit a PR with your proposed changes","text":"<ol> <li>In the \"Open a pull request page\", add an informative title and a description of the changes in the PR.</li> <li>In the right panel, make sure to assign an admin (as of July 2024, @costantinoai) to review your changes.</li> <li>Click on \"Create pull request\" to submit your changes.</li> </ol> Add multiple commits to a single PR <p>If you want to make additional changes related to an already opened PR (e.g., you need to change info in two separate files, or make additional adjustments), you do not need to open a new PR. Just go to the main page of the branch you created (you can find the branch in the branches list) and keep editing your files in this branch. Every new commit you make in this branch will have the option to \"Commit directly to the  branch\" or \"Create a new branch for this commit and start a pull request\". Make sure you select the first option to include your new commits to the original PR. Importantly, if you plan to add several commits to a PR this way, make sure you assign a reviewer only after your last commit to avoid merging PRs halfway in the process, or you can create a draft PR until all your changes are included. <p>These steps above will create a new branch in the repository, that will be visible in the branches list, and a new PR visible in the PRs list. Once the PR is approved by at least one reviewer and merged into the main branch, the newly created branch will be automatically deleted and the changes will go live.</p>"},{"location":"contribute.html#advanced-workflow-for-extensive-changes","title":"Advanced Workflow (for Extensive Changes)","text":"<p>The preferred way to contribute if you need to make significant/multiple changes, but it requires some familiarity with git, Python, and Conda environments. If you are not a Wiki maintainer, this workflow is probably overkill. </p> <p>With this workflow, you will make and preview all the edits locally (on your computer). This allows for more control and flexibility, as it lets you see your changes in a live session. </p> <p>How should I organize my PR?</p> <p>A Pull Request (or PR) \"is a proposal to merge a set of changes from one branch into another\". Ideally, a PR should include all the commits for a specific feature or bugfix from end-to-end. Avoid making PRs that contain multiple unrelated changes. For instance, if you are working on a feature that requires modifications across multiple files, ensure all those changes are included in the same PR. Conversely, avoid combining changes for different features (e.g., adding unrelated updates to the fMRI workflow and the getting started section) in a single PR. Each PR should represent a cohesive unit of work.</p> <p>Here's a step-by-step guide that includes forking and cloning the repository, making and testing changes locally, and then submitting those changes for review through a pull request.</p>"},{"location":"contribute.html#step-1-forking-the-repository-and-cloning-your-fork","title":"Step 1: Forking the Repository and Cloning Your Fork","text":"Using the CLIUsing GitHub Desktop <ol> <li> <p>Navigate to the original repository:</p> <p>Open your web browser and go to the GitHub page for the <code>hoplab-wiki</code> repository located under the <code>HOPLAB-LBP</code> organization.</p> </li> <li> <p>Fork the repository:</p> <p>Click the \"Fork\" button at the top right corner of the repository page. This will create a copy of the repository under your GitHub account.</p> </li> <li> <p>Clone Your Fork:</p> <ol> <li>Click the \"Code\" button on your forked repository page and copy the URL.</li> <li>Open your terminal (Command Prompt on Windows, Terminal on macOS and Linux) and navigate to the directory where you want to store the project, then type:    <pre><code>git clone https://github.com/your-username/hoplab-wiki.git\n</code></pre></li> <li>Change into the directory of the cloned repository:    <pre><code>cd hoplab-wiki\n</code></pre></li> </ol> </li> </ol> <ol> <li> <p>Navigate to the Original Repository:</p> <p>Open your web browser and go to the GitHub page for the <code>hoplab-wiki</code> repository located under the <code>HOPLAB-LBP</code> organization.</p> </li> <li> <p>Fork the Repository:</p> <p>Click the \"Fork\" button at the top right corner of the repository page. This will create a copy of the repository under your GitHub account.</p> </li> <li> <p>Open GitHub Desktop:</p> <p>If you do not have GitHub Desktop installed, download and install it from GitHub Desktop's official website.</p> </li> <li> <p>Clone your fork using GitHub Desktop:</p> <ol> <li>Open GitHub Desktop.</li> <li>In the top menu, click on <code>File &gt; Clone Repository</code>.</li> <li>In the \"URL\" tab, paste the URL of your forked repository from your GitHub account into the \"Repository URL\" field.</li> <li>Choose the local path where you want to store the repository on your computer.</li> <li>Click \"Clone\".</li> </ol> </li> </ol>"},{"location":"contribute.html#step-2-setting-up-your-local-environment","title":"Step 2: Setting up your local environment","text":"<ol> <li> <p>Install Conda:</p> <p>If you don't have Conda installed, download and install it from Conda's official website.</p> </li> <li> <p>Create and activate a Conda environment:</p> <pre><code>conda create --name hoplab-wiki python=3.9\nconda activate hoplab-wiki\n</code></pre> </li> <li> <p>Install necessary packages:</p> <pre><code>pip install mkdocs mkdocs-material mkdocs-task-collector mkdocs-git-revision-date-localized-plugin mkdocs-git-authors-plugin\n</code></pre> </li> </ol>"},{"location":"contribute.html#step-3-making-changes","title":"Step 3: Making changes","text":"<ol> <li>Edit documentation:      You can now make changes to your local clone of the documentation. Use a text editor or an IDE to open and edit the Markdown files in the repository. If changes are extensive, consider splitting them into smaller, manageable commits that focus on specific pages or sections for clarity and ease of review.</li> </ol>"},{"location":"contribute.html#step-4-testing-your-changes-locally","title":"Step 4: Testing your changes locally","text":"<ol> <li>Serve the documentation locally:</li> <li>While in your project directory and with the Conda environment activated, launch the local server by typing:       <pre><code>mkdocs serve\n</code></pre></li> <li>Open a web browser and navigate to <code>http://127.0.0.1:8000/</code>. This allows you to see your changes as they would appear on the live site.</li> <li>Keep this server running as you make changes; refresh your browser to update the preview.</li> </ol>"},{"location":"contribute.html#step-5-closing-the-local-server","title":"Step 5: Closing the local server","text":"<ol> <li>Stop the server:     When you are done previewing and editing and you are done with the changes, go back to the terminal where your server is running and press <code>Ctrl+C</code> to stop the server.</li> </ol>"},{"location":"contribute.html#step-6-committing-your-changes","title":"Step 6: Committing Your Changes","text":"Using the CLIUsing GitHub Desktop <ol> <li>Stage and commit your changes:<ol> <li>From your terminal, add all modified files to your commit:   <pre><code>git add .\n</code></pre></li> <li>Commit the changes, including a clear message about what was modified and why:   <pre><code>git commit -m \"Detailed description of changes\"\n</code></pre></li> </ol> </li> <li>Push your commits to the forked repository on GitHub:       <pre><code>git push origin main\n</code></pre></li> </ol> <ol> <li> <p>Stage and commit your changes:</p> <ol> <li>In GitHub Desktop, you should see the list of changed files in the left sidebar.</li> <li>Review the changes by clicking on each file.</li> <li>Once you are ready to commit, write a summary of the changes in the \"Summary\" field at the bottom left.</li> <li>Add a more detailed description in the \"Description\" field if necessary.</li> <li>Click the \"Commit to main\" button.</li> </ol> </li> <li> <p>Push your changes:</p> <ol> <li>In GitHub Desktop, click on the <code>Push origin</code> button at the top to push your commits to GitHub.</li> </ol> </li> </ol>"},{"location":"contribute.html#step-7-creating-a-pull-request","title":"Step 7: Creating a Pull Request","text":"<ol> <li>Navigate to your forked repository on GitHub.</li> <li>Click on the \"Pull requests\" tab.</li> <li>Click on \"New pull request\".</li> <li>Choose the original repository's <code>main</code> branch as the base, and your fork's <code>main</code> branch as the compare.</li> <li>Fill out the form to describe the changes.</li> <li>In the right panel, make sure to assign an admin (as of July 2024, @costantinoai) to review your changes.</li> <li>Click on \"Create pull request\" to submit your changes.   </li> </ol> <p>Automatic Deployment with GitHub Actions</p> <p>This repository is set up to use GitHub Actions for automatic deployment. This means that every time changes are merged into the <code>main</code> branch, the documentation will automatically be built and deployed to GitHub Pages. You do not need to manually run the <code>mkdocs gh-deploy</code> command each time you make changes. Simply push your changes to the <code>main</code> branch, and GitHub Actions will handle the deployment.</p>"},{"location":"contribute.html#reviewing-and-accepting-pull-requests-for-admins","title":"Reviewing and Accepting Pull Requests (for Admins)","text":"<ol> <li>Go to the <code>hoplab-wiki</code> repository on GitHub.</li> <li>Click on the \"Pull requests\" tab.</li> <li>Review the pull request (Approve changes or suggest edits)</li> <li>When the changes are satisfactory, approve the changes and click \"Merge pull request\". This will delete the temporary branch.</li> </ol>"},{"location":"contribute.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"contribute.html#permission-denied-or-authentication-issues","title":"Permission Denied or Authentication Issues","text":"<p>If you encounter issues with pushing to the repository, you may need to use a personal access token. Follow these steps:</p> <ol> <li>Create a fine-grained personal access token here.</li> <li>Use this token for authentication when pushing to the repository. More information on how to do so can be found on this issue. Please, first get a fine-grained Personal Access Token as described in the link above, and then follow the instructions in the \"Manually temporary resolving this issue for a single git repository\" section.</li> </ol>"},{"location":"contribute.html#additional-help","title":"Additional Help","text":"<p>For further assistance, refer to the following resources:</p> <ul> <li>MkDocs Documentation</li> <li>MkDocs Material Theme</li> <li>GitHub Pages Documentation</li> </ul> <p>Or ping me.</p> <p>Thank you for contributing to the Hoplab Wiki!</p>"},{"location":"outreach.html","title":"Do great science and show it to the world","text":"<p>(We'll start small and go bigger)</p>"},{"location":"outreach.html#lab-meetings","title":"Lab meetings","text":""},{"location":"outreach.html#types-of-lab-meetings","title":"Types of lab meetings","text":"<p>There are different types of lab meetings that happen throughout the academic year.</p> Individual meetingsQuick updatesConference updatesNew papers <p>On a rotation basis, each lab member is assigned with a slot and is in charge of the presentation. On this occasion, you can present an update on your research, an interesting / relevant paper for a Journal Club (JC), or other important topics (practicing an upcoming talk or interview, for example).</p> <p>Once per month, lab memebers assigned to a specific batch will give a quick (1-2 slides, 15 minutes) update on the latest development in their work. This meeting is an occasion to ask for help if you are stuck on a particular step, method, analysis, or to just keep each other in the loop. If your individual meeting is scheduled for the week before or the week after your batch, you can skip the quick update for the longer one.</p> <p>This type of lab meeting is usually scheduled after a lab member is back from a conference / workshop. The idea is for that person (or those people) to give an overview of the conference and present studies of interest for the lab. This type of meeting does not have a regular frequency.</p> <p>Once every two months, we meet to discuss new papers that came out recently. This is an occasion to discuss new topics, new methods, or just interesting work that you might have seen around. There is no fixed presenter, we will go over the list of papers one by one and the person who added it will introduce it. At any time, if you see an intersting paper you can add it to the reading list.</p>"},{"location":"outreach.html#schedule","title":"Schedule","text":"<p>Lab meetings usually happen once per week on Mondays, after lunch. On that day, we have lunch together in the meeting room and then move to the more science-y affairs. The meeting room will be specified in a reminder email that is sent every Thursday before the lab meeting and can be seen in the meeting schedule. The schedule is made at the beginning of the academic year and consists of a mix of meetings with updates and others with more fixed topics.</p>"},{"location":"outreach.html#attendance","title":"Attendance","text":"<p>You can attend the meetings in person or online, there will be always a link to a Teams call. If you will be online, or absent, let the lab meeting manager know (as of August 2024, Filippo Cerpelloni).</p>"},{"location":"outreach.html#who-is-presenting","title":"Who is presenting","text":"<p>Each lab member is part of the standard rotation of presenters for the individual meetings. Additionally, they are assigned to one of the batches that will alternate in the quick updates. Interns are usually not part of the standard roations, but will be assigned to a batch.</p> <p>The rotation of presenters is made by randomizing the order of the names, but can be adjusted based on anyone's availability. If you want / need to switch your slot, you can directly contact the person you would like to switch with and then let the lab meeting manager know about the change. If you would like to change with a slot assigned for a group meeting, ask the lab meeting manager directly.  </p>"},{"location":"outreach.html#presentation-guidelines","title":"Presentation Guidelines","text":"<p>To keep our lab meeting presentations organized, please follow these guidelines:</p> <p>Naming Convention for Individual Lab Meetings</p> <p>Use the format: <code>YYYY-MM-DD_presenter_topic.pdf</code></p> <p>Example: <code>2024-08-24_andrea_dnns-in-vision.pdf</code></p>"},{"location":"outreach.html#storage-instructions","title":"Storage Instructions","text":"1-Slide UpdatesIndividual MeetingsNew Papers Meetings <ul> <li> <p>Before the meeting:   Store your slides in:   <pre><code>Hoplab &gt; Science outreach &gt; Lab meetings &gt; 1-slide updates\n</code></pre>   This allows all updates to be presented from one computer.</p> </li> <li> <p>After the meeting:   Move your presentation to:   <pre><code>Hoplab &gt; Science outreach &gt; Lab meetings &gt; Past LM presentations\n</code></pre></p> </li> </ul> <ul> <li>After the meeting:   Store your presentation in:   <pre><code>Hoplab &gt; Science outreach &gt; Lab meetings &gt; Past LM presentations\n</code></pre></li> </ul> <ul> <li>Before the meeting:   Add papers of interest to:   <pre><code>Hoplab &gt; Science outreach &gt; Lab meetings &gt; Papers\n</code></pre>   This allows lab members to review them before the meeting.</li> </ul>"},{"location":"outreach.html#useful-links","title":"Useful Links","text":"<ul> <li> <p> Meeting Schedule</p> <p>View the specific and updated schedule for lab meetings</p> <p> Open Schedule</p> </li> <li> <p> Presenters and Batches</p> <p>Check the list of presenters and batch divisions</p> <p> View List</p> </li> <li> <p> New Papers List</p> <p>Access the list of new papers to discuss in meetings</p> <p> Browse Papers</p> </li> </ul> <p>Access Required</p> <p>These files are located in the NeuroSPACE / Hoplab Team organizational folders. You need access to the Teams group to view them.</p>"},{"location":"outreach.html#conference-posters","title":"Conference Posters","text":""},{"location":"outreach.html#useful-resources","title":"Useful Resources","text":"<ul> <li> <p> Design Guidelines</p> <p>Learn best practices for creating effective academic posters</p> <p> View Guidelines</p> </li> <li> <p> Printing Services</p> <p>Information about university printing options</p> <p> Printing Services  Additional Info</p> </li> <li> <p> KU Leuven Templates</p> <p>Access official KU Leuven poster templates</p> <p> Browse Templates</p> </li> <li> <p> Printing Request Form</p> <p>Submit your poster for printing</p> <p> Open Form</p> </li> <li> <p> Tips and Tricks</p> <p>Learn do's and don'ts for creating effective posters</p> <p> Read Tips</p> </li> </ul>"},{"location":"outreach.html#poster-printing-request","title":"Poster printing request","text":"<p>To print your poster, submit a \"Large-format printing request\" using this form. Follow these steps:</p> <ol> <li> <p>Choose paper type:</p> <ul> <li><code>140gr</code>: Standard paper</li> <li><code>Glanzend</code>: Shiny version</li> <li><code>Canvas</code>: Foldable, thick canvas (good for travel)</li> </ul> </li> <li> <p>Select size and quantity</p> </li> <li> <p>Upload your PDF file</p> </li> <li> <p>Additional options:</p> <ul> <li>Poster tube requirement</li> <li>White border trimming</li> </ul> </li> <li> <p>Pick-up location</p> </li> <li> <p>Finalize request:</p> <ul> <li>Ask for an invoice</li> <li>Use An Van Kets' u-number (<code>u0057838</code>) when finalizing your request.</li> </ul> </li> </ol>"},{"location":"outreach.html#present-your-work-and-announce-it","title":"Present your work (and announce it)","text":""},{"location":"outreach.html#presentation-templates","title":"Presentation Templates","text":"<p>You can find KU Leuven PowerPoint and L<sup>a</sup>T<sub>e</sub>X templates for presentations here.</p> <p>Please consult this page to correctly use the official KU Leuven brand templates.</p>"},{"location":"outreach.html#lbp-social-media","title":"LBP social media","text":"<p>The Laboratory of Biological Psychology has its own  profile!</p> <p> @biolpsychol</p> <p>If you are active on :</p> <ul> <li>Tweet about your lab's and your own achievements</li> <li>Share conference presentations / posters</li> <li>Announce pre-prints and new papers</li> <li>Highlight any relevant scientific wins</li> </ul> <p>Tip</p> <p>Remember to tag @biolpsychol in your tweets, or DM LBP for a re-tweet to boost visibility!</p>"},{"location":"outreach.html#ppw-faculty-calendar","title":"PPW Faculty calendar","text":"<p>It's good practice to announce your PPW presentations in the faculty calendar.</p> <p>Typically, Kirsten Blommaerts coordinates B&amp;C level announcements. If she's unavailable, follow these steps:</p> <ol> <li>Go to the PPW faculty calendar</li> <li>Log in with your KU Leuven credentials</li> <li>Click \"Add new items (Dutch)\"</li> <li>Select the year of your presentation</li> <li>Click \"+ Nieuw\" &gt; \"Agenda-item\"</li> <li>Add your presentation details (title, abstract, time, location)</li> <li>Save your changes</li> </ol> <p>Note</p> <p>After submission, there's a brief moderation period before your announcement appears online.</p> <p>Having issues? Check the manual for help.</p>"},{"location":"outreach.html#example-announcements","title":"Example announcements","text":"<p>Below you can find an example announcement for a general seminar:</p> <p></p> <p>Below you can find an example announcement for a doctoral school presentation:</p> <p></p>"},{"location":"outreach.html#science-communication-scicomm","title":"Science communication (SciComm)","text":""},{"location":"outreach.html#reasons-to-start-engaging-in-scicomm","title":"Reasons to start engaging in SciComm","text":"<p>If you are already interested in science communication, that's great! If you need some more convincing, here are some good reasons to start engaging:</p> <ul> <li>It allows you to connect with other researchers and stakeholders, which can boost your motivation and lead to new collaborations.</li> <li>It helps you to develop skills relevant to your future career, such as public speaking, explaining complex concepts, writing and networking.</li> <li>Funders (like the KU Leuven, FWO and ERC) value SciComm efforts.</li> <li>You are funded by society, so it's crucial for the public to know about your research.</li> <li>Despite the fact that it can be stressful, it is also enjoyable and very rewarding!</li> </ul>"},{"location":"outreach.html#straightforward-ideas-for-scicomm-participation","title":"Straightforward ideas for SciComm participation","text":"<p>The easiest and most straightforward ideas to start participating in SciComm yourself are:</p> <ul> <li>Being active on social media (e.g., on X)</li> <li>Creating video abstracts for your paper and starting a Youtube channel</li> <li>Writing a press release if you have done newsworthy research (ask the KU Leuven press office for help)</li> <li>Writing and publishing a blog post about your research (e.g., on EOS blogs, see for example Klara's blog)</li> <li>Making a podcast about your research</li> </ul>"},{"location":"outreach.html#training-and-opportunities","title":"Training and opportunities","text":"<p>Check out the following sources if you want to train yourself:</p> <ul> <li>Visuals and slide design: Principiae. Jean-Luc Doumont is a great speaker and frequently gives workshops at KU Leuven on how to create effective slides, posters, presentations, etc.</li> <li>Pitching, writing and presenting: The Floor is Yours. They offer workshops and coaching on how to tell your story in a clear and convincing manner, can help you bring your research in the media or make policy impact.</li> <li>Crash course on SciComm: Let's Talk Science summer school. A must-do three-day event all about communicative competences.</li> <li>Infographics: Baryon. Company of Koen Van den Eeckhout (PhD in physics), who provides workshops on how to visually present complex data.</li> <li>All things SciComm: SciMingo. One of the driving forces of SciComm in Flanders. They are the driving force behind many initiatives, such as Science Figured Out, the Flemish PhD Cup and the Scicomm Academy (see below). They also offer lots of workshops (e.g., for podcasting).</li> <li>SciComm in the form of theater: ERLNMYR. Company of Ben Verhoeven (PhD in computational linguistics), who uses methods from improvisational theater to improve science communication.</li> <li>Science videos: Amazink. Company of Rob Zink (PhD in biomedical signal processing), who offers a variety of freelance visualization services.</li> </ul>"},{"location":"outreach.html#engaging-initiatives","title":"Engaging initiatives","text":"<p>If you can't wait to start engaging, check out the following initiatives:</p> <ul> <li>Science Figured Out (SciMingo) is a project in which you are coached to record a 3-minute pitch about your research. A professionally edited video is afterwards published on several social media channels. For example, check out Klara's video here.</li> <li>Dag van De Wetenschap is the biggest yearly science event in Flanders, to which you can participate with your research group and organize something fun!</li> <li>Kinderuniversiteit organizes yearly workshops for children to get acquainted with research.</li> <li>Science is Wonderful! gives primary and secondary school students the change to interact with EU-funded researchers in an engaging format.</li> <li>Falling Walls Lab is an international pitching competition for researchers (with coaching). The winner goes to Berlin!</li> <li>The Flemish PhD Cup is a competition for freshly promoted PhD'ers, including media training at VRT NWS. The winner gets a lot of media attention, can give a video lecture at the Universiteit van Vlaanderen and can publish a book.</li> <li>The Battle of the Scientists is a competition where you explain your research to children in schools.</li> <li>Nerdland Festival is the biggest science festival in Flanders hosted by Lieven Scheire, to which you can participate with cool ideas.</li> </ul> <p>(It should be mentioned that all credits for this section go to Klara's former colleague Simon Geirnaert!)</p>"},{"location":"get-started/index.html","title":"Welcome to HOPLAB","text":"<p>We are thrilled to have you join our team at HOPLAB. Our lab is dedicated to exploring how the brain processes and interprets complex visual scenes, integrating insights from neuroscience, psychology, and computational science. As part of our team, you\u2019ll contribute to cutting-edge research that bridges human cognition with artificial intelligence. We look forward to your contributions and to working together on exciting discoveries.</p>"},{"location":"get-started/index.html#getting-started-at-hoplab","title":"Getting started at HOPLAB","text":"<p>On this onboarding page, you'll find all the information you need to navigate the KU Leuven environment and adapt to our lab\u2019s practices. We\u2019ll guide you in setting up both your physical and digital workspaces, and offer step-by-step instructions for the most common administrative procedures at KU Leuven. Additionally, we\u2019ve curated a selection of resources to give you a strong foundation as you start your journey with us.</p>"},{"location":"get-started/index.html#quick-links","title":"Quick links","text":"<p>To help you get settled quickly, here are some essential resources:</p> <ul> <li> <p> Setting up your digital workspace</p> <p>Get started with your computer, software installations, and network access.</p> <p> Computer setup guide</p> </li> <li> <p> Setting up your physical workspace</p> <p>Learn how to access the lab, get keys, and set up your office space.</p> <p> Practical setup guide</p> </li> <li> <p> Administrative procedures</p> <p>Navigate the KU Leuven systems for HR tasks, reimbursements, leave requests, and more.</p> <p> Admin procedures</p> </li> <li> <p> Mailing lists</p> <p>Stay informed by subscribing to important lab and university mailing lists.</p> <p> Mailing lists guide</p> </li> <li> <p> Student starter pack</p> <p>Essential resources for students, including coding tutorials and academic writing tips.</p> <p> View starter pack</p> </li> <li> <p> Useful links</p> <p>A curated list of links to university platforms, lab tools, and external resources.</p> <p> View useful links</p> </li> </ul>"},{"location":"get-started/index.html#need-help","title":"Need help?","text":"<p>If you have any questions or run into issues while getting set up, the following resources are available:</p> <ul> <li>ICT helpdesk: For technical issues related to your computer or network access. ICT FAQ</li> <li>HR advisor: Find your HR advisor's details in your KU Loket profile.</li> <li>Admin contact: If you need assistance with (general) lab operations such as equipment and orders, reach out to Ying Cai </li> </ul>"},{"location":"get-started/index.html#unit-of-confidence","title":"Unit of confidence","text":"<p>At Hoplab, we care deeply about creating a safe and respectful environment for everyone involved. If you ever experience or witness inappropriate behavior, or if something doesn\u2019t feel right, know that you are not alone and there is always someone to talk to.</p> <p>KU Leuven\u2019s Unit of Confidence offers a confidential and supportive space to talk things through. Whether you are unsure about a situation or need advice, they are here to help. Visit the website here.</p>"},{"location":"get-started/index.html#get-involved","title":"Get involved","text":"<ul> <li>Visit the HopLab website for information on our research projects, team members, and recent publications.</li> <li>Contribute to this Wiki: Your insights are valuable! If you spot missing information or have suggestions for improvement, please let us know.</li> </ul> <p>We hope this guide helps you settle into your new role. Dive in, and enjoy your journey with HOPLAB!</p> <p>Added from file \u2190 https://ppw.kuleuven.be/en/research (to getting started) \u2192</p>"},{"location":"get-started/admin-procedures.html","title":"Admin 101","text":"<p>On this page, we will specify the procedures for the most common administrative procedures you might need to follow. If something is missing, please contact us so we can add it and keep this page up to date (and your lives easier).</p> <p>KU Loket is the university portal through which many administrative things are done. You can access it here.</p>"},{"location":"get-started/admin-procedures.html#consult-your-personnel-file","title":"Consult your personnel file","text":"<p>Log in to KU Loket and go to \"Personnel\", then click on \"My details\" (shown in red below):</p> <p></p> <p>At the very top of this page, you can find the name of your HR advisor (1). Your HR advisor offers you a listening ear, provides clarifying advice on HR-related matters and supports you with the administrative obligations that come along with employment at KU Leuven.</p> <p>Below, you can edit your personal details (2), such as adding a new private address, updating your family situation, adding emergency contacts or adding a new bank account number by clicking on the pencil in the top right corner of the corresponding section.</p> <p>Browse through the other tabs to find out and/or change information on communication means (3), insurances and other financial advantages you are entitled to (4), your contract details (5), mobility (e.g., to order a KU Leuven bike, get a refund for public transportation commuting costs, or to register your personal vehicle for work use) (6) and your curriculum at KU Leuven (7).</p> <p>Through your personnel file, you can also apply for teleworking allowances (for KU Leuven staff only) (8) and/or apply for a bicycle allowance if you bike (part of) your home-work journey (9). If you are financed by FWO, check out this page. If you are looking for information on the parking policy of KU Leuven, go here.</p>"},{"location":"get-started/admin-procedures.html#reimbursement-for-professional-expenses","title":"Reimbursement for professional expenses","text":"<ol> <li>Log in to KU Loket and go to \"Finance &amp; Purchasing\"</li> <li>Click on \"Professional expenses\" (shown in red below)</li> <li>Click on \"Forms\", create a \"+ New form\" and give it an informative name (e.g., congres X)</li> <li>Add all your expenses to the form (e.g., train tickets, dinner expenses, the registration fee)</li> <li>Link a digital proof to each expense</li> <li>Couple the appropriate financial antenna to your form (An Van Kets, u0057838)</li> <li>Add information on which fund your expenses should be paid from (ask  your PI)</li> </ol> <p>For more information, check out the available demo videos and/or read the FAQ. If you have specific questions concerning this procedure, contact An (an.vankets@kuleuven.be) or go find her in office 02.80.</p>"},{"location":"get-started/admin-procedures.html#register-personal-leave","title":"Register personal leave","text":"<p>There are a couple of things we usually do in the lab when we take a holiday:</p> <ol> <li>Notify Hans by sending an email with the subject \"Holiday\", to keep an official trace of your holiday request (that also helps him keep track of who\u2019s on leave).</li> <li>Mark the days you\u2019re off in the Hoplab Google calendar, so everyone is aware of your absence.</li> <li>Notify your employer, which can be different depending on your contract (the procedure below is for KU Leuven employees, for instance).</li> </ol>"},{"location":"get-started/admin-procedures.html#procedure-for-ku-leuven-employees","title":"Procedure for KU Leuven employees","text":"<p>The procedure for KU Leuven employees is as follows (If you are financed by FWO, check out this page). When you plan your holiday, you have to register this in KU Loket and get it approved. To do this, follow these steps:</p> <ol> <li>Log in to KU Loket and go to \"Personnel\"</li> <li>Navigate to \"My Leave\" and click on \"Requesting leave\"</li> <li>Click on a day and fill in the appropriate start and end date of your holiday period</li> <li>Click on the arrow, choose how many hours of each type of leave you are entitled to you want to use</li> <li>Submit your request to your approver</li> </ol> <p></p> <p>To register other kinds of personal leave (e.g., short-term or social leave or sick leave), check out the corresponding links. For further questions on this topic, browse to this page.</p>"},{"location":"get-started/admin-procedures.html#register-professional-leave","title":"Register professional leave","text":"<p>This has to be done (in advance) for absence in the context of any work-related event (national and international), such as a conference or the annual LBP retreat. This registration is needed in order to claim professional expenses made during this trip, to make sure you are correctly insured and to allow the KU Leuven to monitor the safety level of the trip.</p> <p>Proceed as follows:</p> <ol> <li>Log in to KU Loket and Go to \"Personnel\"</li> <li>Navigate to \"Register missions\" and click on \"New request\"</li> <li> <p>Fill out the required information on (amongst others):</p> <ul> <li>When you will be absent</li> <li>Where you will be</li> <li>How you will get there</li> <li>Why you are going</li> </ul> </li> <li> <p>Add the correct financial antenna to the form (An Van Kets, u0057838)</p> </li> <li>Submit the form</li> </ol> <p></p> <p>For more information, check out this page. If you are using your own vehicle for the trip, please make sure it is registered for insurance reasons. To do so, follow the steps on this page.</p>"},{"location":"get-started/admin-procedures.html#reserve-a-room-for-a-meeting","title":"Reserve a room for a meeting","text":"<p>In order to book a room, contact Ying. You can check which rooms are available yourself through KU Loket:</p> <ol> <li>Go to KU Loket, navigate to the tab \"HSE &amp; Spaces\" and click on \"Classroom reservations\" (Dutch only)</li> <li>Click \"Zoek op naam\" and enter <code>PSI</code> to find all the rooms in the building.</li> <li>Select the rooms that fit your needs</li> <li>Click \"toon reservaties\"</li> <li>Browse to the date &amp; time when you need the room</li> <li>Look for an available room and send this info to Ying</li> </ol> <p></p>"},{"location":"get-started/admin-procedures.html#reserve-equipment-or-a-room-for-testing","title":"Reserve equipment or a room for testing","text":"<p>The standard booking tool for testing rooms and equipment (testing booths, EEG caps, etc.) in our faculty is Calira.</p> <ul> <li>To access Calira, you need to make a user account using an invitation link. This link decides to which infrastructure you have access to and is different for every research unit/group. For our group (B&amp;C Human), you can get the link by sending an email to Klara Schevenels (klara.schevenels@kuleuven.be).</li> <li>When you have access, choose the option to log in via your organization, so you can access it through the KU Leuven login tool.</li> <li>Make sure to use Calira to book your testing time slots when you use common rooms or material. Request access to the items you don\u2019t have permission to in case you need it. You can find more information on how to use Calira in this presentation.</li> <li>The faculty also has their own MS Teams <code>GHUM PPW - Research rooms and equipment</code> where you can find more detailed information on the available research infrastructure as well as a manual on how to book infrastructure with Calira (for more info, check out this page).</li> </ul>"},{"location":"get-started/admin-procedures.html#order-stuff","title":"Order stuff","text":"<p>Please contact Ying for assistance with arranging and/or ordering and/or paying for the following things:</p> <ul> <li>Hotel reservations</li> <li>Flight bookings</li> <li>Online purchases</li> <li>Computer equipment</li> <li>Physical mail and packages (post)</li> </ul> <p>General orders (e.g., milk, coffee, office material) are also handled by Ying. In case of bigger purchases, make sure to include the financial antenna (An Van Kets) in the process.</p> <p>The IT department handles orders related to computer supplies (ppw.dict@kuleuven.be). All computer equipment needs to be bought through the official university providers by the financial antenna (An Van Kets). Therefore, when computer supplies need to be bought, make sure to contact them both. It can also be useful to include Ying in CC in that case.</p> <p>If you need technical assistance for your order (e.g., you want to order an EEG-cap), you can contact Klara or send an email to neurospace@kuleuven.be (general mailing address to reach the NeuroSPACE support staff).</p>"},{"location":"get-started/admin-procedures.html#arrange-a-parking-spot-for-a-visitor","title":"Arrange a parking spot for a visitor","text":"<p>If you need to arrange parking for a visitor in the city center, you can do so by requesting a day code for the visitor to operate the barriers of the KU Leuven personnel parkings. For more information, check out this page. It is worth noting that the car park underneath the Herman Servotte Residence located on the Social Sciences campus (Parkstraat 39-53, 3000 Leuven) can be accessed outside of working hours (18h-24h) and during holidays with your staff card.</p> <p>Parking spots at the university hospital (UZ Leuven campus Gasthuisberg) can be refunded in the form of a parking ticket that visitors can use to pay their parking fee. We can buy these tickets (in batches) in advance, so make sure to check beforehand with Klara if we have tickets left or not. New tickets can be ordered by Ying.</p>"},{"location":"get-started/admin-procedures.html#what-to-do-when-you-leave","title":"What to do when you leave","text":"<p>We are very sad to see you leave! But since you do, make sure to tick the following boxes.</p> <ol> <li> <p>Return the following items:</p> <ul> <li>Any pc material (laptop, mouse, keyboard, etc.)</li> <li>Your external hard drive (and potential USB-key)</li> <li>Your office key(s)</li> </ul> </li> <li> <p>Make sure your data is organized and backed up correctly. That mostly means having back-ups in different locations and different ways. If you need advice on this, refer to the person in charge of data storage (as of December 2024, that is Klara).</p> </li> <li>For more information on what to do when you leave, check out this link for ICT-related info, this link for HR-related info and this link if you are an international researcher leaving Belgium.</li> </ol>"},{"location":"get-started/computer-setup.html","title":"Setting up your digital working environment","text":"<p>Please read the Welcome to ICT@PPW three-pager to get up and running with IT at our faculty. In case of ICT-related problems, make sure to check the FAQ page of PPW Dienst ICT here.</p>"},{"location":"get-started/computer-setup.html#using-a-computer-managed-by-the-university","title":"Using a computer managed by the university","text":"<p>Faculty issued computers can be recognized by their name starting with GHUM. To set you up, follow these steps:</p> <ol> <li>Login:<ul> <li>You can log in to faculty issued computers with your intranet account.</li> <li>Use your u-number (or r-number in case you are a student) and the password of your e-mail address to do so.</li> </ul> </li> <li>Internet access:<ul> <li>To access the university's wireless network, look for campusroam in the list of available networks. This network offers the broadest access to faculty resources but only accepts u-numbers. When asked to authenticate, enter your u-number followed by @kuleuven.be and your intranet password. If you have an r-number, you can connect to the eduroam network, but this network does not allow access to PPW faculty file shares or printers.</li> <li>To gain access to the wired network in the PPW-buildings, get your network outlet activated by completing this form (in case it is not pre-activated).</li> </ul> </li> <li>Administrator access:<ul> <li>If you are not already, contact dICT (dict@ppw.kuleuven.be) to make you administrator for the computer you are going to work on. You need to provide them the hostname of your pc (GHUM-\u2026) and your u-number.</li> <li>Alternatively, if someone else is already administrator, ask them to add you (Windows control panel &gt; Change account type &gt; Add &gt; Add a new LUNA account).</li> </ul> </li> <li>Multi-factor authentication:<ul> <li>To access KU Leuven intranet pages, you will need to log in with KU Leuven Authenticator. You can register your device with a smartphone or tablet via the KU Leuven Authenticator App (read the instructions).</li> <li>If you are having issues with MFA, check this FAQ page.</li> </ul> </li> </ol> Where do I find the hostname of my pc? <p>The hostname is usually printed on a sticker on the computer. If not, go to Start, right click on \"This PC\", choose properties, and check the \"Device name\" field.</p>"},{"location":"get-started/computer-setup.html#installing-matlab","title":"Installing MATLAB","text":"<p>The installation process differs for students and personnel. Please follow the instructions in the appropriate tab below:</p> StudentsPersonnel <ol> <li>Register on the MathWorks website using your KU Leuven student email address.</li> <li>After registering, download the software directly from the MathWorks website.</li> </ol> <ol> <li> <p>Choose the appropriate MATLAB license:</p> <ul> <li> <p>Individual License: Recommended for most users. This license allows you to use MATLAB on multiple computers (up to 2 simultaneously) and includes access to MATLAB desktop software and online services (e.g., MATLAB Online, Add-Ons, and MATLAB online training). This option in suited for individual personnel.</p> </li> <li> <p>Designated Computer License: Use this license if MATLAB is to be installed on a computer that is permanently offline or where users cannot log in under their own account. It allows any number of users to access MATLAB on that specific computer, though not simultaneously. This option is generally suited for lab/shared computers.</p> </li> </ul> <p>The license fee can be covered using individual professional funding sources (e.g., bench fees, grant money, etc.), depending on your contractual situation. For more details, please discuss with your PI.</p> Transition to \"Individual License\" <p>The old '5pack' license will no longer be available after October 31, 2024. To continue using MATLAB, users must switch to an 'Individual License' or 'Designated Computer License'. It is recommended to remove any older versions of MATLAB and install the most recent version as an 'Individual License' user. For newer versions (from R2023b onwards), you can easily switch licenses by placing a new license file. Detailed instructions are available here.</p> </li> <li> <p>Request access from ICTS:</p> <ul> <li>The information for Matlab can be found here. The request form can be found at this link. </li> <li>The u-number workflowreceiver is <code>u0057838</code> (An van Kets) </li> <li>The number for the organizational unit is <code>53197848</code> (Brain and Cognition). </li> <li>The \"credit\" section can be left blank. You can write KULC if your research funding code does not begin ZL\u2026. If it begins with ZL, then then you can fill in with LRD. </li> </ul> </li> <li> <p>Download MATLAB:</p> <ul> <li>Once approved, get the MATLAB installation files from the KU Leuven portal. You will receive an email once approved with this link.</li> <li>Follow the instructions to download the installer for your operating system.</li> </ul> </li> <li> <p>Install and activate MATLAB:</p> <ul> <li>Run the MATLAB installer and follow the on-screen instructions.</li> <li>During the activation process, select \"Individual License\" and log in with your MathWorks account.</li> <li>Input the license key provided through the ICTS License Catalogue when prompted.</li> </ul> </li> </ol> Updating your PC registration <p>To change your registered computer name or IP address:</p> <ol> <li>Go to https://icts.kuleuven.be/apps/license</li> <li>Click the pencil icon under \"register your PC\"</li> <li>Update your hostname and IP address as needed</li> </ol>"},{"location":"get-started/computer-setup.html#frequently-used-software","title":"Frequently used software","text":"<p>Here are some programs we frequently use in the lab, which you might find useful to download:</p> <ul> <li> <p>Google Calendar: Make sure you have writing access to the lab's Google calendar (ask Andrea or Klara).</p> </li> <li> <p>TeamViewer: For remote access to a desktop PC, e.g. the fMRI PC to run your analyses.</p> TeamViewer Setup Guide <ol> <li>Download the free private version from the official website.</li> <li>Create an account:<ul> <li>Open TeamViewer and click \"Sign Up\".</li> <li>Enter your email, name, and a strong password.</li> <li>Verify your email address.</li> </ul> </li> <li>Add a computer:<ul> <li>Sign in and go to \"Computers &amp; Contacts\".</li> <li>Click \"Add Computer\".</li> <li>Name the computer (e.g., \"Lab Desktop\").</li> <li>Click \"Add\" to save.</li> </ul> </li> <li>Connect:<ul> <li>Open TeamViewer and log in.</li> <li>Find the computer in your list.</li> <li>Double-click to connect.</li> <li>Enter the remote computer's password when prompted.</li> </ul> </li> </ol> </li> <li> <p>AnyDesk: A good alternative if TeamViewer is inaccessible. Install the free private version. It is advisable to install and configure both TeamViewer and AnyDesk to avoid being locked out if one of them is not accessible.</p> </li> <li> <p>Google Chrome: This is the preferred web browser. For example, the MR calendar is only compatible with this browser.</p> </li> <li> <p>Slack: Slack is used for communication within the lab. Ask any lab member to add you to the relevant channels.</p> </li> <li> <p>Skype for business and Microsoft Teams: KU Leuven offers both Skype for business and MS Teams for communication purposes. See this table for a comparison between the different platforms. Currently, MS Teams is the newer and preferred option, however, it only allows its users to reach other MS Teams users. With MS Teams, it is currently not possible to call (or be called by) telephone numbers (landline and mobile). You can use Skype for this.</p> </li> <li> <p>SSL VPN Pulse Client / Ivanti Secure Access Client: The VPN offered by the university. For more information, check out this link.</p> </li> <li> <p>Overleaf: An online L<sup>a</sup>T<sub>e</sub>X editor for collaborative writing and publishing.</p> Why use Overleaf? <p>Overleaf is a powerful tool for academic writing, especially for scientific papers and theses. Here's why it's important:</p> <ol> <li>L<sup>a</sup>T<sub>e</sub>X-based: Produces high-quality, professional-looking documents with complex equations and formatting.</li> <li>Collaboration: Real-time collaboration with co-authors, similar to Google Docs.</li> <li>Version control: Tracks changes and allows reverting to previous versions.</li> <li>Journal templates: Many journals provide L<sup>a</sup>T<sub>e</sub>X templates that can be directly used in Overleaf, streamlining the submission process.</li> <li>Integration: Works with reference managers like Mendeley and Zotero.</li> <li>Accessibility: Web-based, so you can work from any computer without installing software.</li> </ol> <p>While L<sup>a</sup>T<sub>e</sub>X has a learning curve, investing time in learning it can significantly improve your academic writing workflow and the quality of your documents.</p> </li> </ul> <p>Administrative privileges on KU Leuven PCs</p> <p>In case you have issues installing software (e.g., because of lack of administrator access), you can double click the \"Make Me Admin\" icon on your Windows desktop and follow the instructions to get temporary administrator rights on your computer. Additionally, make sure to install the software in <code>C:\\Workdir\\MyApps\\</code>. Please contact the ICT helpdesk if problems persist.</p>"},{"location":"get-started/computer-setup.html#data-storage","title":"Data storage","text":"<p>All KU Leuven staff and students have their own OneDrive environment with 2 TB of online storage space to store personal work files. The files on OneDrive are synced to a folder on your local device (Windows Explorer), but can be accessed from various devices from any location. It is also possible to share documents with someone else and work together on the same document.</p> <p>Back up your data twice</p> <p>Keep your data in one main folder (folder name = your first name) if you are an intern, or on your PC if you are personnel, and back-up this data to:</p> <ol> <li>A portable hard drive (shared between interns, or ask Ying if you are personnel). Don\u2019t forget to give the external drives back to your supervisor when your role ends.</li> <li>Your online OneDrive folder. To get started with OneDrive, check out this page.</li> </ol>"},{"location":"get-started/computer-setup.html#printing","title":"Printing","text":"<p>Find info on how to install printing services on your desktop or laptop connected to the KU Leuven or faculty network here. The printer names are:</p> <ul> <li>PRLBP (Black &amp; White printer)</li> <li>PRLBP2 (Color printer)</li> </ul> <p>If the installation doesn\u2019t work, or you are using a windows desktop or laptop not connected to the KU Leuven or faculty network, use a USB key to print on the black and white printer in room 02.28. You can also get permanent access by asking Ying to add you to the list of users.</p> <p>For mac users, follow the instructions on this page to print from your personal computer. More generally, this manual tries to give an overview of most frequently asked questions concerning configuration and initial setup of a secure work environment on Mac OS X.</p> <p>I think it happens automatically at least for Windows  TODO: [Klara] Delete all Skype info as it will be deprecated TODO: [Klara] Make it clearer that all software has to be installed in Workdir!! TODO: [Klara] Edit info on data storage after BADM consultation has ended (e.g., OneDrive info is outdated and not recommended any longer by KU Leuven) TODO: [Klara] Add info on how to print downstairs \u2192</p>"},{"location":"get-started/mailing-lists.html","title":"Some useful mailing lists","text":"<p>If you just got started in the lab, you might want to make sure you're added on all of the relevant mailing lists that we often use or refer to.</p> The essentialThe recommendedThe optional <p>Here are the mailing lists you need to be subscribed to.</p> <ol> <li>The LBP mailing list (PSYLBP@LS.KULEUVEN.BE). The most important mailing list administratively. Any first-hand administrative communication will go through there, including building-, retreat-, and kitchen-related information. You will need to find the LBP technician (Zsuzsanna) and ask to be registered. Use this email when you need to communicate something at the LBP level.</li> <li>The HopLab group email (hop_lab@googlegroups.com). Many important communications are made lab-wise through it. Most importantly, lab meeting details are sent through there, alongside all of the communication that cannot be made via chat. Use this email to send something to the lab that is not more easily shared on our chat platform (Slack), or to forward important emails. Ask the lab website responsible person (Klara) to add you.</li> <li>The NeuroSPACE mailing list (NEUROSPACEMEMBERS@LS.KULEUVEN.BE). This is a mailing list containing all members of the teams of profs. Hans Op de Beeck (HOPLAB), Bert De Smedt (MATHLAB), C\u00e9line Gillebert (Neuropsychology Lab Leuven) &amp; Kobe Desender (DesenderLab), which was created in the context of the Methusalem grant uniting all four research groups. Ask the scientific support staff working on this project to add you to this list (Klara or Silke). </li> </ol> <ol> <li>The LBI newsletter (register through this website) communicates any event the Leuven Brain Institute organises. This is mostly important to stay updated on things like the annual LBI scientific meeting, but also interesting science communication events.</li> <li>The BAPS newsletter (register on their main page) can be of relevance for Belgium-wise psychological science events &amp; news. For instance, the yearly BAPS meeting will be shared through this channel.</li> <li>The MRI mailing list (MRI@LS.KULEUVEN.BE) will report any news concerning the MR8 scanner at the hospital. If you scan or plan on scanning, this is a must. It will tell you if the scanner breaks down, if there are any new guidelines to follow, or more commonly, if any booked timeslot is cancelled. This is a KU Leuven-based mailing list, which you can register to on this website.</li> </ol> <ol> <li>The visionlist is the main community-wide newsletter for vision sciences. Any opening position, conference, summer school you might be interest in will be on there (more info here).</li> <li>The CVnet mailing list communicates about everything colour and vision-related. Conferences, positions, community news will be posted on there (more info here).</li> <li>If you're a PhD student, the Doctoral school mailing list (ppw.doctoraten@kuleuven.be) can be useful to you. You can find it on the KU Leuven mailing list server. </li> </ol>"},{"location":"get-started/practical-setup.html","title":"Setting up your real life workspace","text":""},{"location":"get-started/practical-setup.html#keys","title":"Keys","text":"<p>Ask the LBP lab technician (Ying Cai), whom you can find in room 02.37, for a key to your new office.</p>"},{"location":"get-started/practical-setup.html#staff-card","title":"Staff card","text":"<p>The staff card (different from the student card if you\u2019re an intern or a PhD researcher) can be used to access the PSI building at all time. Just badge it on the card reader at the main entrance (Tiensestraat 102) when you need access outside of working hours. You will receive it by mail in the first weeks of your contract.</p>"},{"location":"get-started/practical-setup.html#office-set-up","title":"Office set-up","text":"<p>For each new member, a laptop and other pc material (e.g., an external hard drive, computer screen, mouse, keyboard, etc.) will be ordered or be made available. If you are missing something or need something specific, please contact Ying with Klara in CC.</p>"},{"location":"get-started/practical-setup.html#phone","title":"Phone","text":"<p>If you need to call or be called by external numbers, Skype for business can be used and also has a voicemail system associated to it. For good quality calls, you can use a headset, forward the calls to your personal phone number or opt for an old-school stationary phone linked to your Skype for business account (e.g., if you need to make a lot of calls and you only want to be reachable from your desk). You can get a phone from Ying. If you do, make sure you submit the serial number here.</p>"},{"location":"get-started/student-starter-pack.html","title":"Student starter pack","text":"<p>If you're a student newly starting in the lab, this page is made for you. It is packed with resources and knowledge that you will find useful during your time with us. Enjoy!</p> <p>The starter pack is divided into three sections. In no order of importance, you will find important papers from authors we might refer to every once in a while, coding tutorials to get up to speed with Python and develop your programming skills, and miscellaneous resources that don't fit in these two categories but are still relevant.</p>"},{"location":"get-started/student-starter-pack.html#coding-tutorials","title":"Coding tutorials","text":"<p>Developing strong coding skills is crucial for success in our lab. Here are some excellent resources to help you improve your Python programming and data science skills:</p> <ol> <li> <p> The Good Research Code Handbook: This is a must-read for anyone joining the lab. It covers essential knowledge on how to structure and write your code in research, suitable for both beginners and experienced researchers. Good coding practices are also summarised in this paper.</p> </li> <li> <p>You can find some fun and interactive Python tutorials on DataCamp and Codecademy.</p> </li> <li> <p>Software carpentry Python fundamentals: Basic Python concepts for beginners.</p> </li> <li> <p>EdX: Using Python for research: Free course on Python applications in research.</p> </li> <li> <p>Scientific Python lectures: Advanced course for confident programmers.</p> </li> <li> <p>Python data science handbook: Comprehensive guide to data science with Python.</p> </li> <li> <p>If you know nothing about it, take some time to learn the Unix shell and the essentials of Git &amp; GitHub.</p> </li> <li> <p>If you're going to use MatLab, you can check this live script tutorial to learn the fundamentals.</p> </li> </ol>"},{"location":"get-started/student-starter-pack.html#miscellaneous","title":"Miscellaneous","text":"<ul> <li> <p> Academic writing</p> <p>Learning how to write is fundamental to academic training. If you're struggling with writing or structuring your papers, check out:</p> <p> Ten simple rules for structuring papers</p> </li> <li> <p> Machine learning fundamentals</p> <p>To understand the basics of machine learning and modeling, this Coursera class is a must:</p> <p> Machine learning specialization</p> </li> <li> <p> Awesome PhD resources</p> <p>A curated list of carefully selected tools and resources for both early career and senior researchers:</p> <p> Awesome PhD repository</p> </li> <li> <p> Awesome neuroscience resources</p> <p>Curated list of awesome neuroscience libraries, software and any content related to the domain.</p> <p> Awesome neuroscience repository</p> </li> </ul>"},{"location":"get-started/student-starter-pack.html#papers-lectures","title":"Papers &amp; lectures","text":"<p>This section contains some foundational papers to give you some background on the research going on in the lab, as well as interesting lectures if you can't find anything to watch on Netflix. This list might not always be up to date, but you can find our latest publications on the lab website.</p> Some reviewsClassic papersfMRI studies we refer toDNN papersLectures <p>Bracci &amp; Op de Beeck 2023</p> <p>Grill-Spector and Weiner 2014</p> <p>DiCarlo et al 2012</p> <p>Kravitz et al 2013</p> <p>Op de Beeck et al 2019</p> <ul> <li> <p>The classic publication by Felleman &amp; Van Essen 1991 covers the intricate pattern of connectivity that characterises the ventral stream and the visual system in general. </p> </li> <li> <p>This publication by Roger Shepard 1980 is a pioneer in the development of multivariate analyses.</p> </li> </ul> <ul> <li> <p>The study by Bracci et al. 2016 is a good illustration of how we use carefully crafted stimulus sets in combination with multivariate fMRI to answer questions about visual representations in the brain.</p> </li> <li> <p>More recent examples in the lab include Ritchie et al. 2021 and Yargholi &amp; Op de Beeck 2023.</p> </li> <li> <p>The foundational paper by Kriegeskorte et al 2008 introduced the use of representational similarity analysis (RSA) to compare representations across brains, species, models and more.</p> </li> </ul> <ul> <li> <p>If you feel like DNNs and brains are a great match, check out this reading list by Anna Wolff and Martin Hebart.</p> </li> <li> <p>The fundamental papers of Yamins et al and Khaligh-Razavi &amp; Kriegeskorte both published in 2014 showed for the first time that DNNs develop similar representations to the brain.</p> </li> <li> <p>In our lab, Kubilius et al 2016 showed that the representations of shape in DNNs display some properties that had been seen before in the human brain and perception, and not in earlier artificial models.</p> </li> <li> <p>Among the very often cited papers is the publication by Geirhos et al 2022 which showed that DNNs are biased towards texture, suggesting that their processing of shape is still not the same as in humans.</p> </li> <li> <p>Opinions diverge on how to use modelling to further our understanding of the brain. This 2023 BBS paper by Bowers et al surely started a serious discussion on that topic.</p> </li> <li> <p>The tradition in the lab to use carefully crafted stimulus set is a promising approach to understand the differences between human visual processing and DNNs, as for example illustrated by Bracci et al 2019.</p> </li> </ul> <ul> <li>A visual and intuitive understanding of deep learning is a great lecture to begin with if you feel like convolutional neural networks are still obscure to you.</li> </ul>"},{"location":"get-started/useful-links.html","title":"Useful links when getting started","text":"<p>Luckily, you have found the most useful link of all, i.e., the link to this Wiki! Besides this, there are some other links worth checking out.</p> <p>Info</p> <p>In the Wiki, we frequently will refer you to shared documents in the lab's MS Teams folder, which you can find here.</p>"},{"location":"get-started/useful-links.html#getting-started-at-ku-leuven","title":"Getting started at KU Leuven","text":"<ul> <li> <p> First steps</p> <p>For information on the first administrative steps upon arrival for international students/staff</p> <p> Link</p> </li> <li> <p> Orientation days</p> <p>For information on welcome activities (KU Leuven Orientation Days) specifically aimed for new international students/staff</p> <p> Link</p> </li> <li> <p> My KU Leuven</p> <p>For all administrative tasks and more</p> <p> Link</p> </li> <li> <p> KU Loket</p> <p>For accessing a wide range of university services and resources</p> <p> Link</p> </li> <li> <p> Toledo</p> <p>For all course-related things</p> <p> Link</p> </li> <li> <p> Filesender</p> <p>For safely transferring large files</p> <p> Link</p> </li> </ul>"},{"location":"get-started/useful-links.html#getting-started-at-hoplab","title":"Getting started at Hoplab","text":"<p>Apart from this Wiki, we try to keep the HopLab website up to date, and that includes new people. Send a picture of yourself and a short introductory text (see the members page for inspiration) to the person in charge of the Lab website (Klara) along with useful links if wanted (ORCID, LinkedIn, Scholar, BlueSky,\u2026).</p> What is ORCID? <p>ORCID is a unique digital identifier system for researchers, used by most journals during paper submission. To create your ORCID ID, go here, select \"Sign In/Register\", choose \"Access through your institution\" and select \"KU Leuven Association\".</p>"},{"location":"research/index.html","title":"Research at the Hoplab","text":"<p>Welcome to the central hub for all research activities at Hoplab. This page serves as your gateway to our research methodologies, data analysis procedures, and resources. Whether you're involved in behavioral studies, EEG, eye-tracking, or fMRI, you'll find the essential guidelines and workflows here.</p> <ul> <li> <p> Behavioral research</p> <p>Explore our experimental setups, participant management, and best practices for conducting behavioral research.</p> <p> Learn more</p> </li> <li> <p> Coding resources</p> <p>Access coding guides, scripts, and resources for various research tools, including fMRI and behavioral experiments.</p> <p> View coding resources</p> </li> <li> <p> Deep Neural Networks (DNN)</p> <p>Dive into our research on deep neural networks and their application in cognitive and neuroscience research.</p> <p> Explore DNN research</p> </li> <li> <p> EEG studies</p> <p>Find protocols, analysis workflows, and tips for conducting and analyzing EEG studies in the lab.</p> <p> Access EEG resources</p> </li> <li> <p> Ethics</p> <p>Get guidance on the ethical procedures and approval processes required for research in our lab.</p> <p> Review ethical guidelines</p> </li> <li> <p> Functional MRI (fMRI)</p> <p>Find everything from fMRI scan setup to data analysis workflows and coding resources.</p> <p> fMRI resources</p> </li> </ul>"},{"location":"research/index.html#quick-links-to-resources","title":"Quick links to resources","text":"<p>Here are some additional links to key research resources, tutorials, and tools:</p> <ul> <li>SPM software guide</li> <li>EEGLAB documentation</li> <li>Nilearn for machine learning in neuroimaging</li> <li>BIDS specification for neuroimaging</li> <li>MNE Python for EEG analysis</li> </ul> <p>this is in the fmri section for now. perhaps we can link it? \u2192 would wait until the end of the BADM consult process, perhaps we can make a page dedicated to RDM </p> <p>TODO: [Klara] Refer to the resources for researchers on the PPW Research page</p> <p>TODO: [Klara] Make list of events throughout the year e.g. conferences, scicomm activities, days to remember (e.g., stroke day) \u2192</p>"},{"location":"research/behaviour/index.html","title":"Behavioural research","text":"<p>Our lab focuses on understanding human behaviour through controlled experiments, from recruitment and experimental setup to data collection and analysis. This section will guide you through the key aspects of running behavioural studies in our lab, offering resources and step-by-step guides to ensure your research is carried out smoothly.</p> <p>Here, you\u2019ll find everything you need for setting up experiments, recruiting participants, and ensuring compliance with ethical guidelines. Use the quick links below to dive into specific topics or read through the guides to get a comprehensive understanding of our procedures.</p> <p>We hope this section helps you conduct your behavioural research with ease and precision. Happy researching!</p> <ul> <li> <p> Experimental setup</p> <p>Learn how to prepare your experimental environment, including online and in-person setups.</p> <p> View experimental setup guide</p> </li> <li> <p> Participant recruitment</p> <p>Find detailed information on recruiting participants, using the EMS system, and managing consent.</p> <p> Participant recruitment guide</p> </li> <li> <p> Ethics approval</p> <p>Ensure your study complies with KU Leuven's ethical standards and obtain the necessary approvals.</p> <p> Ethics guide</p> </li> <li> <p> Reimbursement &amp; compensation</p> <p>Guidance on compensating participants, including credit and payment processes.</p> <p> Reimbursements Guide</p> </li> </ul>"},{"location":"research/behaviour/bh-participants.html","title":"Participant recruitment","text":"<p>Before starting participant recruitment, ensure your study is formally approved by the ethical committee (if you didn't do this yet, check out this page). Prepare this well in advance. All recruitment materials (e.g., flyers, posters) must include the study's end date as specified in the ethics application.</p> <p>(Healthy young adult) participants can be recruited in different ways:</p> <ol> <li> <p>Through the laboratory's participant database:</p> <ul> <li>Including previous participants and individuals who have expressed interest in participating through word-of-mouth referrals </li> </ul> </li> <li> <p>Through social connections of the researchers involved and/or by sharing your announcement via social media:</p> <ul> <li>For example, announce your experiment on this facebook page, which collects the latest info on psychological experiments at KU Leuven</li> <li>Make sure to read the guidelines on recruiting participants via social media provided by the ethical committee that approved your study (guidelines SMEC, guidelines EC onderzoek)</li> </ul> </li> <li> <p>Through the online recruitment system of the faculty (see below)</p> </li> </ol>"},{"location":"research/behaviour/bh-participants.html#recruitment-through-the-facultys-online-recruitment-system","title":"Recruitment through the faculty's online recruitment system","text":"<p>The Experiment Management System (EMS) of the faculty is an online platform used to facilitate the management of experimental research and the recruitment of participants. As it is managed by Sona Systems, it is often referred to with this name as well. It can be accessed through this link. Here is some general info about the system:</p> <ul> <li>The EMS can be used to schedule the experiment, set time slots, and easily manage the availability of participants. It can also facilitate communication between researchers and participants, such as sending reminders for scheduled experiments or sharing important information. The system provides a clear overview of available experiments, the status of participation, and accredited course credits.</li> <li>All first-year psychology students can participate in experimental research by signing up through the EMS as part of their research methods course (typically between mid October and the end of May and in July).</li> <li>A credit system is used in which they can earn course credits (18 research participation points) by participating in collective (max. 12 points) and/or individual testing sessions.</li> <li>Students can also opt to do an alternative assignment to earn 2 course credits (i.e., writing a 1-2 page description of the participant experience from the methods section of a recent empirical paper). In this way, students can earn the same course credits as they would for participating in an experiment. This option is provided to ensure voluntariness of participation at all times.</li> <li>In addition, both students and non-students can sign up through the EMS for paid studies throughout the entire year.</li> </ul> <p>PPW-affiliated researchers (you!) can offer their (on-site as well as online) experiments on the platform through a researcher account:</p> <ul> <li>To request such an account, send an email to ioco@kuleuven.be, and provide the following information: [firstname], [lastname], [u-number]@kuleuven.be, [firstname].[lastname]@kuleuven.be).</li> <li>Master's thesis students supervised by PPW staff are also allowed to access the participant pool, but supervisors need to request an account on their behalf.</li> <li>Participants can create an account themselves by following the instructions on the website of the EMS.</li> </ul> <p>If you or your participants have any questions or problems related to the EMS, please check out these Youtube tutorials (for participants, for researchers) and/or rely on peer support (fellow students/researchers) before addressing the system administrator at ioco@kuleuven.be.  </p> <p>Recruitment Bias</p> <p>It is important to consider potential recruitment biases when using the EMS. Since the pool primarily draws from university students, particularly first-year psychology students, the sample may not be fully representative of the general population. Additionally, there is often a higher proportion of female students in our faculty, which can introduce gender bias into the research findings. You should take this into account when designing studies and interpreting results when drawing from this participant pool.</p>"},{"location":"research/behaviour/bh-participants.html#create-a-new-experiment-on-the-platform","title":"Create a new experiment on the platform","text":"<p>As students rely on these experiments to collect the necessary research participation points for their research methods course (pass/fall evaluation, 1 ECTS), make sure to think about inclusivity and accessibility. Always specify whether the study location is wheelchair accessible and if students with disabilities can participate (e.g., motor, visual impairments).</p> <p>To add a new study, provide the following information (for more detailed explanations, please watch this):</p> <ol> <li>Study type: In case the experiment is online only, choose \"Online External Study\"</li> <li>Choose between a paid or credit study (in case both are options, \"credit\" must be selected)</li> <li> <p>Study information:</p> <ul> <li>Study name: Include the type of compensation in the title</li> <li>Brief abstract</li> <li>Detailed description</li> <li>Eligibility requirements: Can be linked to the pre-screen participants fill out when signing up</li> <li>Session duration</li> <li>Credits: The total credit cap (= a technical matter on PI basis) can be set at 999 (if this is reached, email Tom Beckers to increase it)</li> <li>Preparation: What to bring and/or how to prepare</li> <li>Researcher &amp; PI details</li> <li>SMEC or EC approval number and expiration date</li> <li>Approval status: The study needs to be approved by the system administrator when it's added</li> <li>Activity status: Switch to inactive when you are not actively recruiting, toggle back when you resume</li> </ul> </li> <li> <p>Advanced settings (optional):</p> <ul> <li>Pre-requisite and disqualifier studies</li> <li>Course restrictions, select either</li> <li>Payed volunteer (\"betaalde vrijwilliger\"), to make sure participants expect payment, or</li> <li>The ongoing methods course, to make sure participants expect course credit.</li> <li>Age restrictions</li> <li>Be careful with restrictions, think about inclusivity (be as inclusive as possible)</li> <li>Ensure first-years are eligible by putting the lower age limit at 16 in the ethical dossier</li> <li>Study invitation code</li> <li>Study URL in case of web-based studies</li> <li>Participant sign-up and cancellation deadlines</li> <li>The cancellation deadline must have at least the same duration as the sign-up deadline</li> <li>You cannot cancel participation before you sign up</li> <li>Enable automated e-mails</li> <li>Assign timeslots to a specific researcher</li> <li>Automatic credit granting</li> <li>Frequency of participation</li> <li>Shared and private comments</li> </ul> </li> <li> <p>Add timeslots for your experiment: For online studies, the timeslot is the participation deadline</p> </li> </ol> <p>Make sure to prepare your study carefully before asking for EMS approval, as some changes will require re-approval. To ask for approval, click the \"asking approval\" button. You can start preparing your study in EMS before obtaining ethics approval, but you cannot request EMS approval until you obtain formal ethical approval. Once the latter is obtained, approval in EMS will take 10 days at the very most (usually less than a few days; sending a polite reminder after a few days is fine).</p> <p>Tip</p> <p>The Sona Mobile app can be used to facilitate experiment scheduling (by adding/removing time slots as needed), but not to set up your study.</p>"},{"location":"research/behaviour/bh-participants.html#collective-testing-sessions","title":"Collective testing sessions","text":"<ul> <li>Students can earn up to 12 research participation credits per academic year by participating in collective testing sessions.</li> <li>For such sessions, researchers have to make all necessary arrangements themselves (e.g., rooms, equipment, setting up the study in the EMS, credit granting).</li> <li>In the EMS, you can create collective sessions by allowing multiple participants to access the same time slot.</li> <li>Students are encouraged to participate, but of course this is not mandatory, so you can not imply in your announcement that they have to participate.</li> <li>An overarching informed consent is provided through the EMS as a condition for signing up to a collective session, to allow data pooling across different collective testing sessions. However, individual consent is still necessary for each specific session, along with separate ethics approval.</li> </ul>"},{"location":"research/behaviour/bh-participants.html#screener","title":"Screener","text":"<ul> <li>Upon getting an account, participants need to fill out a pre-screener. In this screener, they are asked to indicate their handedness, gender, vision status, age, availability in weekends and evenings, fluency in Dutch and English and the last digit of their student number (for random assignment).</li> <li>While setting up your experiment, you can set a few pre-screen restrictions to restrict the visibility of your study to participants who fail to meet those restrictions. Vice versa, you can also choose to send an automated email to all students who qualify for the pre-screen restrictions specific to your study.</li> <li>If you want to see more items added to the pre-screen, you can send an email to ioco@kuleuven.be and/or Tom Beckers.</li> </ul>"},{"location":"research/behaviour/bh-participants.html#participant-consent","title":"Participant consent","text":"<p>Before starting the experiment, participants need to be fully informed about the study via a study information sheet, after which they need to sign the informed consent form. In these forms, it is important to make clear that they can withdraw from the study at any time and to provide contact details to give them the opportunity to ask questions (before and/or after the experiment).</p> <ul> <li>In case of on-site experiments, the information sheet as well as the informed consent can be presented (and signed) on paper.</li> <li>In case of online experiments, the information sheet as well as the informed consent can be presented digitally via the EMS. Instead of signing the ICF, participants can then explicitly select whether they agree or not agree to participate.</li> </ul> <p>After participation or after the study is finished, you can choose to debrief participants orally or via mail about the (general) study results and/or discuss their individual task performance during the experimental session.</p>"},{"location":"research/behaviour/bh-participants.html#data-confidentiality","title":"Data confidentiality","text":"<p>Generally, except for gender and age, personal information does not need to be recorded. As all participants receive a system-assigned ID code (EMS code) to log in to the experiment, their participation and research data can remain fully anonymous. Also external people can use the EMS to ensure confidentiality.</p> <ul> <li>To avoid that a link is created between the EMS code and participant identity, please make sure to never ask for any identifying information such as their name or student number. This information will also never be provided by the administrator.</li> <li>Of course, if a person participates in a paid study, you will need to ask for their name, address, email and bank account number to process the payment. This communication can occur through the EMS, such that this information is not shared on any of the online platforms.</li> <li>Likewise, if a student wants to receive an update about the study results after the study is finished, they will need to provide their email address. These personal data should be saved in a password-protected file that will be stored as long as the study is ongoing, but deleted after publication of the research data.</li> </ul>"},{"location":"research/behaviour/bh-participants.html#reimbursement","title":"Reimbursement","text":"<p>Reimbursement can take the following forms:</p> <ul> <li>Credit study: Research participation credits must be assigned in multiples of 0.25, i.e., 0.25 credit per 15 minutes, such that 1h equals 1 credit. Credits cannot be transferred between academic years.</li> <li>Paid study: The standard pay rate is up to 10 euros per hour, without additional incentives. Exceptions are possible for high-effort or aversive studies (e.g., fMRI, ESM).</li> <li>Mixed study: Award 0 credits to those who receive payment.</li> </ul> <p>Of course, all (psychology) students also benefit from participation by gathering insights into the common procedures of psychological research (wink).</p> <p>Please record participant presence (\"show-ups\") and assign credits or pay participants promptly (preferably the same day and always within 1 week). To do so, you will need to collect the student's EMS code.</p> <p>Tip</p> <p>The Sona Mobile app can be used for quick credit granting by scanning the QR code in the reminder email to the participants.</p>"},{"location":"research/behaviour/bh-participants.html#no-shows","title":"No shows","text":"<ul> <li>Participants should contact the researcher if they cannot attend the session, which qualifies as an excused no-show.</li> <li>Unexcused no-shows incur a penalty of -0.5 credit. If a participant has 5 or more no-shows, their account will be inactivated.</li> </ul>"},{"location":"research/behaviour/bh-tasks.html","title":"Behavioural task scripts","text":"<p>This page provides links to external resources and repositories for scripts and templates to design and run behavioural experiments. These templates serve as starting points for developing tasks used in Hoplab.</p> <p>Templates are organized into two sections:  </p> <ul> <li>On-site experiments: Designed for in-person, on-site testing. Can be adapted for online platforms if needed.  </li> <li>Online experiments: Optimized for deployment on platforms like Pavlovia, using jsPsych or PsychoPy.</li> </ul>"},{"location":"research/behaviour/bh-tasks.html#on-site-experiments","title":"On-site experiments","text":"<ul> <li> <p> Inverse MDS task</p> <p>Visual similarity multi-arrangement task, using inverse MDS.</p> <p>See Bracci et al., 2015</p> <p> Learn more</p> </li> <li> <p> Mouse tracker task </p> <p>An example of on-site mouse tracker coded with Psychopy.</p> <p> Learn more</p> </li> </ul>"},{"location":"research/behaviour/bh-tasks.html#online-experiments","title":"Online experiments","text":"<ul> <li> <p> Mouse tracker task </p> <p>A structured template for designing mouse tracker experiments using jsPsych.</p> <p>See Koenig-Robert et al., 2024</p> <p> Learn more</p> </li> <li> <p> Classification task </p> <p>A template for online categorization tasks using jsPsych, featuring an animacy task example.</p> <p> Learn more</p> </li> </ul>"},{"location":"research/behaviour/experimental-setup/index.html","title":"Experimental setup for behavioural studies","text":"<p>Here, you'll find resources and instructions for designing and conducting both online and in-person behavioral experiments. Whether you're setting up a study on Pavlovia or preparing an in-person session, you'll find useful information to get started.</p> <ul> <li> <p> Online experiments with Psychopy</p> <p>Instructions for designing and deploying online behavioral experiments designed with Psychopy on Pavlovia, including steps for setup, participant interaction, and data management.</p> <p> View Psychopy guide</p> </li> <li> <p> Online experiments with jsPsych</p> <p>Instructions for designing and deploying online behavioral experiments designed with jsPsych on Pavlovia. Optimal workflow, testing and publishing, including links to relevant resources.</p> <p> View jsPsych guide</p> </li> <li> <p> In-person experiments</p> <p>Information on conducting in-person behavioral studies, from setting up equipment to managing participants and data collection.  </p> <p> [PLACEHOLDER]</p> </li> </ul>"},{"location":"research/behaviour/experimental-setup/bh-onsite.html","title":"On-site experiments","text":""},{"location":"research/behaviour/experimental-setup/pavlovia-jspsych.html","title":"Online experiments with jsPsych","text":"<p>This page guides you through setting up, managing, and troubleshooting experiments on Pavlovia using the jsPsych framework. Follow these steps and best practices to streamline your experiment creation and minimize errors.</p> <p>For ready-to-use example, please refer to the Behavioural tasks page.</p>"},{"location":"research/behaviour/experimental-setup/pavlovia-jspsych.html#learning-resources","title":"Learning resources","text":"<p>If you're unfamiliar with jsPsych, it's a good idea to start by learning some basics. </p> <p>There is plenty of documentation on the main jsPsych page. In particular, go through the tutorial section, which contains some very accessible, step-by-step instructions on how to build your script. </p> <p>You can also check out the content from Christophe Bossens' workshop on online experiments with jsPsych.</p>"},{"location":"research/behaviour/experimental-setup/pavlovia-jspsych.html#the-ideal-workflow","title":"The ideal workflow","text":"<p>JsPsych offers a framework to write your experiment in javascript. While this might sound like extra complexity, as compared to using the Psychopy builder or to scripting your experiment in Python, it offers the significant advantage of being directly readable by any browser. The code for your experiment will be written in the <code>&lt;script&gt;</code> part of a regular <code>html</code> document, which can then be opened like any other webpage.</p> <p>A basic workflow that you might want to adopt when scripting your experiment is the following:</p> <ol> <li> <p>Open your experiment folder in your editor of choice and create a new <code>index.html</code> file. Build your javascript code and test it locally by running it in your browser.</p> <p>Tip</p> <p>Since you'll be editing javascript, html, css, but also probably some python code at the same time, it's probably a good idea to use an editor that can handle all of these. A great option is VSCode.</p> </li> <li> <p>Once your experiment is well built and running locally, upload it to GitLab, which you can access through your Pavlovia account (Dashboard &gt; Profile &gt; click on your account name). You can choose to do this on your own account or the lab account. Once your experiment is created on there, it will show on your Pavlovia dashboard.</p> </li> <li> <p>From there, work on your experiment like you would work on any Git repository: make changes locally and sync your changes with GitLab. Start by adding the necessary components for your script to run on Pavlovia. These will allow your script to communicate with the Pavlovia servers, without which your experiment can't run from Pavlovia.</p> <p>Using the lab account</p> <p>To run experiments beyond piloting, you may need credits. We have a Hoplab account for this purpose. Ask Klara or Silke how to get access to it.</p> </li> <li> <p>Finish refining your code by repeating this process: make changes locally &gt; sync them with GitLab &gt; try your task on Pavlovia (switch to Piloting or Running to be able to try your task).</p> <p>Tip</p> <p>It can sometimes be cumbersome to go through the complete local change &gt; commit &gt; test loop just to test out a minor code change. An elegant alternative is to use flags in your code that will activate or de-activate the Pavlovia components. The latter are just two: a <code>init</code> and a <code>finish</code> event. Set these behind an <code>if</code> statement, and you'll be able to switch from online to local with one flag, so that you can go back to trying your code locally before syncing your changes (see an example here).</p> </li> </ol>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html","title":"Online experiments with Psychopy","text":"<p>This page guides you through setting up, managing, and troubleshooting experiments on Pavlovia using the Psychopy Builder. Follow these steps and best practices to streamline your experiment creation and minimize errors.</p>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#uploading-to-pavlovia","title":"Uploading to Pavlovia","text":"<p>Follow the steps below to successfully upload your experiment to Pavlovia.</p> <ol> <li>Create the experiment in Psychopy Builder.</li> <li>Create a Pavlovia account and log in.</li> <li>Link your account to the Builder via this button </li> <li>Place all files for the experiment in a single directory.</li> <li>Click the Syncing Globe  in the Builder, enter a project name, and upload. The icon will turn green when the upload is complete.</li> <li>Access your experiment on Pavlovia by navigating to Dashboard &gt; Experiments in your account.</li> <li>To test the project, change the status to piloting and click pilot.</li> </ol> <p></p> <p>Tip</p> <p>If the experiment doesn\u2019t run, click view code. If the repository is empty, retry the sync from the Builder.</p> <p>Running experiments with lab credits</p> <p>To run experiments beyond piloting, you may need credits. We have a Hoplab account for this purpose. Ask Klara or Silke how to get access to it.</p> <p>More information can be found on the PsychoPy website on this page and this page.</p>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#avoiding-errors-in-pavlovia","title":"Avoiding Errors in Pavlovia","text":"<p>Here are some common pitfalls and best practices to prevent errors when setting up experiments.</p>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#excel-files","title":"Excel files","text":"<p>To ensure compatibility, follow these conventions:</p> <ul> <li>No empty columns or rows: Remove any extra spaces or blanks in your spreadsheet.</li> <li>Unique column names: Every column needs a unique name, and the top row should not have empty cells.</li> <li>Avoid special characters in the text fields.</li> <li>Save as CSV: Convert your Excel files to CSV format before using them in Pavlovia.</li> </ul>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#writing-custom-code","title":"Writing custom code","text":"<p>Add custom code through the Components &gt; Custom &gt; Code tab in the Builder, and do not add code directly in Coder.</p> <p>For phase-specific code snippets, add them to the appropriate section in the Code Component (e.g., <code>Begin Experiment</code>, <code>Begin Routine</code>, <code>Each Frame</code>).</p> <p>Ensure that Python code is entered in the box on the left and JavaScript code (or both) in the box on the right. Use the <code>Code Type</code> option to specify the language.</p> <p>Consult the Psychopy Python-to-JavaScript crib sheet for additional guidance.</p>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#manual-translation-from-python-to-javascript","title":"Manual translation from Python to JavaScript","text":"<p>Some Python code does not automatically translate to JavaScript. Generally, automatic translation to JS works well if you configure everything in the Builder\u2019s GUI. However, directly coding or modifying code snippets can sometimes lead to translation failures, particularly if you need to:</p> <ul> <li>Track variables over time, such as adapting later trials based on earlier ones.</li> <li>Introduce breaks after a specific number of trials.</li> </ul> <p>For more on these issues, see this thread on Pavlovia troubleshooting.</p> <p>You may need to edit translations manually by setting the top-right <code>Code Type</code> box to Both.</p> <p></p> <p>This setting allows you to modify both the left (Python) and right (JavaScript) code boxes independently, without affecting the other.</p>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#examples-tracking-trials-and-commands-for-translation","title":"Examples: Tracking trials and commands for translation","text":"<p>If you want to track a fixed number of trials, you can create a list and remove the oldest trial entry once the list length exceeds your desired count. For instance, to track responses for the last 20 trials, create a list of responses and use a different method in each language:</p> <ul> <li>Python: <code>.pop(0)</code></li> <li>JavaScript: <code>.shift()</code></li> </ul> <p>To stop a loop, JavaScript does not recognize specific loop names. For instance, if stopping a loop named <code>preparation</code>, this will translate as <code>preparation.finished</code> in JavaScript. Change this in the right box to <code>trials.finished</code>.</p> <ul> <li>Stopping Nested Loops: <code>trials.finished</code> only stops the inner loop. To stop an outer loop (e.g., <code>PreparationBlock</code>), you may need a different solution, as stopping nested loops is limited in Pavlovia.</li> </ul> <p></p>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#define-common-commands-at-the-start-of-the-experiment","title":"Define common commands at the start of the experiment","text":"<p>For Python commands that do not exist in JavaScript, define them at the beginning of the experiment in a JS-only code chunk to simplify translation. Note that some of these commands may still need manual translation per code chunk.</p> <p>Some manual definitions:</p> <pre><code>// The experiment identifier\nthisExp=psychoJS.experiment;\n\n// Psychopy window\nwin=psychoJS.window;\n\n// Event manager of Psychopy\nevent=psychoJS.eventManager;\n\n// Shuffle command\nshuffle = util.shuffle;\n\n// Sorting array elements and returning a sorted list\nsort = function(array) {\n    return array.sort();\n};\n\n// Appending elements to a list\nArray.prototype.append = [].push;\n\n// Getting the index of an element in a list\nArray.prototype.index = [].indexOf;\n\n// Summing elements in a list\nsum = function(arr) {\n    return arr.reduce((a, b) =&gt; a + b);\n};\n\n// Counting occurrences of a value in a list\nArray.prototype.count = function(value) {\n    let count = 0;\n    this.forEach(item =&gt; {\n        if (item === value) count++;\n    });\n    return count;\n};\n</code></pre>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#using-movie-stimuli","title":"Using movie stimuli","text":"<p>Pavlovia supports standard video formats, such as .mp4. If your videos are in a different format, convert them using free software like Handbrake.</p>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#setting-up-videos-in-psychopy-builder","title":"Setting up videos in Psychopy Builder","text":"<ul> <li>Set the video to play every repeat in the Builder. If videos need to repeat, Pavlovia will only display the last frame unless reset.</li> <li>To reset the video between trials, use a code snippet. For details, check the Psychopy community guide and this link.</li> </ul> <p>Video troubleshooting</p> <p>If your videos don\u2019t display, try adjusting the Units setting to <code>pix</code> or another compatible format.</p>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#rest-trials","title":"Rest trials","text":"<p>To introduce rest trials or breaks, add a code snippet under Each Frame. For specific approaches, refer to this guide and this page.</p>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#text-stimuli","title":"Text stimuli","text":""},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#adjusting-text-for-screen-display","title":"Adjusting text for screen display","text":"<ul> <li>WrapWidth: If the text doesn\u2019t fit on the screen, increase the WrapWidth under the Advanced tab. WrapWidth defines the maximum width at which text wraps to a new line.</li> <li> <p>Centering text: To center text, add the following JavaScript code snippet in a JS-only code chunk:</p> <p><pre><code>name_text.setAlignHoriz('center');\n</code></pre> This will center the text online; however, offline functionality may be impacted if the code isn\u2019t set to JS-only.</p> </li> </ul>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#informed-consent","title":"Informed consent","text":"<p>For setting up informed consent forms for online experiments, refer to this guide on informed consent. </p>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#images","title":"Images","text":"<p>Make sure all image files are in the html &gt; Resources directory. Missing files will cause a resource error on Pavlovia.</p> <p>To ensure consistent display sizes across different screens, follow these image guidelines.</p>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#language-selection-and-conditional-loops","title":"Language selection and conditional loops","text":""},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#implementing-language-selection","title":"Implementing language selection","text":"<ol> <li>Place the code component in a separate routine from the key press defining the selection.</li> <li>To create conditional loops based on key press selection, refer to this guide.</li> </ol> <p>Example for language selection (e.g., \u2018e\u2019 for English, \u2018n\u2019 for Dutch):</p> <ul> <li>Store the chosen key in a variable.</li> <li>Add a code component, set <code>nReps</code> to a variable based on the selection, and configure the loop accordingly.</li> </ul> <p> </p>"},{"location":"research/behaviour/experimental-setup/pavlovia-psychopy.html#cellphone-detection","title":"Cellphone detection","text":"<p>To prevent participants from accessing the experiment on mobile devices, use this JS-only code snippet at the start of the experiment:</p> <pre><code>if (/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)) {\n    quitPsychoJS('Mobile device detected. Goodbye!', false);\n}\n</code></pre>"},{"location":"research/coding/index.html","title":"Coding practices","text":"<p>Welcome to the Coding Practices section! Here, you'll find essential guidance for setting up your coding environment, managing projects, collaborating using GitHub, and following best practices to write clean, maintainable code.</p> <ul> <li> <p> Good coding practices   In the lab, we aim to build tools that are reproducible, reusable, and efficient. Learn more about our general approach to building and managing projects.</p> </li> <li> <p> Setting up your project   Every new project starts with the right environment setup. Find out how to create a proper environment for your coding projects.</p> </li> <li> <p> Understanding your code   Encountering inexplicable errors? Want to know what data are you plotting? Don't know how to use a misterious function? Learn how to effectively debug your code using Spyder\u2019s powerful tools.</p> </li> <li> <p> Using version control   Discover how to integrate Git and GitHub into your workflow to keep track of changes and collaborate with ease.</p> </li> </ul>"},{"location":"research/coding/index.html#why-coding-practices-matter","title":"Why coding practices matter","text":"<p>When you code for your research project, remember that you're not just coding for yourself today\u2014you\u2019re coding for:</p> <ul> <li>Your future self: Six months from now, you might not remember the specifics of your current project.</li> <li>Other scientists: Your code might be used or reviewed by researchers with varying coding skills and backgrounds. Writing clean and well-documented code ensures that your work can be understood and built upon by others.</li> </ul> <p>Keeping your code tidy, easy to understand, and maintainable is crucial for effective research collaboration and aligns with the principles of Open Science.</p>"},{"location":"research/coding/index.html#recommended-resources","title":"Recommended resources","text":"<p>Make sure to explore our suggested Coding tutorials. We especially recommend The good research code handbook, which provides valuable insights into writing robust research code. Key sections include Writing decoupled code and Keeping things tidy.</p> <p>By following these practices, you'll not only improve the quality of your research code but also make it easier to share your work with others, enhancing transparency and reproducibility. Invest time in reading and practicing. Developing good coding habits will pay off in the long run by making your work more efficient, easier to understand, and more valuable to the research community. Happy coding!</p> <p>Tip</p> <p>If you're new to coding and many of the terms on this page seem unfamiliar, start by exploring some of the essential tools you\u2019ll use. Check out tutorials on Python, Git, and the Unix Shell on the Student starter pack page.</p> <p>What if I code in MATLAB?</p> <p>While the information in this page focuses on Python, the principles of writing clean, maintainable code are universal. Debugging, structuring code, and organizing projects apply just as much to MATLAB as they do to Python. Be sure to apply these practices regardless of the language you're using!</p>"},{"location":"research/coding/index.html#special-note-for-fmri-projects","title":"Special note for fMRI projects","text":"<p>If you're working on fMRI projects, you\u2019ll find specific information on setting up your environment in the Set-up your environment page of the fMRI section. This guide includes additional tips for managing data and code in neuroimaging research.</p>"},{"location":"research/coding/index.html#best-practices-for-organizing-code-and-projects","title":"Best practices for organizing code and projects","text":"<p>A well-structured project helps in maintaining readability and collaboration. Here are some recommendations:</p>"},{"location":"research/coding/index.html#1-folder-structure","title":"1. Folder structure","text":"<p>Use a logical structure for your project files:</p> <pre><code>my_project/\n\u251c\u2500\u2500 data/              # Raw data files\n\u251c\u2500\u2500 modules/           # Scripts to store your classes and functions\n\u251c\u2500\u2500 results/           # Output results and figures\n\u251c\u2500\u2500 environment.yml    # Conda environment file\n\u2514\u2500\u2500 README.md          # Project overview\n</code></pre>"},{"location":"research/coding/index.html#2-naming-conventions","title":"2. Naming conventions","text":"<ul> <li>Files: Use lowercase letters with underscores (e.g., <code>data_processing.py</code>).</li> <li>Folders: Use meaningful names that reflect their contents.</li> <li>Variables: Use descriptive names (e.g., <code>participant_id</code> instead of <code>id</code>).</li> </ul>"},{"location":"research/coding/index.html#3-general-coding-tips","title":"3. General coding tips","text":"<p>Tip</p> <p>Write modular code by breaking down tasks into functions and classes. This approach enhances reusability and readability.</p> <ul> <li>Avoid \"spaghetti code\": Keep functions short and focused.</li> <li> <p>Use Docstrings to document functions and classes:</p> <pre><code>def load_data(file_path):\n    \"\"\"\n    Loads data from a specified file path.\n\n    Args:\n        file_path (str): The path to the data file.\n\n    Returns:\n        pandas.DataFrame: Loaded data as a DataFrame.\n    \"\"\"\n</code></pre> </li> <li> <p>Follow PEP 8: Use tools like <code>black</code> to ensure code style compliance.</p> </li> </ul>"},{"location":"research/coding/index.html#4-saving-results","title":"4. Saving results","text":"<p>Organizing your results properly is crucial for reproducibility, collaboration, and long-term maintainability of your research code. This section covers how to structure your results folders, save scripts and logs, and use utility functions to streamline these processes.</p> <p>To keep your project organized, we\u2019ve provided a set of utility functions that automate common tasks like setting random seeds, creating unique output directories, saving scripts, and configuring logging. These functions should be defined in a separate file called <code>utils.py</code> located in the <code>modules/</code> directory of your project.</p> Utility functions in modules/utils.py <p>The following functions are defined in <code>modules/utils.py</code> (see the box below for the definitions):</p> <ul> <li><code>set_random_seeds(seed=42)</code>: Sets random seeds for reproducibility.</li> <li><code>create_run_id()</code>: Generates a unique identifier based on the current date and time.</li> <li><code>create_output_directory(directory_path)</code>: Creates a directory for saving results.</li> <li><code>save_script_to_file(output_directory)</code>: Saves the executing script to the output directory.</li> <li><code>setup_logger(log_file_path, level=logging.INFO)</code>: Configures logging to log both to the console and a file.</li> </ul> modules/utils.py<pre><code># ./modules/utils.py\nimport logging\nimport os\nimport shutil\nimport random\nimport torch\nimport numpy as np\nimport inspect\nfrom datetime import datetime\n\ndef set_random_seeds(seed=42):\n    \"\"\"\n    Set the random seed for reproducibility in PyTorch, NumPy, and Python's random module.\n\n    This function sets the seed for random number generation in PyTorch, NumPy, and Python's built-in random module.\n    It also configures PyTorch to use deterministic algorithms and disables the benchmark mode for convolutional layers\n    when CUDA is available, to ensure reproducibility.\n\n    :param seed: The random seed. Defaults to 42.\n    :type seed: int\n    \"\"\"\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.set_default_dtype(torch.float32)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\ndef create_run_id():\n    \"\"\"\n    Generate a unique run identifier based on the current date and time.\n\n    This function creates a string representing the current date and time in the format 'YYYYMMDD-HHMMSS'.\n    It can be used to create unique identifiers for different runs or experiments.\n\n    :returns: A string representing the current date and time.\n    :rtype: str\n    \"\"\"\n    now = datetime.now()\n    return now.strftime(\"%Y%m%d-%H%M%S\")\n\ndef create_output_directory(directory_path):\n    \"\"\"\n    Creates an output directory at the specified path.\n\n    This function attempts to create a directory at the given path.\n    It logs the process, indicating whether the directory creation was successful or if any error occurred.\n    If the directory already exists, it will not be created again, and this will also be logged.\n\n    :param directory_path: The path where the output directory will be created.\n    :type directory_path: str\n    \"\"\"\n    try:\n        logging.info(f\"Attempting to create output directory at: {directory_path}\")\n        if not os.path.exists(directory_path):\n            os.makedirs(directory_path)\n            logging.info(\"Output directory created successfully.\")\n        else:\n            logging.info(\"Output directory already exists.\")\n    except Exception as e:\n        logging.error(f\"An error occurred while creating the output directory: {e}\", exc_info=True)\n\ndef save_script_to_file(output_directory):\n    \"\"\"\n    Saves the script file that is calling this function to the specified output directory.\n\n    This function automatically detects the script file that is executing this function\n    and creates a copy of it in the output directory.\n    It logs the process, indicating whether the saving was successful or if any error occurred.\n\n    :param output_directory: The directory where the script file will be saved.\n    :type output_directory: str\n    \"\"\"\n    try:\n        # Get the frame of the caller to this function\n        caller_frame = inspect.stack()[1]\n        # Get the file name of the script that called this function\n        script_file = caller_frame.filename\n\n        # Construct the output file paths\n        script_file_out = os.path.join(output_directory, os.path.basename(script_file))\n\n        logging.info(f\"Attempting to save the script file to: {script_file_out}\")\n\n        # Copy the script and additional files to the output directory\n        shutil.copy(script_file, script_file_out)\n\n        logging.info(\"Script files saved successfully.\")\n    except Exception as e:\n        logging.error(f\"An error occurred while saving the script file: {e}\", exc_info=True)\n\ndef setup_logger(log_file_path=None, level=logging.INFO):\n    \"\"\"\n    Set up a logger with both console and file handlers.\n\n    :param log_file_path: The path for the log file. If None, only console logging is enabled.\n    :type log_file_path: str, optional\n    :param level: Logging level (e.g., logging.INFO, logging.DEBUG).\n    :type level: int\n    :return: Configured logger.\n    :rtype: logging.Logger\n    \"\"\"\n    # Create a logger\n    logger = logging.getLogger(__name__)\n    logger.setLevel(level)\n\n    # Create a formatter for logs\n    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n\n    # Create a stream handler (for console output)\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n\n    # If a log file path is provided, add a file handler\n    if log_file_path:\n        file_handler = logging.FileHandler(log_file_path)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n    # Prevent adding duplicate handlers\n    logger.propagate = False\n\n    return logger\n</code></pre> Using the utility functions in a script <p>To use the functions defined in <code>utils.py</code>, import them in your script and follow the example below. This will ensure reproducibility and proper organization of your experimental results.</p> main.py<pre><code>import os\nfrom modules.utils import (\n    set_random_seeds,\n    create_run_id,\n    create_output_directory,\n    save_script_to_file,\n    setup_logger\n)\n\n# Set random seeds for reproducibility\nset_random_seeds(42)\n\n# Define parameters for the run\nresults_dir = \"./results\"\ndataset_dir = \"./datasets\"\nepochs = 20\nlearning_rate = 1e-5\nbatch_size = 64 * (2**3)\n\nprob_a = 0.2\nprob_b = 0.2\nprob_test = 0.6\ntemperature_model_a = 0.1\ntemperature_model_b = 5\n\n# Create a unique run ID and results directory\nrun_id = f\"{create_run_id()}_train-pair-temp-ws-softmax_proba-{prob_a}_probb-{prob_b}_probtest-{prob_test}_tempa-{temperature_model_a}_tempb-{temperature_model_b}_lr-{learning_rate}\"\nresults_dir = os.path.join(results_dir, run_id)\ncreate_output_directory(results_dir)\n\n# Save the current script to the results directory for reproducibility\nsave_script_to_file(results_dir)\n\n# Set up logging to log both to the console and a file\nlog_file_path = os.path.join(results_dir, \"log_output.txt\")\nlogger = setup_logger(log_file_path)\nlogger.info(f\"Results will be saved in: {results_dir}\")\nlogger.info(\"Run ID: %s\", run_id)\nlogger.info(f\"Starting the experiment with the following parameters:\")\nlogger.info(f\"Learning Rate: {learning_rate}, Epochs: {epochs}, Batch Size: {batch_size}\")\n\n# ... Your training or analysis code here ...\n</code></pre> <p>Example results folder structure</p> <p>After running the script, your results might be structured as follows:</p> <pre><code>results/\n\u251c\u2500\u2500 20241018-153045_train-pair-temp-ws-softmax_proba-0.2_probb-0.2_probtest-0.6_tempa-0.1_tempb-5_lr-1e-5/\n\u2502   \u251c\u2500\u2500 log_output.txt        # Logs of the run\n\u2502   \u251c\u2500\u2500 main_script.py        # Copy of the script that generated the results\n\u2502   \u251c\u2500\u2500 output_data.csv       # Output data generated by the run\n\u2502   \u2514\u2500\u2500 model_weights.pth     # Saved model weights\n</code></pre> <p>Why create a <code>results</code> folder for each run?</p> <ul> <li>Reproducibility: Ensures that each set of results corresponds to a specific code version and parameters.</li> <li>Comparison: Makes it easier to compare results between different runs with varying parameters.</li> <li>Organization: Keeps your project clean by preventing files from different experiments from mixing together.</li> </ul> <p>With these functions, you can ensure a well-organized, reproducible workflow, making it easier to manage long-term research projects and collaborate with others.</p>"},{"location":"research/coding/index.html#setting-up-a-conda-environment","title":"Setting up a Conda environment","text":"<p>Using isolated <code>conda</code> environments ensures that each project has the specific dependencies it needs without conflicts. Follow the steps below to create and manage your environments.</p>"},{"location":"research/coding/index.html#1-install-anacondaminiconda","title":"1. Install Anaconda/Miniconda","text":"<p>Download and install Miniconda or Anaconda.</p> What's the difference? <ul> <li>Miniconda is a minimal version that includes only <code>conda</code> and Python, allowing you to install only the packages you need.</li> <li>Anaconda comes with a full suite of pre-installed packages like <code>numpy</code>, <code>pandas</code>, <code>scipy</code>, and many others, and with a GUI to manage packages and environments.</li> </ul> WindowsMacUbuntu <ul> <li>Download the installer from the Anaconda website.</li> <li>Run the Installer: Double-click the <code>.exe</code> file and follow the installation wizard.</li> <li>Add Conda to PATH: During installation, check the box that says \"Add Anaconda to my PATH environment variable\" if you plan to use <code>conda</code> directly from the command prompt.</li> </ul> <p>Warning</p> <p>Adding Anaconda to PATH can sometimes cause conflicts with other software. Only do this if you are familiar with PATH management.</p> <ul> <li>Download the installer from the Anaconda website.</li> <li>Run the Installer: Open the downloaded <code>.pkg</code> file and follow the installation instructions.</li> <li>Verify Installation:     <pre><code>conda --version\n</code></pre></li> </ul> <p>Tip</p> <p>If you encounter issues with permissions, run the installer with <code>sudo</code>: <pre><code>sudo bash Anaconda3-&lt;version&gt;-MacOSX-x86_64.sh\n</code></pre></p> <ul> <li> <p>Download the installer script from the terminal:     <pre><code>wget https://repo.anaconda.com/archive/Anaconda3-&lt;version&gt;-Linux-x86_64.sh\n</code></pre></p> </li> <li> <p>Run the Installer:     <pre><code>bash Anaconda3-&lt;version&gt;-Linux-x86_64.sh\n</code></pre></p> </li> <li> <p>Follow the prompts: Accept the license terms, specify an installation path, and allow the installer to initialize <code>conda</code>.</p> </li> <li> <p>Activate changes:     <pre><code>source ~/.bashrc\n</code></pre></p> </li> </ul> <p>Info</p> <p>Make sure to replace <code>&lt;version&gt;</code> with the correct version number of the Anaconda installer.</p>"},{"location":"research/coding/index.html#2-create-and-manage-a-conda-environment","title":"2. Create and manage a Conda environment","text":"CLIGUI (Anaconda Navigator) <ol> <li> <p>Create a new environment:    Use the following command to create a new environment. Replace <code>myenv</code> with the name of your environment:    <pre><code>conda create --name myenv python=3.9\n</code></pre></p> </li> <li> <p>Activate the environment:    <pre><code>conda activate myenv\n</code></pre></p> </li> <li> <p>Install packages:    Install necessary packages, e.g., <code>numpy</code>, <code>pandas</code>, and <code>matplotlib</code>:    <pre><code>conda install numpy pandas matplotlib\n</code></pre></p> </li> <li> <p>Export environment for reproducibility:    Save your environment to a file:    <pre><code>conda env export &gt; environment.yml\n</code></pre>    This allows others to recreate your environment with:    <pre><code>conda env create -f environment.yml\n</code></pre></p> </li> </ol> <ol> <li> <p>Open Anaconda Navigator: Launch the Anaconda Navigator from your start menu.</p> </li> <li> <p>Create a new environment:</p> <ul> <li>Go to the \"Environments\" tab.</li> <li>Click on \"Create\" and give your environment a name (e.g., <code>myenv</code>).</li> <li>Select the desired Python version.</li> </ul> </li> <li> <p>Install packages:</p> <ul> <li>With your environment selected, click on \"Not installed\" to view available packages.</li> <li>Search for the packages (e.g., <code>numpy</code>, <code>pandas</code>) and install them by checking the boxes and clicking \"Apply\".</li> </ul> </li> </ol>"},{"location":"research/coding/index.html#setting-up-spyder-for-python-projects","title":"Setting up Spyder for Python projects","text":"<p>Spyder is a powerful IDE for scientific programming in Python. Here\u2019s how to set it up:</p>"},{"location":"research/coding/index.html#1-install-spyder","title":"1. Install Spyder","text":"Using Conda (Recommended)Using Anaconda Navigator <pre><code>conda install spyder\n</code></pre> <ul> <li>Open Anaconda Navigator.</li> <li>Find Spyder in the \"Home\" tab and click \"Install\".</li> </ul>"},{"location":"research/coding/index.html#2-create-a-project-in-spyder","title":"2. Create a project in Spyder","text":"Why use Spyder projects? <p>Using a project allows Spyder to set the root folder for your scripts. This means that all imports and file paths are relative to this root, simplifying package management and file organization.</p> <ol> <li> <p>Create a New Project:</p> <ul> <li>Go to <code>Projects &gt; New Project</code> in Spyder.</li> <li>Select a directory to store your project files.</li> <li>Spyder will set this folder as the root for relative imports.</li> </ul> </li> <li> <p>Organize your project:</p> <ul> <li>Use a structure like this:</li> </ul> <pre><code>my_project/\n\u251c\u2500\u2500 data/              # Raw data files\n\u251c\u2500\u2500 modules/           # Scripts to store your classes and functions\n\u251c\u2500\u2500 results/           # Output results and figures\n\u251c\u2500\u2500 environment.yml    # Conda environment file\n\u2514\u2500\u2500 README.md          # Project overview\n</code></pre> </li> <li> <p>Activate Your Environment in Spyder:</p> <ul> <li>Go to <code>Tools &gt; Preferences &gt; Python Interpreter</code>.</li> <li>Select the interpreter from your <code>conda</code> environment.</li> </ul> </li> </ol>"},{"location":"research/coding/index.html#understanding-your-code","title":"Understanding your code","text":"<p>Spyder offers powerful tools for debugging, understanding, and navigating your code. Here\u2019s an in-depth guide on how to leverage these features, with examples to make each step clear and actionable.</p>"},{"location":"research/coding/index.html#viewing-all-panes-in-spyder","title":"Viewing all panes in Spyder","text":"<p>Before diving into debugging and navigation, it's important to set up your Spyder workspace for maximum efficiency. Spyder's default layout includes several panes that provide valuable insights into your code's execution and structure.</p> <ol> <li> <p>Accessing the view menu:</p> <ul> <li>Go to <code>View &gt; Panes</code> to see a list of available panes.</li> <li>The most useful panes include:<ul> <li>Editor: This is where you write your code.</li> <li>IPython Console: Allows you to run commands interactively.</li> <li>Variable Explorer: Displays all variables in your current environment.</li> <li>Documentation: Shows documentation for selected functions and objects.</li> <li>File Explorer: Browse files and folders in your working directory.</li> </ul> </li> </ul> </li> <li> <p>Enable recommended panes:</p> <ul> <li>Ensure that the Variable Explorer, IPython Console, Breakpoints, and Documentation panes are enabled.</li> <li>This setup will help you keep track of variables, navigate breakpoints, and access function documentation easily.</li> </ul> </li> </ol> <p></p>"},{"location":"research/coding/index.html#understanding-the-code-by-debugging","title":"Understanding the code by debugging","text":"<p>Using breakpoints and Spyder's debugging tools allows you to:</p> <ul> <li>Pause code execution and inspect variables at critical points.</li> <li>Step through code line-by-line to understand how each operation transforms the data.</li> <li>Use the Variable Explorer for a visual overview of complex data structures.</li> <li>Run quick checks in the IPython console for on-the-fly validation.</li> </ul> <p>These tools are crucial for identifying and fixing bugs in your scripts, whether you're working with simple calculations or more complex data processing tasks. By mastering them, you'll save time and gain deeper insights into your code's behavior.</p> <p>Best practices for debugging</p> <ul> <li>Use breakpoints strategically: Place breakpoints at critical points in your code to verify data at those stages.</li> <li>Step through loops: Use \"Step Over\" and \"Step Into\" to see how data changes inside loops.</li> <li>Log important values: If you\u2019re debugging a specific issue, add print statements to log values at various points.</li> </ul> <p>Example scenario: Debugging a simple calculation script</p> <p>Let\u2019s say you have a script that generates some random numbers, processes them by applying a mathematical operation, and then plots the result. You want to ensure that the numbers are correctly generated and processed before they are plotted. Here\u2019s how you can use breakpoints to achieve this:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random data\ndata = np.random.rand(100)\n\n# Process data: apply a mathematical operation\nprocessed_data = data * 2 + 5\n\n# Plot data\nplt.plot(processed_data)\nplt.title('Processed Data')\nplt.show()\n</code></pre>"},{"location":"research/coding/index.html#1-adding-a-breakpoint","title":"1. Adding a breakpoint","text":"<ul> <li>Set a breakpoint on the line where <code>processed_data</code> is calculated by clicking in the left margin next to the line or using:<ul> <li>Windows/Linux: <code>Ctrl + B</code></li> <li>Mac: <code>Cmd + B</code></li> </ul> </li> </ul> <p>The line will be highlighted in red, indicating that the breakpoint is active.</p> <p>Why use this?: This breakpoint allows you to pause before <code>processed_data</code> is calculated, so you can inspect the <code>data</code> values and verify that the generated numbers look as expected before the transformation is applied.</p>"},{"location":"research/coding/index.html#2-running-code-in-debug-mode","title":"2. Running code in debug mode","text":"<p>Start debugging by clicking the \"Debug\" button  in the Spyder toolbar. - The execution will pause when it reaches the breakpoint on <code>processed_data = data * 2 + 5</code>.</p> <ul> <li>Once paused, you can:<ul> <li>Step into a function (<code>Ctrl + F11</code>): This allows you to step inside any function calls to see how they operate internally.</li> <li>Step over (<code>Ctrl + F10</code>): This moves to the next line without diving into the details of function calls\u2014ideal for quickly advancing through simpler lines.</li> <li>Continue (<code>Ctrl + F12</code>): Resumes execution until the next breakpoint or the end of the script.</li> </ul> </li> </ul> <p>Why use this?: Step-by-step execution helps you isolate logical errors or verify how variables change through different stages, especially when debugging a transformation or complex calculation.</p>"},{"location":"research/coding/index.html#3-inspecting-variables-during-debugging","title":"3. Inspecting variables during debugging","text":"<ul> <li> <p>With the code paused at the breakpoint, use the Variable Explorer to examine the contents of <code>data</code>:</p> <ul> <li>Look at the array of generated numbers to ensure they are within the expected range (0 to 1 since <code>np.random.rand()</code> generates random floats).</li> <li>After confirming the raw <code>data</code>, proceed with the next step to see how <code>processed_data</code> changes.</li> </ul> </li> <li> <p>Double-click on <code>data</code> in the Variable Explorer to open a detailed view, allowing you to see the entire array and verify its values.</p> </li> </ul> <p>Why use this?: It allows you to visually inspect the contents of arrays, lists, or other data structures without needing to add print statements. This can be especially useful for quickly understanding the state of your data at different points.</p> <p></p>"},{"location":"research/coding/index.html#4-using-the-console-for-interactive-debugging","title":"4. Using the console for interactive debugging","text":"<ul> <li> <p>While debugging, you can interact with variables directly in the IPython console to verify specific values or perform calculations without modifying the script.</p> </li> <li> <p>Example: To see the first few values of <code>data</code>, type:     <pre><code>print(data[:10])\n</code></pre>     This will print the first 10 values of the <code>data</code> array in the console, allowing you to confirm that the random numbers are as expected.</p> </li> <li> <p>Another Example: Check the shape of <code>data</code> to ensure it has the correct number of elements:     <pre><code>data.shape\n</code></pre></p> </li> </ul> <p>Why use this?: This feature allows you to perform ad-hoc checks on variables or run quick tests without altering your script, which is useful for exploring potential issues during debugging.</p>"},{"location":"research/coding/index.html#understanding-by-looking-at-definitions","title":"Understanding by looking at definitions","text":"<p>Spyder makes it easy to navigate large codebases and understand how functions, classes, and variables are connected. Using features like \"Go to definition,\" \"Find references\", object inspection, and the Documentation Viewer, you can explore and manage complex projects more efficiently.</p> <p>Pro tips for code navigation</p> <ul> <li>Use \"Go to definition\" to trace complex functions: This helps you see the original implementation without scrolling through files.</li> <li>Use the Variable Explorer for quick checks: It\u2019s a faster way to spot-check variables rather than adding numerous print statements.</li> </ul> <p>Overview:</p> <p>The Go to definition feature allows you to quickly jump to where a function, class, or variable is defined. This is especially useful when working with large scripts or when using functions imported from other files or libraries. Instead of scrolling through the code to find a definition, you can directly jump to it.</p> <ul> <li>How to use: Right-click on the function or class name and select \"Go to Definition\" or use the shortcut:</li> <li>Windows/Linux: <code>Ctrl + G</code></li> <li> <p>Mac: <code>Cmd + G</code></p> </li> <li> <p>Why use this?: This feature saves time and makes it easier to understand how a function or class is implemented without losing context in your main script.</p> </li> </ul> <p>Example scenario: Navigating a machine learning pipeline</p> <p>Suppose you have a script with multiple functions for data cleaning, feature extraction, model training, and evaluation. Using \"Go to Definition,\" you can quickly jump between functions to understand the flow of your code.</p> <pre><code>def clean_data(df):\n    # Data cleaning logic\n    return df\n\ndef extract_features(df):\n    # Feature extraction logic\n    return features\n\ndef train_model(features):\n    # Model training logic\n    return model\n\n# Main script\ndata = clean_data(data)\nfeatures = extract_features(data)\nmodel = train_model(features)\n</code></pre> <ul> <li>Scenario: You want to see the logic inside <code>clean_data</code> while working on the main script.<ul> <li>Highlight and right-click on <code>clean_data</code> and select \"Go to Definition.\"</li> <li>Spyder will take you directly to where <code>clean_data</code> is defined, allowing you to review the function without scrolling.</li> </ul> </li> </ul> <p></p>"},{"location":"research/coding/index.html#understanding-by-inspecting","title":"Understanding by inspecting","text":"<p>Overview: Spyder\u2019s object inspection feature allows you to explore the attributes and methods of objects directly within the editor. This is particularly useful when working with unfamiliar libraries or custom classes, as it enables you to see what functions or properties are available and how to use them. This feature can be a lifesaver when you encounter a function with unclear parameters or complex behavior.</p> <ul> <li>How to use: Select an object or function in the editor and press:</li> <li>Windows/Linux: <code>Ctrl + I</code></li> <li> <p>Mac: <code>Cmd + I</code></p> </li> <li> <p>Why use this?: This feature provides a quick way to understand the capabilities and usage of an object or method without needing to look up documentation online. It can save time when learning new libraries or debugging issues with complex data structures.</p> </li> </ul> <p>Example scenario: Inspecting a NumPy function</p> <p>Suppose you want to generate a set of random integers using the <code>np.random.randint</code> function, but you\u2019re not sure about its input arguments and what it returns. You can use Spyder\u2019s object inspection to quickly get this information without leaving the IDE.</p> <pre><code>import numpy as np\n\n# Generate random integers between 0 and 10\nrandom_numbers = np.random.randint(0, 10, size=100)\n</code></pre> <ul> <li> <p>Scenario: You want to know what arguments <code>np.random.randint</code> accepts and how to use it properly (e.g., what is <code>size</code>, and can you generate a 2D array?).</p> </li> <li> <p>Step 1: Select the Function: Highlight <code>np.random.randint</code> in the editor.</p> </li> <li> <p>Step 2: Press the Shortcut: Use <code>Ctrl + I</code> (Windows/Linux) or <code>Cmd + I</code> (Mac) to bring up the documentation in the Help pane.</p> </li> <li> <p>What You See: The documentation for <code>np.random.randint</code> appears, showing:</p> <ul> <li>Input Arguments/Parameters: The range of integers (<code>low</code> and <code>high</code>), <code>size</code> for specifying the shape of the output array, and other optional parameters.</li> <li>Description: An explanation of what the function does\u2014generating random integers within a specified range.</li> <li>Returns: Information on what the function outputs (an array of integers).</li> <li>Examples: If available, code snippets showing how to use the function.</li> </ul> </li> <li> <p>Why use this?: This allows you to quickly understand how to use <code>np.random.randint</code> without having to search online. You can verify if the function supports multi-dimensional arrays by checking the <code>size</code> parameter.</p> </li> </ul>"},{"location":"research/coding/index.html#version-control-with-git-and-github","title":"Version control with Git and GitHub","text":"<p>Version control is crucial for collaborative coding and tracking changes in your projects. Here\u2019s how to set up and use Git and GitHub, including practical tips for effective collaboration.</p> <p>For a beginner-friendly guide, with explanation on main steps and terminology see this page.</p>"},{"location":"research/coding/index.html#1-install-git","title":"1. Install Git","text":"WindowsMacUbuntu <ul> <li>Download the installer from the Git website.</li> <li>Follow the installation wizard, using default options.</li> </ul> <ul> <li>Install via Homebrew:     <pre><code>brew install git\n</code></pre></li> <li>Alternatively, download the Git installer.</li> </ul> <pre><code>sudo apt-get update\nsudo apt-get install git\n</code></pre>"},{"location":"research/coding/index.html#2-configure-git","title":"2. Configure Git","text":"<p>Set up your Git identity using the following commands:</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"youremail@example.com\"\n</code></pre>"},{"location":"research/coding/index.html#3-using-github","title":"3. Using GitHub","text":"GitHub Desktop (GUI)Command Line (CLI) <ol> <li>Download GitHub Desktop.</li> <li>Sign in with your GitHub account.</li> <li>Clone a Repository:<ul> <li>Go to <code>File &gt; Clone Repository</code> and enter the repository URL.</li> </ul> </li> <li>Commit Changes:<ul> <li>Make changes to files, then click <code>Commit</code> to save a snapshot of your changes.</li> </ul> </li> <li>Push to GitHub:<ul> <li>After committing, click <code>Push</code> to sync changes with GitHub.</li> </ul> </li> </ol> <ol> <li>Clone a Repository:    <pre><code>git clone https://github.com/your-username/repo-name.git\ncd repo-name\n</code></pre></li> <li>Stage and Commit Changes:    <pre><code>git add .\ngit commit -m \"Initial commit\"\n</code></pre></li> <li>Push Changes:    <pre><code>git push origin main\n</code></pre></li> </ol>"},{"location":"research/coding/index.html#4-workflow-tips-for-effective-collaboration","title":"4. Workflow tips for effective collaboration","text":"<ol> <li> <p>Always pull before making changes:</p> <ul> <li>Before starting any work, ensure your local repository is up-to-date with the latest changes:</li> </ul> <pre><code>git pull origin main\n</code></pre> <ul> <li>This prevents merge conflicts and keeps your local version in sync with the remote repository.</li> </ul> </li> <li> <p>Typical workflow:</p> <ul> <li>Fetch updates:</li> </ul> <pre><code>git fetch\n</code></pre> <ul> <li>Pull latest changes:</li> </ul> <pre><code>git pull origin main\n</code></pre> <ul> <li>Make edits: Modify files as needed.</li> <li>Stage changes:</li> </ul> <pre><code>git add .\n</code></pre> <ul> <li>Commit changes with a clear message:</li> </ul> <pre><code>git commit -m \"Describe the changes made\"\n</code></pre> <ul> <li>Push to remote:</li> </ul> <pre><code>git push origin main\n</code></pre> </li> <li> <p>Commit often, but meaningfully:</p> <ul> <li>Frequent commits help track your progress, but ensure each commit is meaningful and descriptive.</li> </ul> </li> </ol>"},{"location":"research/coding/index.html#common-git-issues","title":"Common Git issues","text":"Merge conflict <p>Issue: This occurs when changes are made in the same part of a file in both the local and remote versions.</p> <p>Solution:    - Resolve the conflict manually in the affected file. - Stage the resolved file:    <pre><code>git add &lt;file&gt;\n</code></pre> - Commit the resolution:    <pre><code>git commit -m \"Resolved merge conflict in &lt;file&gt;\"\n</code></pre></p> Detached HEAD <p>Issue: Happens when you are not on a branch but on a specific commit.</p> <p>Solution:  - Switch back to your branch:    <pre><code>git checkout main\n</code></pre></p> Push rejected <p>Issue: Your push was rejected because the remote has changes that you don't have locally.</p> <p>Solution:  - Pull the latest changes, resolve any conflicts, and try pushing again:    <pre><code>git pull origin main\ngit push origin main\n</code></pre></p> Failed to push Some refs <p>Issue: Occurs when there are changes on the remote that need to be merged before pushing.</p> <p>Solution: - Run:    <pre><code>git pull --rebase origin main\n</code></pre> - This replays your changes on top of the pulled changes and then allows you to push again.</p> Changes not staged for commit <p>Issue: Files were modified but not added to the staging area.</p> <p>Solution:  - Add the changes to the staging area:    <pre><code>git add &lt;file&gt;\n</code></pre> - Or add all changes:    <pre><code>git add .\n</code></pre></p> File deleted locally, but not in remote <p>Issue: A file was deleted locally but still exists in the remote repository.</p> <p>Solution:  - To stage the deletion:    <pre><code>git rm &lt;file&gt;\n</code></pre> - Commit and push the change:    <pre><code>git commit -m \"Deleted &lt;file&gt;\"\ngit push origin main\n</code></pre></p> Authentication failed <p>Issue: This happens if your credentials are incorrect or have expired.</p> <p>Solution:  - Update your Git credentials:    <pre><code>git config --global credential.helper store\n</code></pre> - Re-run the <code>git push</code> command, and enter your credentials when prompted.</p> Branch not found <p>Issue: Occurs when you try to checkout a branch that doesn\u2019t exist locally or remotely.</p> <p>Solution:  - Create the branch:    <pre><code>git checkout -b branch-name\n</code></pre> - Or fetch all remote branches:    <pre><code>git fetch --all\n</code></pre></p> Untracked files <p>Issue: New files are created locally but not yet added to Git.</p> <p>Solution:  - Stage the files:    <pre><code>git add &lt;file&gt;\n</code></pre> - To ignore certain files, add them to <code>.gitignore</code>.</p> File size too large <p>Issue: Git prevents files larger than 100MB from being pushed.</p> <p>Solution:  - Use Git Large File Storage (LFS) to manage large files:    <pre><code>git lfs install\ngit lfs track \"&lt;file-pattern&gt;\"\ngit add &lt;large-file&gt;\ngit commit -m \"Add large file using Git LFS\"\ngit push origin main\n</code></pre> - Alternatively, remove the large file and add it to <code>.gitignore</code>:    <pre><code>git rm --cached &lt;large-file&gt;\necho \"&lt;large-file&gt;\" &gt;&gt; .gitignore\ngit commit -m \"Remove large file and update .gitignore\"\ngit push origin main\n</code></pre></p> Repository size exceeds limit <p>Issue: GitHub imposes a repository size limit, typically 1GB for free accounts.</p> <p>Solution:  - Clean up your repository by removing large files from history using <code>git filter-branch</code> or tools like BFG Repo-Cleaner:    <pre><code>bfg --delete-files &lt;large-file&gt;\ngit reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive\ngit push --force\n</code></pre> - If large files are essential, consider hosting them elsewhere (e.g., cloud storage) and linking to them.</p> Packfile too large <p>Issue: This error can occur when trying to push a repository with a large packfile.</p> <p>Solution:  - Reduce the size of the packfile:    <pre><code>git gc --aggressive --prune=now\n</code></pre> - If the repository is still too large, consider splitting it into smaller repositories.</p> History contains large files <p>Issue: Even if a large file has been deleted, it may still be present in the repository history.</p> <p>Solution:  - Remove the file from history with:    <pre><code>git filter-branch --tree-filter 'rm -f &lt;large-file&gt;' HEAD\ngit push origin --force\n</code></pre> - Note: Use <code>git filter-branch</code> carefully as it rewrites history.</p> <p>By following these practices, you can ensure smoother collaboration and minimize common issues when working with Git and GitHub.</p> <p>We hope this guide helps you establish a solid coding practice. Follow these steps to ensure your code is well-organized, collaborative, and reproducible!</p>"},{"location":"research/dnn/index.html","title":"Neural networks","text":"<p>Page under development</p> <p>via GIPHY</p>"},{"location":"research/eeg/index.html","title":"EEG (Electroencephalography)","text":"<p>We have access to two EEG devices that fall under the NeuroSPACE consortium.</p> <ul> <li> <p>The first one is a mobile EEG system, currently used mostly by the Neuropsychology Lab (PI: C\u00e9line Gillebert).</p> </li> <li> <p>The second EEG apparatus is a high-resolution EEG system, used primarily by the Hoplab and the Desenderlab. This is a 128-channel system of BioSemi. The 128-channel EEG is located in room PSI 00.52. It can be booked through the Calira system (see general Research pages), where you can find it as \"BioSemi EEG Lab 00.52\".</p> </li> </ul> <p>In Hoplab we typically set up experiments for multivariate EEG (\"Representational Dynamics\"). Our standard analysis pipeline is described most in detail in Chen et al. (2023, Imaging Neuroscience).</p> <p>This publication is accompanied by an OSF archive that contains the analysis code to do multivariate analysis with the help of Matlab and the CosmoMVPA toolbox.</p>"},{"location":"research/eeg/eeg-acquisition.html","title":"BioSemi EEG manual","text":"<p>This manual is based on the BioSemi EEG manual from the Cognitive Control in Context (CogTex) research group led by Eva Van den Bussche and was modified by Chiu-Yueh Chen and Klara Schevenels. It is specifically written for the 128-channel BioSemi EEG system located in PSI room 00.52. Eva's lab also provides a video user tutorial (for their 64-channel BioSemi system), which is highly recommended to watch given the many similarities with our system.</p> <p>Do's and don'ts</p> <ul> <li>You might want to ask your participants to wash their hair the morning of the test session for optimal contact between the skull and the electrodes.</li> <li>Always keep one battery charging. Batteries can break if they are fully discharged (\u201cdeep discharge\u201d), but there\u2019s no risk of damage from overcharging.</li> <li>Keep the connectors dry. During cleaning, only the caps and electrodes should get wet. To protect the connectors, wrap them carefully in a towel or plastic bag.</li> <li>Avoid using hot air for drying. If you need to speed-dry a wet cap, you may use a blow dryer\u2014but only with cold air.</li> <li>Report low supplies or broken equipment. Immediately inform the lab manager (currently Klara Schevenels) if something appears broken or stock is running low (e.g., towels, Signa gel, shampoo, adhesive tape, interdental brushes, etc.). The equipment is costly, so handle electrodes and other items with great care.</li> <li>Do not change any hardware or permanent software settings (e.g., monitor refresh rate) without approval from the EEG lab manager.</li> <li>Reserve your slots via Calira. Make sure all your lab sessions are properly scheduled. If you don\u2019t yet have an account, contact Klara.</li> <li>If you are planning to use the EEG lab for your experiment, ask Klara to add you to the EEG channel on the NeuroSPACE Slack through which everything related to the EEG lab is communicated.</li> <li>Transfer your data after each session. For example, make sure to copy your data at the end of the day. The computers are regularly cleaned, and it\u2019s your own responsibility to avoid data loss.</li> <li>Always return the key of the EEG lab to the locker when you\u2019re not in the lab. You can find the key in the keybox next to the coffee machine on the ground floor. You can ask Klara for the code.</li> </ul>"},{"location":"research/eeg/eeg-acquisition.html#first-things-to-do","title":"First things to do","text":"<p>These are the first things to do upon arriving in the EEG lab:</p> <ol> <li>Switch on the stimulus presentation computer and the EEG acquisition computer. For both PCs, the log-in password is the same as the username on the screen.</li> <li>Swith on the general power strip and the A/D-box in the EEG booth using the on/off switch (see Figure 1).</li> <li>Check on the A/D-box whether its battery is charged. If the battery is low, a red light will turn on (see Figure 1). In that case, take out the battery by opening the black clips at the sides and replace it with the spare battery that can be found in the left corner of the desk with the stimulus computer. Make sure that one battery is always charging (see Figure 2). At a later stage you can also check the battery status in ActiView (see Figure 10).</li> </ol> <p> </p>"},{"location":"research/eeg/eeg-acquisition.html#before-the-participant-arrives","title":"Before the participant arrives","text":"<p>You can find all necessary materials in the EEG closet in the room. Make sure that everything is within reach. It is easiest to prepare all the following items on the round table next to the closet, away from the computers:</p> <ol> <li>Fill two syringes with Signa gel. Do not fill them completely to the rim (see Figure 3). Make sure to clean gel remainders off the tip of the tube before and after filling the syringes to prevent old gel from blocking them (see Figure 4).</li> <li>Order and untie the electrodes. If you use the external electrodes, make sure the electrodes and the cleaning scrub to disinfect the participants' face before applying the electrodes are within reach.</li> <li>Place the measuring tape and adhesive tape within reach.</li> <li>Make sure the head cap you need is dry. You can speed-dry it with the hair dryer, but use cold air only to not damage the cap. We have the following caps:<ul> <li>2x small-medium (head circumference of 52-56 cm)</li> <li>1x medium (head circumference of 54-58 cm)</li> <li>2x medium-large (head circumference of 56-60 cm)</li> </ul> </li> </ol> <p> </p>"},{"location":"research/eeg/eeg-acquisition.html#upon-participant-arrival","title":"Upon participant arrival","text":"<p>It is probably the first time for your participant to participate in an EEG-experiment. As it may be a bit scary for them, make sure to explain everything slowly and in great detail. Prepare the participant for data acquisition as follows:</p> <ol> <li>Explain the procedure to the participant.</li> <li>Give the participant enough time to read the information sheet, ask questions, and sign the informed consent.</li> <li>Ask the participant to remove earrings, hair elastics, smartwatches, etc.</li> <li>In case the participant wears glasses, it is easiest to remove them for the application of the EEG cap and the electrodes. Right before the start of the experiment, the participant can put their glasses back on. If possible, ask the participant to wear glasses rather than contact lenses to decrease the blinking frequency during the experiment.</li> <li> <p>Measure the participant's linear distance from nasion (indentation at the top of the nose between the eyes) to inion (small bump just above the neck) to determine the correct cap size (see Figure 5). The sizes can be found on the label in the back of the cap (e.g., if you measure 54 cm, then take the small-medium sized head cap with range 52-56 cm).</p> <p></p> </li> <li> <p>Place the cap on the participant's head: Spread both of your hands in the cap and ask the participant to hold the front part. Make sure both the ears and the label on the back stick out of the cap.</p> </li> <li> <p>Make sure the cap is properly centered by measuring whether A1 (Vertex) is in the center, i.e., halfway between the ears and halfway between the nasion and inion (see Figure 6). Adjust if necessary.</p> <p></p> </li> <li> <p>Close the cap with the velcro straps under the chin. To avoid itching from the velcro, you can add a gauze pad underneath. Make sure that the participant is comfortable.</p> </li> <li> <p>Fill all electrode gaps with superconducting gel (see Figure 7a) in the following way:</p> <ul> <li>Gently fiddle around with the syringe in the gap, as this moves hair aside. Ask the participant for feedback to avoid scratching their scalp too hard.</li> <li>Squeeze a small amount of gel in the gap while moving the syringe upwards.</li> </ul> <p>Important</p> <p>Too much gel can create \u201cbridges\u201d between individual electrodes. Be especially careful around the CMS/DRL electrodes as these are crucial for proper data collection. It is easier to add extra gel later than to remove an excess of gel. As a reference, you should be able to use one tube of Signa gel for +- 5 participants (but this might vary with individual factors such as hair thickness and density).</p> <p>Tip</p> <p>You may notice that you easily lose track of the gaps that you have already filled. It can help to come up with a system (e.g., start from the back, or if you are with two experimenters you can each start from a side and work your way to the middle).</p> </li> <li> <p>Carefully put the electrodes in the appropriate gaps (see Figure 7b). The electrodes are divided over 4 sets of 32 electrodes (labeled with the letters A to D). When you are done, tape the electrodes to the participant's shoulder or the chair to avoid putting stress on the cables. At the very end, also apply the set with the DRL and CMS electrodes.</p> <p>Important</p> <p>Electrodes easily damage, so always handle them with great care. Do not put too much pressure when you insert the electrodes to avoid pushing gel downwards causing it to spread below the cap.</p> <p>Tip</p> <p>Per set, individual electrodes are numbered. Place the electrodes in ascending order to avoid tangling the wires. Start at the base of the skull and work your way up towards the forehead. Make sure the wires end up in the participant's neck.</p> <p></p> </li> <li> <p>Go into the EEG booth, let the participant take place in the right position for the experiment, take off the protective caps from the electrode sets and carefully connect them to the A/D-box (see Figure 8). Make sure the participant sits comfortably to avoid them moving too much during the experiment. You can adjust the height of the chair and the chin rest if needed.</p> <p></p> </li> <li> <p>In case the participant wears glasses, you can put them back on now.</p> </li> <li>Turn the lights low (the light switch is near the door) and make sure the lightning is consistent across all your participants. Do not lower the lights too much as a high contrast between the room and monitor increases the blinking frequency. During breaks, you might want to turn the lights up (to increase alertness) and ask the participant if they would like to drink some water (to decrease swallowing).</li> </ol>"},{"location":"research/eeg/eeg-acquisition.html#setting-up-the-datafile","title":"Setting-up the datafile","text":"<p>Go to the acquisition computer, and take care of the following steps:</p> <ol> <li> <p>Start up ActiView.</p> <p></p> </li> <li> <p>In the right top corner you can check the battery status of the A/D-box (see Figure 9). ActiView will give a warning if the battery status is below 20%.</p> </li> <li>Go to the <code>about actiview</code> tab and click <code>set up configuration file</code>. Choose the 128-channel configuration file. You can find the one that was previously used by Elahe' and Chiu-Yueh in the desktop folder named <code>KS</code>. You can load this one, but please do check whether the specifications match your experiment needs.  </li> <li>Click <code>start</code> and then <code>start file</code> (see Figure 10). Browse to your personal folder where you want to save the data, create a participant folder (e.g., <code>sub-001</code>) and choose a name for your data file, preferably in BIDS-format (e.g., <code>sub-&lt;participant_id&gt;_task-&lt;task_name&gt;&lt;mapping&gt;_run-&lt;run_number&gt;_eeg.bdf</code>). To save each run in a different file, you will have to redo this step for each run.</li> <li>Change the decimation according to your needs (see Figure 9), e.g., to \u00bc (gives you a 512 Hz sampling rate; those are fractions of 2048 Hz).  </li> <li> <p>Go to the <code>electrode offset</code> tab to check the electrode impedances and change the displayed range scale to 50 \u00b5V (see Figure 10):</p> <ul> <li>The impedance of each electrode must be between -20 \u00b5V and 20 \u00b5V (note that this measure is actually an electrode offset value with the CMS electrode as a reference, so negative values are not actually negative). Adjust the electrodes that are not within these values (e.g., Fz in Figure 10) by adding or removing some gel, moving the hair below the electrode with the tip of the syringe and/or by wiggling the electrode a bit.</li> <li>If the electrode impedance stays far outside the [-20 20 \u00b5V] range, the electrode might be broken and needs to be replaced. In case this electrode is not crucial for your study, you can write down the code of the electrode and later contact the EEG lab manager for the back-up electrode set.  </li> </ul> <p></p> <p>Tip</p> <p>You can mirror the screen of the acquisition pc with the electrode offsets in the EEG booth, so that you don't have to run back and forth when adjusting the electrodes to get the impedance right. Don't forget to turn off the screen when you are done.</p> </li> <li> <p>Go back to the <code>monopolar display</code> tab and check the signal:</p> <ul> <li>Bridges would appear in the signal as a set of neighbouring electrodes giving exactly the same or a unusual signal, along with very similar impedance values in adjacent electrodes. You would for example notice that fixing the impedance in one channel also affects its neighbour(s). Note that eyeblinks also can create sudden similar changes in multiple (frontal) channels, this is not bridging.</li> <li>A bad connection of the reference electrode results in a bad signal for all electrodes (see Figure 11a). Check whether the DRL and CMS electrodes are placed correctly and give a good signal.</li> <li>If the cap label is not sticking out, this results in a typical signal distortion. (see Figure 11b)</li> <li>If all electrodes are connected correctly, Actiview should look like Figure 12.</li> <li>Show the participant what happens when they blink, cough, swallow, etc. Instruct the participant to minimize these movements during the measurement phase.</li> </ul> <p> </p> </li> </ol>"},{"location":"research/eeg/eeg-acquisition.html#recording-eeg-data","title":"Recording EEG-data","text":"<p>Hooray, you are now ready to start acquiring EEG-data! To do so, follow these steps on the acquisition pc:</p> <ol> <li>Start recording (= saving data to file) by clicking the <code>paused</code> button in ActiView on the bottom right. When recording, it switches to <code>saving</code> and turns green (see Figure 12). Do not forget this (you wouldn't be the first)!</li> <li>Give your participant the last instructions (a little bit of shouting is necessary) and start the experiment on the stimulus pc.</li> <li>When an experiment run is finished, click on <code>pause save</code> below the green <code>saving</code> button and then on <code>stop</code> on the top left of the screen.</li> <li>Continue to record another run with a different filename by clicking <code>start</code>, <code>start file</code> and entering your new filename in BIDS (see earlier). Do not forget to restart the recording by clicking the <code>paused</code> button again so that it switches to <code>saving</code> and turns green. If you want all your data to be saved in one file, you can skip this step. However, in that case the file size can become inconveniently large.</li> <li>The data is saved in the folder you selected earlier. Make sure to copy the data from the acquisition computer to your personal drive at the end of your experiment. Occasionally, the EEG computer will be cleaned and your data might be lost if it is not backed-up.</li> <li> <p>When you are done, turn off the A/D-box, the general power strip and the lights in the EEG booth.</p> <p>Tip</p> <p>Make sure to write down the \"bad channels\" (i.e., with impedances below or above 20 mV) on your Case Report Form (you can find an example here) for each run separately (the impedances can change throughout the experiment), such that you can take this into account in your analysis (e.g., interpolation).</p> </li> </ol>"},{"location":"research/eeg/eeg-acquisition.html#after-the-experiment","title":"After the experiment","text":"<ol> <li>Unplug the electrode sets from the A/D-box (4 sets + mini-set with DRL and CMS electrodes).</li> <li>Put the protective caps back on the connectors, put the connectors in a waterproof plastic bag and use an elastic band to close the bag. The connectors are very sensitive and should not get wet!</li> <li>Escort the participant out of the EEG booth to the preparation room and put a towel on the participant's shoulders to prevent gel leaking on their clothes.</li> <li>Remove the electrodes from the cap one-by-one. Do not pull on the wires but on the electrode itself, to avoid damage to the wires.</li> <li>Make sure the electrodes never touch metal, this is especially a risk if the chair of the participant contains a lot of metal.  </li> <li>Once all electrodes are removed, place them in the plastic tub (not directly in the metal sink!). Remove the head cap from the participant's head and place it in the tub as well.  </li> <li>Let the participant wash the gel out off their hair in the sink. Below the sink we have shampoo caps they can use.</li> <li>Have the participant sign the reward list if they participate for a monetary reward.  </li> </ol>"},{"location":"research/eeg/eeg-acquisition.html#cleaning","title":"Cleaning","text":"<ol> <li>Put the leftover gel in the syringes back in the tube. </li> <li>Soak the syringes, head cap and electrode sets (not the connectors!) in the plastic tub filled with lukewarm water. Use the shower head to rinse the material.</li> <li>If after soaking and rinsing the electrodes there is still gel left around the electrode tips, carefully brush the gel off with a soft toothbrush. Do this only if really necessary. Use a plastic toothpick to remove the gel from all the gaps in the head cap.</li> <li>Fill the tub with clean lukewarm water and add a bit of the 70% alcohol solution to it. Soak the electrodes, the head cap and the syringes in it for a few seconds to disinfect them. </li> <li>Rinse everything again with lukewarm water and let the caps and syringes dry on the round table. Put the electrode sets back on the rack (make sure they are stable and don't fall on the floor).</li> </ol>"},{"location":"research/eeg/eeg-task.html","title":"Preparing an EEG task","text":"<p>This page describes the steps necessary to prepare your EEG experiment. Follow these steps if you have a behavioural paradigm ready that you need to adapt to EEG testing. You might be in a situation where you have a task script ready from a previous experiment. Here are the aspects of the task, described in this page, that you will need to pay special attention to when bringing your task to the EEG:</p> <ul> <li>Sending triggers: in order to analyse your EEG data, it's crucial to know when your events happened relative to when your EEG data was recorded. To keep track of this timing, we send triggers from the stimulus computer to the EEG computer.</li> <li>Using the photocell: to ensure that the timing of our triggers is accurate with regards to what actually shows on the screen, we use a photo sensitive diode (aka a photocell), taped to the screen, that can detect changes in screen luminance with high temporal precision. Since this photocell is taped to the bottom left of the screen (to avoid obstructing the participant's view), we add a small square to the bottom left of our screen that changes luminance at the same time as we send a trigger.</li> <li>Timing: in most tasks you will need to follow some timing rules to ensure your brain signal is clean and interpretable.</li> </ul> <p>Note that these things are all implemented in the (upcoming) EEG task template (link to GitHub task template repo).</p>"},{"location":"research/eeg/eeg-task.html#sending-triggers","title":"Sending triggers","text":"<p>In order to be able to map psychophysical events from your task to the EEG data, you will need to send triggers from the task computer to the EEG computer. These triggers will then appear in a separate channels alongside your other EEG channels, allowing you to know precisely when e.g. an image was shown with respect to your brain signal. </p> <p>Triggers are sent via a serial port connection. To use it, you will need to link to the serial port within your script. In Python, this can be done using the <code>serial</code> package. The following code snippet shows how to open a connection to the serial port:</p> <pre><code>import serial\nserialport = serial.Serial(\"/dev/ttyUSB0\",baudrate=115200)\n</code></pre> <p>Once your serialport object is defined, you can call it to send triggers using the <code>write</code> method. The following code snippet shows how to send a trigger:</p> <pre><code>import struct\ntrigger = 56 # example trigger value between 0 and 255\nserialport.write(struct.pack('&gt;B', trigger))\n</code></pre> <p>These triggers will be sent in 8-bit format, meaning that you can send integer values between 0 and 255. A good practice here is to define beforehand a mapping of event to trigger, where every important event in your task is assigned a trigger value (think of stimulus, but also fixation cross, blank screen, and response key presses). Here is an example mapping, stored in an external <code>.json</code> file:</p> <pre><code>{\n    \"experiment_start\": 1,\n\n    \"pre_fixation\": 2,\n\n    \"blank\": 3,\n    \"fixation\": 4,\n\n    \"post_fixation\": 5,\n\n    \"experiment_end\": 2,\n\n    \"condition_1\": 10,\n    \"condition_2\": 11,\n    \"condition_3\": 12,\n    \"condition_4\": 13,\n    \"condition_5\": 14,\n\n    \"key_press_left\": 20,\n    \"key_press_right\": 21\n}\n</code></pre>"},{"location":"research/eeg/eeg-task.html#using-the-photocell","title":"Using the photocell","text":"<p>Triggers sent from the task computer are just like any other input from that computer: they take time to travel to their destination. Just like images are not shown the exact moment they are called to be shown, triggers are not received the exact moment they are sent. This delay between trigger sending and trigger receiving is variable, and can be substantial under some conditions. That's why we use a photocell to measure the exact moment when something changes on the screen.</p> <p>The photocell records with high temporal precision changes in luminance on the bottom left of the screen where it is taped. We use it to record the exact moment that images are being presented, by adding a small square to the bottom left of the screen that changes luminance at the same time as we show images. By making sure the stimuli (or any other visual event) and the square change on the same frame, we can use the photocell signal to know exactly when something changed on the screen. The result of this are recorded in the EEG data in a separate channel, alongside the other EEG channels.</p> <p>The best way to make use of this is to place a small black or white square at the correct location on the screen, and changing it everytime something changes on the screen. For instance you could have a black square during fixation, and a white square during stimulus presentation. Have a look at other scripts to ensure you are using the correct location, which will vary based on the monitor size and resolution. The following snippet creates a Psychopy <code>rectangle</code> object with the correct location and size based on the setup used in October 2025:</p> <pre><code>from psychopy import visual\ncolour = 'black' # or 'white'\nphotocell_rectangle = visual.Rect(\n    win=win, width=0.02, height=0.02,\n    ori=0, pos=(-0.8,-0.45), anchor='center',\n    lineWidth=1, opacity=1, depth=-1.0,\n    lineColor=colour, fillColor=colour,\n    interpolate=True\n)\n</code></pre> <p>The advantage of the photocell is not only that it allows you to know exactly when you stimuli appeared on the screen, but also how long they were on the screen for. To record duration as well as onset, you can keep the rectangle on the screen stable for the duration of each event you record. Below is an example of this strategy:</p> <p></p>"},{"location":"research/eeg/eeg-task.html#inter-trial-interval-and-jitter","title":"Inter-trial interval and jitter","text":"<p>When preparing your task for EEG, you will need pay extra attention to the trial timing, in particular to the inter-trial interval (ITI) and jitter. Having a decent ITI is important to allow the brain response to return to baseline. The jitter duration should also align with the design of your experiment\u2014for example, to accommodate late ERP components such as the P300 or N400. A minimum inter-trial interval (ITI) of around 0.9 s provides sufficient time for these late components to resolve. Additionally, adding some jitter in the inter-trial interval is important to prevent participants from developing strong expectations about the upcoming stimulus. As a rule of thumb, think of having a jitter varying between 0.7 and 1.5s.</p>"},{"location":"research/eeg/eeg-task.html#psychopy-specific","title":"Psychopy-specific","text":"<p>Psychopy has built-in functionality to nicely control for the timing of events. In particular, the <code>win.callOnFlip()</code> function allows you to build up a series of command that you would like to execute exactly when the next frame is being presented, which is very handy for sending triggers and logging events. Below is a code snippet showing how to use this function to (1) send a trigger, (2) draw a rectangle, and (3) log an event at the exact moment a stimulus is being presented:</p> <pre><code># prepare stimulus\nstimulus = visual.ImageStim(...)\n# prepare trigger and rectangle\ntrigger = 56 # example trigger value between 0 and 255\nphotocell_rectangle = visual.Rect(...)\n# draw stimulus and rectangle\nstimulus.draw()\nphotocell_rectangle.draw()\n# send trigger and log event at the exact moment the frame is presented\nwin.callOnFlip(serialport.write, struct.pack('&gt;B', trigger))\nwin.callOnFlip(log_event, 'stimulus_presented') # assuming you have a 'log_event' function\n# flip the window to present the frame\nwin.flip()\n</code></pre>"},{"location":"research/ethics/index.html","title":"General information on the ethical procedure","text":"<p>Before you can start your study, you will need to apply for ethical approval - at least in case your study is not covered by one of the existing approved projects.</p> <ul> <li>For (f)MRI and/or patient studies and/or studies carried out at UZ Leuven, you need to file an application at the Ethical committee research UZ/KU Leuven (EC onderzoek). Most of the research done in our lab falls under prospective academic monocentric research, for which you can find the application guidelines here.</li> <li>For behavioral or tDCS studies, which do not fall under the Human Experiments Act, you need to file an application at the Social and Societal Ethics Committee (SMEC) via the PRET (PRivacy and EThics) platform.</li> <li>If your research is close to another study that has already been approved within UZ/KU Leuven, you can also cover the necessary ethical requirements by filing for an amendment to the existing approval (which is considerably less time-consuming).</li> </ul> <p>It is important to know that in any case, your study cannot begin until it has been approved. If you are hesitating whether and/or where you should apply for ethical approval for your project, you can consult this decision tree. Examples of previous applications can be found in this folder of the lab's Teams channel.</p> <p>For more detailed info, please check out the following pages.</p>"},{"location":"research/ethics/MEC.html","title":"Filing your study with the Ethical Committee UZ/KU Leuven","text":""},{"location":"research/ethics/MEC.html#step-1-register-your-study-at-the-uz-leuven-clinical-trial-center-ctc","title":"Step 1: Register your study at the UZ Leuven Clinical Trial Center (CTC)","text":"<ol> <li> <p>Fill in this online form in order to register your study in the UZ/KU Leuven central clinical research database. You can find a user guide on how to do this here. At the minimum, you will need to indicate the study type and upload your research protocol.</p> </li> <li> <p>After submission of the registration form to the CTC, your study will be assigned a study number (the famous S-number), which will be sent to the PI, the identified study contact person at UZ/KU Leuven and the applicant (you) via e-mail. You will need this number for your application to the EC.</p> </li> <li> <p>As soon as an S-number is assigned, supporting UZ Leuven departments (e.g., radiology) from which study-specific support will be required should be contacted, using the relevant forms. You can find more info on how to do this on this flowchart. In addition, the UZ Leuven GDPR questionnaire or KU Leuven PRET questionnaire should be completed. Both the form and the questionnaire are requirements for admissibility to submit your study to the EC.</p> </li> <li> <p>After the internal UZ Leuven stakeholders have reviewed all the documentation and no questions arise during the review process, you will obtain approval from the CTC via an automated validation email. Hooray, you are now ready to submit your study for EC review!</p> </li> </ol> <p>For more info about the CTC, navigate here. If you have questions, you can contact them via email or phone (+32 16 34 19 98).</p>"},{"location":"research/ethics/MEC.html#step-2-apply-for-ec-approval","title":"Step 2: Apply for EC approval","text":"<p>Most likely, you are seeking to get ethical approval for a prospective monocentric (academic) study, for which you can find the application guidelines here. (If you are planning to conducting a different type of study, please start here and navigate to the correct study type to find the corresponding info on the application process.)</p> <p>For a valid application, you are required to upload the following components:</p> <p>Language of documentation</p> <p>All the documentation needs to be written in Dutch, except for the research protocol and the translated versions of the ICFs.</p> <ol> <li> <p>Accompanying letter signed by the PI</p> <ul> <li>Ask for approval (specify the study type) and add a short description of the (goals of the) project</li> <li>Specify where (national/international) and by whom the study will be conducted (your affiliation)</li> <li>Describe the participant group, the recruitment and reimbursement procedure</li> <li>Specify that the informed consent forms will be signed prior to participation</li> <li>Describe the discomfort for the participant and the procedure in case of accidental findings</li> <li>State (in name of the PI) that:<ul> <li>there are no scientific or ethical concerns noted and the study can be executed as described in the protocol</li> <li>no research costs will be charged to the patient, the health insurance or the hospital</li> </ul> </li> <li>Refer to the attached documents</li> </ul> </li> <li> <p>Research protocol including a summary of the protocol in Dutch</p> <ul> <li>Study title, location, rationale and design (incl. study background and objectives)</li> <li>Description of the end points (= primary and secondary outcomes which the study aims to measure)</li> <li>Research method (e.g., behavioral/psychophysical tests and/or neuroimaging)</li> <li>Participant group(s) (incl. inclusion and exclusion criteria)</li> <li>Statistical analysis (incl. software that will be used and sample size calculations)</li> <li>Include an explanation for the sample size, referring to a power analysis</li> <li>In general for typical fMRI studies, a reference to the paper of Friston (2012) is sufficient</li> <li>Any no-fault insurance</li> <li>Quality assurance</li> <li>Direct access to source data</li> <li>Ethical (and any regulatory) approval</li> <li>Method of data processing</li> <li>Publication policy</li> <li>References</li> </ul> </li> <li> <p>Participant recruitment</p> <ul> <li>Describe the procedure that will be used to contact and recruit the study population described in the protocol: where, how, by whom.</li> <li>Make sure to include a justification for the potential recruitment of participants who are unable to give their consent. This info should be provided in a separate document.</li> <li>If recruitment materials (posters, brochures, advertisements, website, etc.) are used, they also have to be submitted under this section. You can read the guidelines on advertising/recruting here.</li> </ul> </li> <li> <p>Informed consent forms (ICFs) (in English and in Dutch)</p> <ul> <li>The EC provides some ICF templates which already include information regarding the legal basis for data processing chosen by UZ/KU Leuven (i.e., \"public interest\", cf. Article 6 of the GDPR), include the necessary contact details of the insurance company, etc. You can find them here (navigate to \"niet-EudraCT studies\").</li> <li> <p>The ICF should be written in clear and understandable language and consist of the following subsections (in a single document):</p> <ol> <li>Essential information to make the decision to participate, such as a clear description of the study project (context, objectives, methodology and procedure), a brief but clear explanation of the participant's rights (voluntary participation, confidentiality, safety precautions, insurance, etc.) and a description of the risks and benefits. Make sure to also include contact details of the PI such that the candidate has a contact point for further questions.</li> <li>The consent form</li> <li>Additional information (appendices) that does not immediately play a role in the decision-making process, such as more detailed information on the study visits (i.e., the number, frequency and content) or on the participant's rights.</li> </ol> </li> <li> <p>Each page in the ICF should be numbered (\"page X\") and mention the full study title and version number and date of the ICF.</p> </li> <li>If the application is covering both behavioral as well as neuroimaging experiments, provide two separate ICFs.</li> </ul> </li> <li> <p>Resume of the Principal Investigator (dated and signed)</p> <ul> <li>The CV should include sufficient information to allow the EC to assess the competence of the PI (i.e., education, current position, professional experience, relevant experience in clinical studies, etc.). This template can be used.</li> <li>The CV should also mention a Good Clinical Practice (GCP) certificate, including the date of certification (no more than 3 years old) and the organization that issued the certificate. GCP training is mandatory for all PIs.</li> </ul> </li> <li> <p>A pdf-version of the completed UZ Leuven GDPR questionnaire or the accepted KU Leuven PRET questionnaire</p> </li> <li> <p>Proof of \"no fault\" insurance, which you can request by sending an email to liability@kuleuven.be</p> </li> <li> <p>Suitability/agreement of the relevant supporting UZ Leuven departments</p> </li> </ol> <p>Once you have written all the necessary documents, you can file your application here by uploading all necessary information. Please not that only 1 (zip)file per component is allowed.</p> <p>\"If the medical ethical committee has an issue with the principal investigator not being connected with UZ Leuven (e.g. in case the PI is a professor at the Faculty of Psychology and Educational Sciences); report this to Prof. Dr. Pol Ghesqui\u00e8re.\" TODO: [Klara] Update this page and make a bit more to the point \u2192</p>"},{"location":"research/ethics/SMEC.html","title":"Filing your study with the Social and Societal Ethics Committee","text":"<p>SMEC evaluates research on human subjects that is not related to health science practices or includes medical or pharmacological procedures.</p> <p>As of February 2020, the SMEC performs an integrated ethics and privacy (GDPR) check on all new applications through the PRET platform (external applicants can download the application form here). For assistance, please make use of the manual on how to use this platform and the SMEC-review checklist for an overview of the aspects that will be assessed during the ethical review. Also, there are ample examples of previous SMEC applications in this folder located on the lab's Teams channel to get you started.</p> <p>Info</p> <p>Please note that there exists a shortened procedure for master's thesis studies in our faculty through this form.</p>"},{"location":"research/ethics/SMEC.html#ethical-review","title":"Ethical review","text":"<p>The SMEC puts a lot of emphasis on a well thought out recruitment and informed consent procedure, for which you can consult their guidelines here. In short:</p> <ul> <li>Researchers should use an information letter, an informed consent form (Dutch example template, English example template) and a GDPR appendix (template) to provide potential participants with the necessary information about the study.  </li> <li> <p>Necessary information in the ICF as well as the information letter includes the following:</p> </li> <li> <p>An invitation to participate in the study</p> </li> <li>Information about the study and the researchers</li> <li>The potential risks and benefits of the study</li> <li>The voluntariness of participation and the possibility of withdrawal at any time without disadvantage</li> <li>Conflicts of interest and plans to commercialize research findings</li> <li>Measures to ensure confidentiality</li> <li>Information on compensation and incentives, as well as plan of action in case of disadvantage</li> <li>If applicable, clarification that the study is not set within the hospital context</li> <li>Contact details of the researchers</li> <li> <p>Ethics committee contact information</p> </li> <li> <p>In some cases, it may be justifiable to use a brief consent form (e.g., a short online questionnaire that is likely to be completed on a small screen). A full consent form might be intimidating in this case or considered a barrier that might limit response rates. A concise consent form should include at least the following elements:</p> </li> <li> <p>Short description of the study and the researchers</p> </li> <li>The voluntariness of participation and the possibility of withdrawal at any time without disadvantage</li> <li>Confidentiality of participants and their data</li> <li>The potential risks of participating in the study</li> <li> <p>Reference and easily accessible link to the information letter with the full info about the study (as listed above)</p> </li> <li> <p>To fully assess the recruitment process, the recruitment materials that will be used (e.g., flyer, poster, call on social media, etc.) should be included in the ethics application. Specific guidelines on recruiting participants through social media can be found here.</p> </li> <li> <p>Specific guidelines on research involving minors can be found here.</p> </li> </ul> <p>For any further questions, you can check out the FAQ section of the SMEC.</p>"},{"location":"research/ethics/SMEC.html#privacy-review","title":"Privacy review","text":"<p>Alongside the ethical review, the privacy impact of the study will be investigated. Here you can find clarifications of the privacy-related questions in the application form.</p>"},{"location":"research/ethics/SMEC.html#outcomes-of-the-reviewing-process","title":"Outcomes of the reviewing process","text":"<p>After the review, you will be informed of the outcome via an automated email from the PRET platform, with three possible results:</p> <ol> <li>Accepted: The ethical approval will remain valid for up to 4 years from its effective date of issue.</li> <li>Minor revisions: Small adjustments are required, after which approval can be granted immediately.</li> <li>Major revisions: Resubmission is required (within 6 months), after which the dossier will be re-reviewed by the SMEC panel.</li> </ol> <p>A flowchart visualising the ethical review process (and its timing) can be consulted here.</p>"},{"location":"research/fmri/index.html","title":"Functional MRI","text":"<p>Welcome to the landing page for all things related to functional MRI (fMRI) in our lab. Whether you're a new student, a researcher, or someone interested in learning more about fMRI, you'll find everything you need here\u2014from getting started with your work environment to data analysis.</p> <ul> <li> <p> First steps</p> <p>Everything you need to know before you start scanning, including MRI booking, invoicing, training and ethical approval.</p> <p> Get started</p> </li> <li> <p> Scanning Procedure</p> <p>Detailed guidelines for preparing and conducting fMRI scans, including participants registration, equipment setup, and scan procedures.</p> <p> View procedures</p> </li> <li> <p> Data Analysis</p> <p>The step-by-step workflow we use to pre-process and analyze fMRI data.</p> <p> Start analyzing</p> </li> <li> <p> fMRI Task</p> <p>You need to code your fMRI task and you don't know where to start? Check out this fMRI task template from the Hoplab Github repositories.</p> <p> See the repo</p> </li> </ul>"},{"location":"research/fmri/index.html#quick-links-to-resources","title":"Quick Links to Resources","text":"<p>Here are some helpful links to external resources for fMRI data analysis, tools, and tutorials:</p> <ul> <li>SPM online documentation - fMRI tutorials</li> <li>fMRI Prep and Analysis with Andrew Jahn</li> <li>Nilearn for neuroimaging in Python</li> <li>SPM Programming Introduction</li> <li>SPM Scripts on GitHub</li> </ul>"},{"location":"research/fmri/fmri-get-started.html","title":"(f)MRI for newbies","text":""},{"location":"research/fmri/fmri-get-started.html#get-acquainted","title":"Get acquainted","text":"<p>Kickstart your (f)MRI learning journey by engaging in the following key activities:</p> Subscribe to the MR mailing listBrowse documentationParticipate in a study <ul> <li>How to subscribe:   Join the MR mailing list by visiting this link. Make sure you are logged in and click on \"Subscribe or Unsubscribe\" in the menu on the top right. Provide your first and last name and hit the subscribe button. A confirmation request will be sent to your email address. Your subscription will be completed if you respond to this request within 48h.</li> <li>Purpose of the list:   This mailing list is used by the MR Safety Officer to report on the status of the MRI equipment and announce upcoming MR safety courses. It also allows MRI researchers to ask each other questions about MRI practices and possible issues.  </li> </ul> <ul> <li>The lab's resources:   Documentation related to (f)MRI studies is available in this Hoplab Teams folder, including manuals, protocols, and information on safety procedures. Regular updates and additional resources will be added to this folder, so make sure to stay informed by regularly checking the documentation.</li> <li>MRI dropbox folder:   The radiology department of UZ Leuven also provides documentation related to the experimental use of the research MRI scanner (MR8) in this Dropbox folder.</li> </ul> <ul> <li>Get involved:   Interested in participating in an (f)MRI study? Discover ongoing studies within our faculty by visiting this Facebook page or by following this page on X (formerly Twitter). To sign up for a study, simply register through the faculty's Experiment Management System (EMS).</li> <li>What to expect:   As a participant in an (f)MRI study, you'll contribute valuable research data by getting your brain scanned. This is a unique opportunity to gain firsthand experience in how (f)MRI studies are conducted.</li> </ul>"},{"location":"research/fmri/fmri-get-started.html#before-you-start","title":"Before you start","text":"<p>Before diving into your (f)MRI study, make sure you're prepared by following the steps below.</p>"},{"location":"research/fmri/fmri-get-started.html#get-formal-ethical-approval","title":"Get formal ethical approval","text":"<p>Follow the yellow section of the flowchart to make sure everything is in order for you to start scanning.</p> <ol> <li>Register your study at the CTC:    After this you receive an S-number (for more info, we refer you to this page).</li> <li>Register your study at the MR research department:    Upload the application form for support from the Radiology department via this link. Include the Clinical Study Coordinator of Radiology (currently, that is lesley.cockmartin@uzleuven.be) as contact person, who will approve your request via email.</li> <li>Get approval from the ethical committee of UZ/KU Leuven:    For more info, we refer you to this page.</li> <li>Follow the MR safety course:    And become an authorized user of the MRI-scanner (see below).</li> </ol> <p>Note that for most fMRI studies (including healthy adult participants), ethical approval has already been obtained and will fall under application nr. S62131 (until end of December 2025) or S70813 (from January 2026 onwards, hopefully). </p>"},{"location":"research/fmri/fmri-get-started.html#attend-the-mr-safety-course","title":"Attend the MR safety course","text":"<ul> <li>Course dates:    This course is organized twice a year by Dr. Ron(ald) Peeters, the MR Safety Officer. The exact course dates will be announced via the MR mailing list, and are typically in February and September.</li> <li>Preparation:    Before attending, carefully read the <code>Safety notes, rules &amp; procedures</code> document in the MRI dropbox folder, which you can find here. In the dropbox folder, you can also find the slides used in a previous safety course (2020 version). Additionally, in our Hoplab Teams folder, you can find some additional safety information.</li> </ul>"},{"location":"research/fmri/fmri-get-started.html#gain-access-to-mr-facilities","title":"Gain access to MR facilities","text":"<ol> <li> <p>Document submission </p> <ul> <li>After obtaining ethical approval, send the completed MR Access file and the approved ICF to ilse.roebben@uzleuven.be and silvia.kovacs@uzleuven.be.  </li> <li> <p>Before entering the Controlled Area for the first time, complete the following:  </p> <ul> <li>MRI Safety Checklist </li> <li>Appendix confirming you completed the MR safety course  </li> <li>Key and badge access form </li> </ul> <p>Send all three documents to ronald.peeters@uzleuven.be, along with the S-number of your study and the following details:</p> Field Value (to be filled in) First name Last name Place of birth Date of birth Start date Expiry date Purpose Scanning on research scanner MR (MR8) Educational institution KU Leuven National register number KU Leuven u-number Email address Extranet required No Phone number </li> </ul> </li> <li> <p>Card activation for MR suite access:    After you have completed all the steps above, Ron will arrange everything and you will automatically gain access to MR8 with your KU Leuven staff/student card.</p> </li> </ol>"},{"location":"research/fmri/fmri-get-started.html#training-and-preparation","title":"Training and preparation","text":"<p>Before you can become an Authorized Other User (AOU), you must undergo practical training and testing:</p> <ol> <li> <p>Observational training:   After reviewing all relevant documentation, observe scan procedures by joining sessions of your colleagues. We have an internal slack channel to keep track of upcoming scans, so make sure you are invited to it if you want to be up to date.</p> </li> <li> <p>Testing protocols:    Before your pilot (f)MRI session with an actual participant, set up your sequences and test your experiment script. Book a phantom session by contacting Dr. Ron(ald) Peeters at ronald.peeters@uzleuven.be. If needed, you can find specific info on the projector screen here.</p> </li> </ol> <p>Independent scanning</p> <p>You are allowed to conduct scans independently after attending approximately 10 sessions with experienced personnel (e.g., more senior colleagues). This will help you learn how to control the scanner effectively. After 2 sessions, you should know how to control the scanner and you are allowed to be the second researcher during a scan session outside office hours.</p>"},{"location":"research/fmri/fmri-get-started.html#during-your-experiment","title":"During your experiment","text":"<p>Understand and follow the detailed MR scan procedures to ensure efficient and safe usage of the MRI facilities.</p>"},{"location":"research/fmri/fmri-get-started.html#scanner-procedures","title":"Scanner procedures","text":"<p>All referenced documents are regularly updated and available in the Hoplab Teams folder. Ensure you read the latest versions before proceeding.</p> <ul> <li> <p>Booking the scanner:   Once you completed all steps above, you can book the scanner via the MRI scientific planning agenda. Details on how to do this can be found in the instructions for use on the planner webiste, the <code>MRI Planning Agenda</code> located in the Hoplab Teams folder and in <code>Safety notes, rules &amp; procedures</code> (pages 8-11) located in the MRI dropbox folder. Note that some periods are booked for users from our faculty. Time slots named Reserved for core user psychology on the calendar are for any researcher from our faculty to use.</p> </li> <li> <p>Using the Scanner:   Operational guidelines for the MR scanner are outlined in the <code>fMRI protocol_MR8_October2019</code> document, available in the Hoplab Teams folder. There, you can also find a useful checklist that you can use as a reminder during scanning. For detailed information on what to do before, during and after a scan, please refer to the ]Scanning procedure](fmri-scanning-procedure.md) page.</p> </li> </ul>"},{"location":"research/fmri/fmri-get-started.html#managing-scan-data-and-invoicing","title":"Managing scan data and invoicing","text":"<ul> <li> <p>Tracking sessions:   Keep detailed records of all scan sessions, noting which sessions provided useful data and/or when you experienced technical issues. Regular reports should be made to your Principal Investigator (PI). In case of technical issues, it is useful to also include what kind of issues you had as well as an estimation of the amount of time lost due to the issues.</p> </li> <li> <p>Quarterly reports:   Every four months, your PI will receive an Excel sheet listing all scan sessions conducted during that period. This file will be forwarded to all researchers who have scanned in the corresponding period.</p> </li> <li> <p>Documenting experiments:   Complete the Excel sheet with the experiment name for each session and clearly note down comments for any session that did not yield useful data for various reasons (e.g., participant cancellation, no-shows, artifacts, technical issues) and send it back.</p> </li> <li> <p>Financial management:   Support staff (currently Klara) will further process the file by including the name of the SAP antenna (i.e., An Van Kets), specifying the funding source for each researcher, and by adjusting the total invoice amount on the invoice to reflect the actual scan hours based on successful data collection sessions (\"corrected total\").</p> </li> </ul>"},{"location":"research/fmri/fmri-hpc.html","title":"KU Leuven HPC: Getting Started","text":"<p>This guide describes how to connect to the KU Leuven/VSC HPC cluster, manage your data, and run <code>fMRIPrep</code>.</p> <p>Login Node</p> <p>When you connect via SSH, you land on a login node. Do not run compute-intensive tasks here, because login nodes are shared among tens of users. Submit jobs to the compute (CPU or GPU) nodes via Slurm.</p> <p>Change directory after login</p> <p>After a login, you land on your home directory <code>$VSC_HOME</code> which has only 3GB quota (by design). This small volume is easy to fill up by mistake; hence, it is recommended to always change the working directory after login to your <code>$VSC_DATA</code> (75 GB) or to your <code>$VSC_SCRATCH</code> (500 GB) upon login, e.g.:</p> <pre><code>cd $VSC_DATA\n</code></pre> <p>This ensures you work in a directory with persistent storage (your files are not deleted) and sufficient disk space. Beware that any files in your scratch folder which is not accessed for 30 days will be automatically deleted, and scratch folder (as the name proposes) has no backup.</p> <p>VSC offers a full documentation of all the services, but in case of question of problems, you  may contact the local support team at KU Leuven.</p>"},{"location":"research/fmri/fmri-hpc.html#1-prerequisites","title":"1. Prerequisites","text":"<ol> <li>You have a valid VSC account (e.g., <code>vsc12345</code>). Request one here.</li> <li>For Windows users, install MobaXterm (SSH client) and FileZilla or WinSCP for file transfer;    for Mac/Linux users, it is sufficient to use the terminal (has built-in SSH) and FileZilla for file transfer.</li> <li>You have logged into the HPC firewall before attempting SSH.</li> <li>You have requested introductory credits (2 million credits for 6 months) via this form.</li> </ol>"},{"location":"research/fmri/fmri-hpc.html#2-connecting-to-the-cluster","title":"2. Connecting to the Cluster","text":"<p>Once your account is active, you may follow these steps to login through Multi-Factor Authentication (MFA) depending on your choice of OS. The two main options are:</p> <ul> <li>KU Leuven OnDemand: a browser-based graphical interface. Follow the instructions in Login to Open OnDemand. Once logged in, you will see a dashboard with options to launch interactive sessions, submit jobs, and manage files. Navigate to the terminal interface by clicking the Login Server Shell Access button.</li> <li>SSH-based login: a terminal-based interface. Follow the instructions in Connecting with an SSH agent. Here, you connect to the cluster by directly opening the terminal interface on your local machine, using the Terminal, Powershell or whichever shell program.</li> </ul> <p>After a successful login, you will see something like:</p> <p></p> <p>Look at the last line:</p> <pre><code>\u2714 [Apr/15 15:13] vsc12345@tier2-p-login-1 ~ $ \n</code></pre> <p>which suggests we are connected to the login node <code>tier2-p-login-1</code> as <code>vsc12345</code> (your VSC username). From now on, what we type in the terminal will be executed on the login node. Note that the login nodes are only meant to move your data around, submit and monitor your jobs, and write/edit your scripts. For all other purposes (actual computation, pre-/post-processing and software installation), you have to start an  interactive or batch job using Slurm.</p> <p>To exit the ssh session and go back to your local terminal, type <code>exit</code>.</p>"},{"location":"research/fmri/fmri-hpc.html#3-data-management-on-the-hpc","title":"3. Data Management on the HPC","text":""},{"location":"research/fmri/fmri-hpc.html#31-structuring-your-data","title":"3.1. Structuring your data","text":"<p>Your VSC account comes equipped with several folders, each with different characteristics. When logging in, you automatically land in your home directory, which is located at <code>/user/leuven/123/vsc12345</code>. This path is stored in the variable <code>VSC_HOME</code>, which you check by typing <code>echo $VSC_HOME</code> in the terminal. Likewise, other important directories are stored in variables names. To navigate quickly to any of these directories, you can use the <code>cd</code> command followed by the variable name, e.g., <code>cd $VSC_DATA</code> to move to your data directory.</p> <p>Folder Structure:</p> <ul> <li><code>VSC_HOME</code>: Small quota, do not use it at all.</li> <li><code>VSC_DATA</code>: Persistent but slow, larger capacity. Store big data, installed packages (e.g., Miniforge/Miniconda), and your project files.   All files and folders in your data directory are snapshoted, so you can recover them if you delete them by mistake.   For that purpose, look inside the <code>.snapshot</code> directory inside any folder in your data directory for available snapshot timestamps.</li> <li><code>VSC_SCRATCH</code>: Temporary storage but fast I/O. Files here will be deleted if not accessed after 30 days. Do not store your project data here!</li> <li><code>VSC_SCRATCH_NODE</code>: Temporary storage available on the compute node which can be only used when your job is running on a compute node.</li> </ul> <p>A typical approach is to keep a dedicated subdirectory in <code>VSC_DATA</code> for each project. For example:</p> <pre><code>VSC_DATA                    # Persistent storage\n\u2514\u2500\u2500 fmri                    # Place to store fMRI data\n    \u251c\u2500\u2500 myproject           # project-specific folder\n    \u2502   \u251c\u2500\u2500 BIDS                # BIDS dataset\n    \u2502   \u2502   \u251c\u2500\u2500 derivatives     # Derivatives\n    \u2502   \u2502   \u2514\u2500\u2500 sub-01          # Subject data\n    \u2502   \u2502       \u251c\u2500\u2500 anat  \n    \u2502   \u2502       \u2514\u2500\u2500 func  \n    \u2502   \u251c\u2500\u2500 license.txt         # FreeSurfer license file  \n    \u2502   \u2514\u2500\u2500 sourcedata          # Raw data\n    \u2502       \u2514\u2500\u2500 DICOM           \n    \u2502           \u2514\u2500\u2500 sub-01      \n    \u2514\u2500\u2500 fmriprep-25.0.0.sif     # Singularity container\n</code></pre>"},{"location":"research/fmri/fmri-hpc.html#32-managing-storage-usage","title":"3.2. Managing storage usage","text":"<p>Operations like fMRIPrep can generate large amounts of data, so it is important to manage your data effectively. You can find some general advice on the VSC documentation.</p> <p>If your HPC storage is full, you might encounter this type of error when sending data to the compute node:</p> <pre><code>rsync: writefd_unbuffered failed to write 32768 bytes [sender]: Broken pipe (...)\nrsync: connection unexpectedly closed (... bytes received so far) [sender]\nrsync error: error in rsync protocol data stream (code 12) at ...\n</code></pre> <p>When encountering this or other similar errors, you should check your storage usage. Use the following command:</p> <pre><code>myquota\n</code></pre> <p>To see, directory by directory, how much space you are using. This command will also show you which directories might be exceeding their quota. You can also check the size of a specific directory, for example, your data directory:</p> <pre><code>du -h $VSC_DATA\n</code></pre> <p>Where <code>-h</code> means \"human-readable\" (e.g., in GB or MB, add <code>-s</code> - \"summary\" for the total size).</p> <p>If you find your storage full, it is a good idea to clean up the files you won't need anymore, as well as the data you've already downloaded locally. In general, it is best not to consider the HPC as a long-term storage solution.</p>"},{"location":"research/fmri/fmri-hpc.html#4-transferring-data-to-the-hpc","title":"4. Transferring Data to the HPC","text":"<p>Once connected to the cluster, you\u2019ll need to transfer your BIDS dataset, FreeSurfer license, and other relevant files to your <code>VSC_DATA</code> directory on the HPC system.</p> Windows (WinSCP)Linux / macOS <ol> <li> <p>Login through the firewall.</p> </li> <li> <p>Open WinSCP:</p> <ul> <li>Host Name: <code>login.hpc.kuleuven.be</code></li> <li>User Name: <code>vsc12345</code></li> <li>Load your private key if needed.</li> </ul> </li> <li> <p>Transfer Files:</p> <ul> <li>Navigate to <code>/data/leuven/123/vsc12345/data/</code> on the remote side.</li> <li>Drag and drop or copy/paste files from your local machine.</li> <li>To avoid overwriting existing files, make sure to only add new content, or configure WinSCP to skip duplicates.</li> </ul> </li> </ol> <ol> <li>Login through the firewall.</li> </ol> <p>2a. Basic <code>scp</code> (secure copy): <pre><code>scp -r /local/path/BIDS \\\nvsc12345@login.hpc.kuleuven.be:/data/leuven/123/vsc12345/data/BIDS\n</code></pre></p> <ul> <li><code>-r</code> copies directories recursively.</li> </ul> <p>2b. Merging folders with <code>rsync</code> (recommended):</p> <p>Use <code>rsync</code> to avoid overwriting existing files:</p> <pre><code>rsync -av --ignore-existing /local/path/BIDS \\\nvsc12345@login.hpc.kuleuven.be:/data/leuven/123/vsc12345/data/\n</code></pre> <ul> <li><code>-a</code> preserves file attributes.</li> <li><code>-v</code> enables verbose output.</li> <li><code>--ignore-existing</code> skips files already present on the remote server.</li> </ul> <p>Make sure you replace <code>/local/path/BIDS</code> with your local BIDS folder, and <code>vsc12345</code> and <code>123</code> with your actual VSC username and the first 3 digits of your ID.</p>"},{"location":"research/fmri/fmri-hpc.html#5-building-an-fmriprep-singularity-container","title":"5. Building an <code>fMRIPrep</code> Singularity Container","text":"<ol> <li>Move to <code>VSC_DATA</code>:</li> </ol> <pre><code>cd $VSC_DATA\n</code></pre> <ol> <li>Build the Container:</li> </ol> <pre><code>singularity build fmriprep-25.0.0.sif docker://nipreps/fmriprep:25.0.0\n</code></pre> <p>This fetches the Docker image and converts it to a Singularity <code>.sif</code> image.</p> <p>Warning</p> <p>Ensure you have enough quota and that you are not attempting this in <code>VSC_HOME</code>!</p>"},{"location":"research/fmri/fmri-hpc.html#6-submitting-and-running-an-fmriprep-job-with-slurm","title":"6. Submitting and Running an <code>fMRIPrep</code> Job with Slurm","text":""},{"location":"research/fmri/fmri-hpc.html#61-creating-a-slurm-jobscript","title":"6.1. Creating a Slurm Jobscript","text":"<p>To submit our job to the compute node, we will need to create a Slurm script. This script will define the job resources, dependencies, and all the steps needed to complete the workflow. Then, submit the jobscript to the cluster using the <code>sbatch</code> command. Your batch job will run remotely on a compute node, and <code>fmriprep</code> will be executed from within a container.</p> <p>To create such file, type in your ssh session:</p> <pre><code>cd $VSC_DATA\nnano run_fmriprep_job.slurm\n</code></pre> <p>This will open a text editor. Use the following template as a guideline, but change relevant fields (such as job name, account name etc) accordingly:</p> <p><pre><code>#!/bin/bash -l\n#SBATCH --account=intro_vsc12345\n#SBATCH --cluster=genius\n#SBATCH --partition=batch\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=20G\n#SBATCH --time=08:00:00\n#SBATCH --job-name=fmriprep_sub-42\n#SBATCH --output=slurm-%j.out\n#SBATCH --error=slurm-%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=your.email@kuleuven.be\n\n# Move to the data directory\ncd $VSC_DATA/data\n\n# Run fMRIPrep with Singularity\nsingularity run --cleanenv \\\n  -B $VSC_DATA/data/BIDS:/data:ro \\\n  -B $VSC_DATA/data/BIDS/derivatives:/out \\\n  -B $VSC_SCRATCH/fmriprep_tmp:/scratch \\\n  -B $VSC_DATA/data/license.txt:/opt/freesurfer/license.txt \\\n  $VSC_DATA/fmriprep-25.0.0.sif \\\n  /data /out/fmriprep participant \\\n  --participant-label 42 \\\n  --skip-bids-validation \\\n  --output-spaces MNI152NLin2009cAsym:res-2 fsaverage \\\n  --work-dir /scratch \\\n  --bold2anat-dof 9 \\\n  --nthreads 16 --omp-nthreads 16 \\\n  --mem-mb 20000 \\\n  --clean-workdir\n</code></pre> Then press <code>CTRL+X</code> to exit and <code>Y</code> to save. Check whether your file was saved correctly:</p> <pre><code>ls $VSC_DATA\n</code></pre> <p>which should return:</p> <pre><code>fmriprep-25.0.0.sif  data  license.txt  run_fmriprep_job.slurm\n</code></pre>"},{"location":"research/fmri/fmri-hpc.html#62-submitting-the-job","title":"6.2 Submitting the Job","text":"<ol> <li> <p>Navigate to the same directory as your script (or specify the full path):</p> <pre><code>cd $VSC_DATA\n</code></pre> </li> <li> <p>Submit:</p> <pre><code>sbatch run_fmriprep_job.slurm\n</code></pre> <p>A message appears:</p> <pre><code>Submitted batch job 58070026\n</code></pre> </li> </ol> <p>where <code>58070026</code> is your job ID (but it will be different in your case).</p> <ol> <li> <p>Check the status of your job:</p> <p>Through the terminal, you can check the status of your job by running:</p> <pre><code>squeue -j 58070026\n</code></pre> <p>Which returns:</p> <p><pre><code>JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n58070026 batch     fmriprep vsc12345  R       0:00      1 (Priority)\n</code></pre> Through the KU Leuven OnDemand interface, you can also check the status of your job by clicking on the Active jobs tab in the dashboard.</p> </li> </ol> <p>Congrats! Your job is now being executed on the cluster.</p> <p>Runtime</p> <p>Your runtime depends critically on various factors, such as the choice of hardware, the number of threads used, the amount of memory used, and your I/O pattern. You may choose one of the <code>batch_*</code> partitions from either of Genius or wICE clusters to find the most performant hardware for your workflow. Detailed information about avaialble hardware can be found on Ku Leuven Tier-2 specifications.</p> <p>From a first test on the call above (which includes the FreeSurfer workflow) on a subjects with one anatomical image and two functional scans:</p> <ul> <li>32 cores and 50GB of RAM end up in a runtime of <code>03:41:05</code> </li> <li>16 cores and 20GB of RAM end up in a runtime of <code>03:59:27</code> on the same subjects</li> </ul>"},{"location":"research/fmri/fmri-hpc.html#63-slurm-basics-you-will-need","title":"6.3 SLURM Basics you will need","text":"<ul> <li><code>sbatch</code>: Submits a jobscript to the cluster.</li> <li><code>srun</code>: Start an interactive session on a compute node.</li> <li><code>squeue -M genius,wice</code>: Shows your jobs on both clusters.</li> <li><code>scancel -M &lt;cluster&gt; &lt;jobID&gt;</code>: Cancels a job with the <code>&lt;jobID&gt;</code> running on <code>&lt;cluster&gt;</code>.</li> <li><code>sstat</code>: Real-time CPU/memory info on running jobs.</li> <li><code>sacct</code>: Shows CPU time, wall time, memory usage, and exit codes for finished jobs.</li> </ul>"},{"location":"research/fmri/fmri-hpc.html#7-monitoring-your-job","title":"7. Monitoring Your Job","text":""},{"location":"research/fmri/fmri-hpc.html#71-job-queue","title":"7.1 Job Queue","text":"<p>Check the status of your running or pending jobs:</p> <pre><code>squeue -M genius,wice -u $USER\n</code></pre> <p>which returns:</p> <pre><code>$ squeue -M genius,wice -u $USER\nCLUSTER: genius\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          58070026     batch fmriprep vsc12345  R    2:25:57      1 r27i27n19\n\nCLUSTER: wice\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n</code></pre> <ul> <li><code>R</code> means running, <code>PD</code> means pending.  </li> <li><code>NODELIST</code> shows the node(s) assigned. If it shows <code>(Priority)</code>, you`re still waiting.</li> </ul>"},{"location":"research/fmri/fmri-hpc.html#72-checking-logs","title":"7.2 Checking Logs","text":"<p>When you submit a Slurm job, two log files are automatically generated in the directory where you ran <code>sbatch</code>:</p> <ul> <li><code>slurm-&lt;jobid&gt;.out</code>: captures standard output (stdout) \u2014 the regular printed output from your script.</li> <li><code>slurm-&lt;jobid&gt;.err</code>: captures standard error (stderr) \u2014 any warnings or errors encountered during the job.</li> </ul>"},{"location":"research/fmri/fmri-hpc.html#check-available-files","title":"Check available files","text":"<p>To see the files in your current directory:</p> <pre><code>ls\n</code></pre> <p>You should see something like:</p> <pre><code>fmriprep-25.0.0.sif  data  run_fmriprep_job.slurm  slurm-58070026.out  slurm-58070026.err\n</code></pre>"},{"location":"research/fmri/fmri-hpc.html#open-and-read-the-logs","title":"Open and Read the Logs","text":"<p>To open the full output log (static view):</p> <pre><code>nano slurm-58070026.out\n</code></pre> <p>To close <code>nano</code>, press <code>Ctrl+X</code> \u2192 then <code>N</code> if prompted to save changes.</p> <p>To view just the last few lines of the file:</p> <pre><code>tail -n 30 slurm-58070026.out\n</code></pre> <p>This prints the last 30 lines of the file \u2014 useful to check progress without opening the full log.</p> <p>To continuously monitor the last lines in real time (refreshes every 1 second):</p> <pre><code>watch -n 1 tail -n 30 slurm-58070026.out\n</code></pre> <p>This will auto-refresh every second and is very helpful to track live progress.</p> <p>To exit the <code>watch</code> session Press <code>Ctrl+Z</code>.</p>"},{"location":"research/fmri/fmri-hpc.html#8-canceling-a-job","title":"8. Canceling a Job","text":"<p>If you want to stop a running or pending job you need to provide the cluster name (defaults to Genius), and the unique <code>&lt;jobID&gt;</code>, e.g.:</p> <pre><code>scancel 58070026\n</code></pre> <p>Or:</p> <pre><code>scancel --user=$USER\n</code></pre>"},{"location":"research/fmri/fmri-hpc.html#9-after-the-job-completes","title":"9. After the Job Completes","text":"<p>Once your job finishes, you will receive an email from the SLURM scheduler. This email tells you whether the job completed, failed, or was cancelled.</p>"},{"location":"research/fmri/fmri-hpc.html#91-example-email-messages","title":"9.1 Example Email Messages","text":"<ul> <li> <p>\u2705 Successful run: <pre><code>Slurm Job_id=58070026 Name=fmriprep_sub-42 Ended, Run time 03:41:05, COMPLETED, ExitCode 0\n</code></pre></p> </li> <li> <p>\u274c Cancelled manually or by the system: <pre><code>Slurm Job_id=58069757 Name=fmriprep_sub-42 Ended, Run time 00:00:00, CANCELLED, ExitCode 0\n</code></pre></p> </li> <li> <p>\u26a0\ufe0f Job failed with error: <pre><code>Slurm Job_id=58069624 Name=fmriprep_sub-42 Failed, Run time 00:00:20, FAILED, ExitCode 255\n</code></pre></p> </li> </ul> <p>Tip</p> <p>If you submit too many jobs, it is best not to enable email notifications.</p>"},{"location":"research/fmri/fmri-hpc.html#92-inspecting-failed-jobs","title":"9.2 Inspecting Failed Jobs","text":"<p>If your job failed, the first thing to do is inspect the <code>.out</code> and <code>.err</code> files written by SLURM. These contain the full log and error output from <code>fMRIPrep</code>.</p> <ol> <li> <p>Go to the correct folder (usually <code>$VSC_DATA</code>):</p> <pre><code>cd $VSC_DATA\n</code></pre> </li> <li> <p>Open the output file:</p> <pre><code>nano slurm-58070026.out\n</code></pre> <ul> <li>Use the arrow keys to scroll.</li> <li>The error is usually toward the end of the file.</li> <li>Press <code>Ctrl+X</code> to exit <code>nano</code>.</li> </ul> </li> <li> <p>Also open the error file:</p> <pre><code>nano slurm-58070026.err\n</code></pre> <ul> <li>This might contain Python tracebacks or messages from Singularity.</li> </ul> </li> </ol> <p>Make sure to check both files, as the error could appear in either.</p>"},{"location":"research/fmri/fmri-hpc.html#93-verifying-successful-runs","title":"9.3 Verifying Successful Runs","text":"<p>If the job completed successfully, it's time to check the output files.</p> <p>In our SLURM script, the <code>--output</code> directory is bound to:</p> <pre><code>-B $VSC_DATA/data/BIDS/derivatives:/out\n</code></pre> <p>So the results will be saved under:</p> <pre><code>$VSC_DATA/data/BIDS/derivatives/fmriprep\n</code></pre> <p>To inspect the results:</p> <pre><code>cd $VSC_DATA/data/BIDS/derivatives/fmriprep\nls\n</code></pre> <p>You should see something like this:</p> <pre><code>dataset_description.json  logs  sourcedata  sub-42  sub-42.html\n</code></pre> <p>Use the <code>tree</code> command to get a visual overview of the folder structure:</p> <pre><code>tree -L 2 .\n</code></pre> <p>Expected output:</p> <pre><code>fmriprep/\n\u251c\u2500\u2500 dataset_description.json\n\u251c\u2500\u2500 logs/\n\u2502   \u251c\u2500\u2500 CITATION.bib\n\u2502   \u251c\u2500\u2500 CITATION.html\n\u2502   \u251c\u2500\u2500 CITATION.md\n\u2502   \u2514\u2500\u2500 CITATION.tex\n\u251c\u2500\u2500 sourcedata/\n\u2502   \u2514\u2500\u2500 freesurfer/\n\u2502       \u251c\u2500\u2500 fsaverage/\n\u2502       \u2514\u2500\u2500 sub-42/         # FreeSurfer recon-all output\n\u251c\u2500\u2500 sub-42/\n\u2502   \u251c\u2500\u2500 anat/\n\u2502   \u2502   \u251c\u2500\u2500 sub-42_desc-preproc_T1w.nii.gz\n\u2502   \u2502   \u251c\u2500\u2500 sub-42_dseg.nii.gz\n\u2502   \u2502   \u251c\u2500\u2500 sub-42_space-MNI152NLin2009cAsym_res-2_desc-preproc_T1w.nii.gz\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 func/\n\u2502   \u2502   \u251c\u2500\u2500 sub-42_task-*_desc-preproc_bold.nii.gz\n\u2502   \u2502   \u251c\u2500\u2500 sub-42_task-*_desc-confounds_timeseries.tsv\n\u2502   \u2502   \u251c\u2500\u2500 sub-42_task-*_space-MNI152NLin2009cAsym_res-2_desc-brain_mask.nii.gz\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 figures/\n\u2502   \u2502   \u251c\u2500\u2500 sub-42_desc-summary_T1w.html\n\u2502   \u2502   \u251c\u2500\u2500 sub-42_task-*_desc-summary_bold.html\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 log/\n\u2502       \u2514\u2500\u2500 &lt;timestamped-folder&gt;/\n\u2502           \u2514\u2500\u2500 fmriprep.toml\n\u251c\u2500\u2500 sub-42.html              # Subject-level report\n</code></pre> <p>This confirms that <code>fMRIPrep</code> completed for subject 42 and produced its derivatives, QC figures, and logs.</p>"},{"location":"research/fmri/fmri-hpc.html#94-downloading-the-results-derivatives-to-your-local-machine","title":"9.4 Downloading the Results (Derivatives) to Your Local Machine","text":"<p>After fMRIPrep completes successfully, you might want to retrieve the <code>derivatives/fmriprep</code> folder from the HPC back to your local computer for further analysis. The steps differ slightly depending on your operating system.</p> Windows (WinSCP)macOS / Linux <ol> <li> <p>Log in via the Firewall     Go to https://firewall.vscentrum.be/auth/login and enter your VSC credentials.</p> </li> <li> <p>Open WinSCP:</p> <ul> <li>Host Name: <code>login.hpc.kuleuven.be</code></li> <li>User Name: <code>vsc12345</code></li> <li>(Optional) Load your private key if needed.</li> </ul> </li> <li> <p>Navigate &amp; Download:</p> <ul> <li>In WinSCP, navigate on the remote side to:   <pre><code>/data/leuven/123/vsc12345/data/BIDS/derivatives/fmriprep\n</code></pre>   (Replace <code>vsc12345</code> with your actual VSC username.)</li> <li>On your local machine side, open (or create) a destination folder where you want to save the files.</li> <li>Drag and drop the <code>fmriprep</code> folder (or individual files) from the remote window to your local folder.</li> <li>Alternatively, right-click and select Download or Download To....</li> </ul> </li> </ol> <p>This will copy the <code>fmriprep</code> output directory (and its subfolders) to your local Windows computer.</p> <ol> <li> <p>Confirm Firewall Access      Visit https://firewall.vscentrum.be/auth/login if your connection is blocked by the HPC firewall.</p> </li> <li> <p>Use <code>scp</code> or <code>rsync</code>:</p> <ul> <li>Basic <code>scp</code> example:    <pre><code>scp -r vsc12345@login.hpc.kuleuven.be:/data/leuven/123/vsc12345/data/BIDS/derivatives/fmriprep /local/path/\n</code></pre></li> <li>The <code>-r</code> flag copies directories recursively.</li> <li>Replace <code>vsc12345</code> and <code>123</code> with your own account details.</li> <li> <p><code>/local/path/</code> is the folder on your Mac/Linux machine where you want the data.</p> </li> <li> <p><code>rsync</code> (recommended for large data):    <pre><code>rsync -av --progress \\\n  vsc12345@login.hpc.kuleuven.be:/data/leuven/123/vsc12345/data/BIDS/derivatives/fmriprep \\\n  /local/path/\n</code></pre></p> </li> <li><code>-a</code> preserves file attributes.</li> <li><code>-v</code> is verbose output.</li> <li> <p><code>--progress</code> shows real-time progress info (optional).</p> </li> <li> <p>Or, if you only want to quickly download reports and related files (for quick results check):    <pre><code>rsync -av --progress \\\n--include='*/' \\\n--include='*.html' \\\n--include='*.svg' \\\n--include='*.png' \\\n--exclude='*' \\\nvsc12345@login.hpc.kuleuven.be:/data/leuven/123/vsc12345/data/BIDS/derivatives/fmriprep/ \\\n/local/path/fmriprep_reports/\n</code></pre></p> </li> </ul> </li> </ol> <p>This will download the entire <code>fmriprep</code> output folder to your Mac or Linux machine for further analysis.</p>"},{"location":"research/fmri/fmri-hpc.html#10-advanced-monitoring-resource-usage","title":"10. Advanced: Monitoring Resource Usage","text":"<p>Sometimes you want to inspect your job in real-time to check whether it is using the resources (CPU, memory) you requested.</p>"},{"location":"research/fmri/fmri-hpc.html#101-identify-the-compute-node","title":"10.1 Identify the Compute Node","text":"<p>First, determine which compute node your job is running on by checking the job queue:</p> <pre><code>squeue -M genius,wice -u $USER\n</code></pre> <p>Example output:</p> <pre><code>CLUSTER: genius\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          58070026     batch fmriprep vsc12345  R      40:46      1 r27i27n19\n</code></pre> <p>Look at the NODELIST column \u2014 in this case, your job is running on node <code>r27i27n19</code>.</p> <p>If NODELIST shows <code>(Priority)</code> with state <code>PD</code> (for pending), it means your job is still waiting in the queue and hasn`t started yet.</p>"},{"location":"research/fmri/fmri-hpc.html#102-ssh-into-the-compute-node","title":"10.2: SSH into the Compute Node","text":"<p>To access the compute node where your job is running:</p> <pre><code>ssh r27i27n19\n</code></pre> <p>You should see a message like:</p> <pre><code>Joining job 58070026\n</code></pre> <p>Also, the prompt will change from the login node (e.g., <code>tier2-p-login-2</code>) to the compute node (e.g., <code>r27i27n19</code>), indicating you are now inside the job environment on the actual node.</p>"},{"location":"research/fmri/fmri-hpc.html#103-run-htop-to-monitor-resource-usage","title":"10.3: Run <code>htop</code> to Monitor Resource Usage","text":"<p>Once inside the compute node, run:</p> <pre><code>htop -u $USER\n</code></pre> <p>This opens a live system monitor like this one:</p> <p></p> <p>Which shows:</p> <ul> <li>All processes currently running under your username</li> <li>CPU usage per core</li> <li>Memory usage</li> <li>Process names and resource consumption</li> </ul> <p>You should see multiple processes associated with <code>fmriprep</code>. Ideally, if you requested 32 cores, you should see all 32 cores utilized \u2014 especially during parallelizable tasks. However, some parts of <code>fmriprep</code> can only run serially. During these phases, only 1\u20132 cores may be active, which is normal.</p> <p>To exit <code>htop</code>, press F10 or q</p>"},{"location":"research/fmri/fmri-hpc.html#11-checking-credit-usage","title":"11. Checking Credit Usage","text":"<p>Each VSC account is allocated a fixed number of compute credits, which are consumed depending on the number of CPUs, memory, and time your jobs require. If your jobs are not running or are failing, it's useful to check how many credits you have left, and how many you\u2019ve already used.</p>"},{"location":"research/fmri/fmri-hpc.html#111-check-your-current-credit-balance","title":"11.1 Check your current credit balance","text":"<p>You can check the total number of credits deposited, used, and refunded using:</p> <pre><code>sam-balance\n</code></pre> <p>You\u2019ll see an output like this:</p> <pre><code>\u2718 [Jul/10 12:06] vsc12345@tier2-p-login-2 /data/leuven/123/vsc12345 $  sam-balance\nID       Name                   Balance         Reserved        Available      \n======== ====================== =============== =============== ===============\n99270    intro_vsc12345         1937378         0               1937378\n</code></pre> <p>or, alternatively:</p> <pre><code>sam-statement --account=intro_vsc12345\n</code></pre> <p>(where <code>--account</code> is your credits account name, and not your username) which returns:</p> <pre><code>###############################################################################\n#\n# Includes Account=intro_vsc12345\n# Generated on Thu Jul 10 11:39:45 2025.\n# Reporting fund activity from 2022-01-01 to NOW\n#\n###############################################################################\n\n============================================================ ==================\nCredits deposited in the given period:                                  2000000\nCredits refunded in the given period (all clusters):                          0\nCredits consumed in the given period (all clusters):                     405153\n============================================================ ==================\n\nJobID      Cluster  Account                User     Partition   Credits\n========== ======== ====================== ======== =========== ===============\n58233013   genius   intro_vsc12345         vsc12345 batch       11\n58233014   genius   intro_vsc12345         vsc12345 batch       9\n58233068   genius   intro_vsc12345         vsc12345 batch       35545\n58233069   genius   intro_vsc12345         vsc12345 batch       17439\n58233295   genius   intro_vsc12345         vsc12345 batch       14745\n58234300   genius   intro_vsc12345         vsc12345 batch       82\n58234301   genius   intro_vsc12345         vsc12345 batch       21461\n</code></pre>"},{"location":"research/fmri/fmri-hpc.html#112-check-how-many-credits-each-job-used","title":"11.2 Check how many credits each job used","text":"<p>To see how many credits were used per job, use either the <code>sam-statement</code> command from the section above, or:</p> <pre><code>sam-list-usagerecords --start=2025-01-01 --end=2025-07-01\n</code></pre> <p>You can adjust the <code>--start</code> and <code>--end</code> dates to fit the period you want to investigate.</p> <p>This returns a table like:</p> <pre><code>JobID      Cluster  Account         State      User      Credits     Start              End\n========== ======== ==============  ========== ========= ==========  ================== ===================\n58270696   genius   intro_vsc12345  COMPLETED  vsc12345   23348       2025-07-08T12:45   2025-07-08T17:15\n</code></pre> <p>This helps you estimate how much a typical run (e.g. an <code>fMRIPrep</code> job) costs. In our experience, a successful fMRIPrep run with 16 cores and 20 GB RAM costs ~20,000\u201325,000 credits.</p>"},{"location":"research/fmri/fmri-hpc.html#12-references-and-links","title":"12. References and Links","text":"<ul> <li>VSC Documentation</li> <li>KU Leuven HPC Info</li> <li>Firewall Access</li> <li>SSH Access via MobaXterm</li> <li>Singularity Docs </li> <li>fMRIPrep Docs </li> </ul> <p>Happy Computing!</p>"},{"location":"research/fmri/fmri-nimare.html","title":"NiMARE Meta-Analysis","text":"<p>This guide demonstrates how to download and process the Neurosynth database using NiMARE, a Python-based framework designed for neuroimaging meta-analyses.</p> <p>This step-by-step tutorial covers downloading the Neurosynth data, converting it into a NiMARE-compatible dataset, enriching it with article abstracts, filtering studies based on specific criteria, and performing custom meta-analyses.</p> <p>Before starting, ensure NiMARE and required dependencies are installed in your environment:</p> <pre><code>pip install numpy pandas pprint scipy nimare biopython nibabel nilearn\n</code></pre> <p>Tip</p> <p>Before starting a new project, make sure you set up your environment correctly. Each project should have its own dedicated environment. Please, follow the instructions provided in this and this section.</p>"},{"location":"research/fmri/fmri-nimare.html#1-importing-necessary-packages","title":"1. Importing Necessary Packages","text":"<p>Begin by importing all necessary libraries required throughout the script. These libraries are essential for downloading data, processing datasets, running analyses, and visualizing results.</p> <pre><code># --- Step 1: Import All Necessary Packages ---\n\nimport os\nimport shutil\nimport numpy as np\nfrom scipy.stats import norm\nfrom nimare.dataset import Dataset\nfrom nimare.extract import download_abstracts, fetch_neurosynth\nfrom nimare.io import convert_neurosynth_to_dataset\nfrom nimare.meta.cbma.ale import ALE  # ALE algorithm for meta-analysis\nfrom nimare.correct import FWECorrector  # Family-wise error correction\nfrom pprint import pprint\nfrom nilearn.plotting import plot_stat_map\nfrom nilearn import datasets  # Neuroimaging datasets\nfrom nilearn.reporting import get_clusters_table\nimport nibabel as nib\n</code></pre>"},{"location":"research/fmri/fmri-nimare.html#2-downloading-the-neurosynth-database","title":"2. Downloading the Neurosynth Database","text":"<p>In this section, we\u2019ll create an output directory where the downloaded Neurosynth data will be stored. The NiMARE function <code>fetch_neurosynth</code> is used to download the data.</p> <p>Note</p> <p>Ensure that you replace the path in <code>out_dir</code> with one that matches your own directory structure.</p> <pre><code># --- Step 2: Downloading the Neurosynth Database ---\n\n# Set the output directory for storing downloaded Neurosynth data\n# Make sure to change this to a directory path on your own machine\nout_dir = os.path.abspath(\"./nimare_data\")\nos.makedirs(out_dir, exist_ok=True)  # Create the directory if it doesn't already exist\n\n# Download Neurosynth data using NiMARE\u2019s `fetch_neurosynth` function\n# Set version to the latest available (version \"7\" as of October 2024)\nfiles = fetch_neurosynth(\n    data_dir=out_dir,  # Specify directory to save data\n    version=\"7\",  # Neurosynth data version to download\n    overwrite=False,  # Set to True to re-download data if it already exists\n    source=\"abstract\",  # Use article abstracts as the source\n    vocab=\"terms\",  # Use \"terms\" as the vocabulary type\n)\nfiles = files[0]\n\n# Print details of the downloaded files to understand the data structure\npprint(files)\n</code></pre> <p>The <code>pprint</code> command should output something like this:</p> <pre><code>pprint(files)  # Display information about the downloaded files\n\n{'coordinates': './nimare_data/neurosynth/data-neurosynth_version-7_coordinates.tsv.gz',\n 'features': [{'features': './nimare_data/neurosynth/data-neurosynth_version-7_vocab-terms_source-abstract_type-tfidf_features.npz',\n               'vocabulary': './nimare_data/neurosynth/data-neurosynth_version-7_vocab-terms_vocabulary.txt'}],\n 'metadata': './nimare_data/neurosynth/data-neurosynth_version-7_metadata.tsv.gz'}\n</code></pre>"},{"location":"research/fmri/fmri-nimare.html#3-converting-downloaded-neurosynth-data-into-a-nimare-compatible-dataset","title":"3. Converting Downloaded Neurosynth Data into a NiMARE-Compatible Dataset","text":"<p>Once Neurosynth data is downloaded, we convert it to NiMARE\u2019s dataset format for easier manipulation and analysis. We\u2019ll then add abstracts to this dataset using study PMIDs.</p> <ol> <li>Define the path for saving the dataset.</li> <li>Convert the data using <code>convert_neurosynth_to_dataset</code>.</li> <li>Add article abstracts with <code>download_abstracts</code>.</li> </ol> <pre><code># --- Step 3: Converting Neurosynth Data to a NiMARE-Compatible Dataset ---\n\n# Define the path for the NiMARE-compatible dataset file\ndataset_path = os.path.join(out_dir, \"neurosynth_dataset.pkl.gz\")\n\n# Convert and save Neurosynth data to NiMARE dataset format if it doesn\u2019t already exist\nif not os.path.exists(dataset_path):\n    neurosynth_dset = convert_neurosynth_to_dataset(\n        coordinates_file=files[\"coordinates\"],  # Path to Neurosynth coordinates\n        metadata_file=files[\"metadata\"],  # Path to metadata file\n        annotations_files=files[\"features\"],  # Path to feature files\n        target=\"mni152_2mm\",\n    )\n    neurosynth_dset.save(dataset_path)  # Save the converted dataset\n    print(f\"Dataset saved to: {dataset_path}\")\nelse:\n    neurosynth_dset = Dataset.load(\n        dataset_path\n    )  # Load existing dataset if already created\n    print(f\"Dataset loaded from: {dataset_path}\")\n\n# Define path for saving the dataset with abstracts\nabstracts_path = os.path.join(out_dir, \"neurosynth_dataset_with_abstracts.pkl.gz\")\n\n# Download and add abstracts to the dataset if it doesn\u2019t already include them\nif not os.path.exists(abstracts_path):\n    # Note: Replace \"example@example.edu\" with your email address for access to PubMed\n    neurosynth_dset = download_abstracts(neurosynth_dset, \"example@example.edu\")\n    neurosynth_dset.save(abstracts_path)  # Save dataset with abstracts\n    print(f\"Dataset with abstracts saved to: {abstracts_path}\")\nelse:\n    neurosynth_dset = Dataset.load(\n        abstracts_path\n    )  # Load dataset with abstracts if already created\n    print(f\"Dataset with abstracts loaded from: {abstracts_path}\")\n\n# Verify dataset by printing a sample of the abstracts and counting entries\npprint(neurosynth_dset.texts.head())  # Display the first few entries of abstracts\nprint(f\"Number of abstracts: {len(neurosynth_dset.texts)}\")\n</code></pre>"},{"location":"research/fmri/fmri-nimare.html#4-creating-a-subset-of-data","title":"4. Creating a Subset of Data","text":"<p>The NiMARE dataset allows data manipulation to create subsets that meet specific requirements. This enables running custom meta-analyses on targeted data. You can, for instance, retrieve studies based on specific MNI coordinates, labels (included as features in the Neurosynth database), or search abstracts using specific keywords.</p> <p>The following code example demonstrates how to search for abstracts containing specific keywords.</p> <pre><code># --- Step 4: Creating a Subset of Data ---\n\n# Define a function to search abstracts for specific keywords, which helps create a filtered subset\ndef search_abstracts(dataset, keywords):\n    \"\"\"\n    Search for abstracts containing specified keywords.\n\n    Parameters:\n    - dataset (NiMARE Dataset): Dataset with abstracts.\n    - keywords (list of str): Keywords to search for in abstracts.\n\n    Returns:\n    - list of str: Study IDs of abstracts containing specified keywords.\n    \"\"\"\n    keyword_ids = []  # Initialize list for storing IDs of matching studies\n    abstracts = dataset.get_texts(text_type=\"abstract\")  # Retrieve abstracts\n    study_ids = dataset.ids.tolist()  # Retrieve study IDs for each abstract\n\n    for idx, abstract in enumerate(abstracts):\n        try:\n            # Check if abstract is a string and contains any keyword\n            if isinstance(abstract, str) and any(\n                keyword.lower() in abstract.lower() for keyword in keywords\n            ):\n                study_id = study_ids[idx]  # Get corresponding study ID\n                keyword_ids.append(study_id)  # Add matching study ID to list\n        except Exception as e:\n            # Print error details if an exception occurs\n            print(f\"Error processing abstract ID {idx}: {str(e)}\")\n            print(f\"Abstract content (partial): {str(abstract)[:100]}...\")\n            print(\"Skipping this abstract.\")\n\n    return keyword_ids\n\n# Define keywords to search for in the abstracts\nkeywords = [\"face recognition\"]\n\n# Execute the search function and print matching study IDs\nmatching_ids = search_abstracts(neurosynth_dset, keywords)\nprint(f\"Number of abstracts containing {keywords}: {len(matching_ids)}\")\nprint(f\"IDs of matching abstracts: {matching_ids}\")\n\n# Filter dataset to include only studies with keywords in abstracts\ndset_filtered_by_abstract = neurosynth_dset.slice(\n    matching_ids\n)  # Keep only selected studies\npprint(dset_filtered_by_abstract.metadata)  # Verify metadata of the filtered dataset\n</code></pre>"},{"location":"research/fmri/fmri-nimare.html#5-running-meta-analysis","title":"5. Running Meta-Analysis","text":"<p>With the filtered studies, we can now perform meta-analyses using algorithms available in NiMARE.</p> <ol> <li> <p>Available algorithms:</p> <ul> <li>ALE: Identifies regions where studies converge on activation with a probabilistic approach.</li> <li>MKDA: Focuses on the consistency of findings across studies, considering dispersed activations.</li> </ul> </li> <li> <p>Correction methods:</p> <ul> <li>FDR: Controls for expected false positives across rejected hypotheses.</li> <li>FWER (Bonferroni): Controls for a single false positive but is highly conservative.</li> <li>FWER (Monte Carlo): Suitable for spatially dependent data and ideal for whole-brain analyses.</li> </ul> </li> </ol> <p>The following code exmple shows how to run an ALE meta-analysis with Monte Carlo correction</p> <pre><code># --- Step 5: Running Meta-Analysis ---\n\n# Create dir to store cache (useful for low memory)\ncache_dir = os.path.join(out_dir, \"cache\")\n\nif os.path.exists(cache_dir):\n    shutil.rmtree(cache_dir) # Delete the old cache folder if it exists\n\nos.makedirs(cache_dir, exist_ok=True)\n\n# Set up the ALE meta-analysis algorithm for activation likelihood estimation\nale = ALE(\n    kernel__sample_size=10,\n    memory=cache_dir,\n    n_cores=-1,\n    memory_level=100,\n)  # Set sample size to control for experiment variability\nale_results = ale.fit(dset_filtered_by_abstract)  # Run ALE on the filtered dataset\n\n# Apply Monte Carlo correction to the ALE results to control for family-wise error\ncorr = FWECorrector(\n    method=\"montecarlo\", n_iters=100, n_cores=-1, voxel_thresh=0.05, vfwe_only=True\n)  # Set up Monte Carlo correction, 10k iterations for publication level\ncres = corr.transform(ale_results)  # Apply correction\n\n# Save corrected maps, such as z-scores and p-values, to the output directory\ncres.save_maps(\"./nimare_data\")  # Save results in the specified directory\n</code></pre>"},{"location":"research/fmri/fmri-nimare.html#6-extracting-peak-activation-coordinates","title":"6. Extracting Peak Activation Coordinates","text":"<p>The results of the meta-analysis can now be used for further analysis and reporting, such as plotting and extracting the ROIs names at the clusters peak activations.</p> <p>This can be done for the uncorrected z maps, or for the Montecarlo corrected ones.</p> <p>To plot the maps:</p> <pre><code># --- Step 6: Plot Peak Activation Coordinates in MNI ---\n\n# Get z-statistics map\nz_img = cres.get_map(\"z\")\nstat_threshold = norm.isf(0.001)\n\nplot_stat_map(\n    z_img,\n    display_mode=\"mosaic\",\n    draw_cross=False,\n    cmap=\"RdBu_r\",\n    symmetric_cbar=True,\n    threshold=stat_threshold,\n    title=\"Uncorrected z maps (p&lt;.001)\",\n)\n\n# Get FWE-corrected z-statistics map\nz_corr_img = cres.get_map(\"z_level-voxel_corr-FWE_method-montecarlo\")\nstat_threshold = norm.isf(0.05)\n\n# Display the thresholded z-statistics map in MNI space\nplot_stat_map(\n    z_corr_img,\n    display_mode=\"mosaic\",\n    draw_cross=False,\n    cmap=\"RdBu_r\",\n    symmetric_cbar=True,\n    threshold=stat_threshold,\n    title=\"Montecarlo corrected z maps (p &lt; .05)\",\n)\n\n# Generate cluster table and image based on the corrected z-statistics map\ncluster_table = get_clusters_table(\n    z_corr_img, stat_threshold=stat_threshold, cluster_threshold=20\n)\n</code></pre> <p>Which should produce the following plots for the uncorrected z scores and Montecarlo corrected scores:</p> <p></p> <p></p> <p>Warning</p> <p>The Monte Carlo algorithm may produce slightly different results, nd will be more precise with more iterations (10.000 iterations are reccommended for publication-level analyses).</p> <p>And for the region names:</p> <pre><code># --- Step 7: Extracting Peak Activation Region Names ---\n# Load the Harvard-Oxford atlas\n\n# All the Harvard-Oxford atlases can be visualized here:\n# https://neurovault.org/collections/262/\n# The nomenclature for these atlases follow this structure:\n# HarvardOxford &lt;area&gt; maxprob thr&lt;threshold&gt; &lt;resolution&gt;mm, where:\n# &lt;area&gt; can be \"cort\" for cortex, \"sub\" for sub-cortical\n# &lt;threshold&gt; is the probability threshold: 0 (bigger ROIs), 25, 50 (smaller ROIs)\n# &lt;resolution&gt; the resolution of the atlas. Can be 1 or 2 mm (same as your images).\natlas = datasets.fetch_atlas_harvard_oxford(\"cort-maxprob-thr0-2mm\")\n\n# Function to map MNI coordinates to the corresponding ROI name\ndef get_region_name(x, y, z, atlas):\n\n    # Get data from the atlas\n    atlas_img = atlas.maps  # Load the atlas map\n    atlas_data = atlas_img.get_fdata()  # Get the atlas data\n    affine = atlas_img.affine  # The affine matrix for the atlas\n    atlas_labels = atlas.labels  # The list of atlas labels (region names)\n\n    # Convert MNI coordinates to voxel indices\n    voxel_indices = np.round(\n        nib.affines.apply_affine(np.linalg.inv(affine), [x, y, z])\n    ).astype(int)\n\n    # Get the label index at the voxel location\n    label_index = atlas_data[tuple(voxel_indices)]\n\n    # Map label index to region name, with a check for out-of-bound or unknown regions\n    return (\n        atlas_labels[int(label_index)]\n        if label_index &gt; 0 and int(label_index) &lt; len(atlas_labels)\n        else \"Unknown\"\n    )\n\n# Retrieve the ROI name for each MNI coordinate\ncluster_table[\"Region\"] = cluster_table.apply(\n    lambda row: get_region_name(row[\"X\"], row[\"Y\"], row[\"Z\"], atlas),\n    axis=1,\n)\n\n# Display the updated table with voxel values\nprint(cluster_table)\n\ncluster_table.to_csv(\n    os.path.join(out_dir, \"peak_activation_coordinates.csv\"), index=False\n)  # Save CSV without row index\nprint(\"Peak information saved to peak_activation_coordinates.csv\")\n</code></pre> <p>Which should output:</p> <pre><code>  Cluster ID     X  ...  Cluster Size (mm3)                                    Region\n0          1 -36.0  ...                6840                  Occipital Fusiform Gyrus\n1         1a -38.0  ...                            Temporal Occipital Fusiform Cortex\n2          2 -18.0  ...                1016                                   Unknown\n3          3  20.0  ...                 784                                   Unknown\n4          4  42.0  ...                8264                  Occipital Fusiform Gyrus\n5          5  44.0  ...                 544  Inferior Frontal Gyrus, pars opercularis\n</code></pre> <p>Note</p> <p>In the table above, the \"Unknown\" labels are automatically assigned to un-labeled voxels -- i.e., voxels not belonging to any ROI in the atlas (sub-cortical, outside the brain, etc.). In this specific example, the \"Unknown\" ROIs are the two anterior small blobs, visible in the second plot at <code>z = -14</code>. These voxels are not part of any ROI in the Harvard-Oxford atlas, but seem to fall close to the inferior frontal gyrus.</p>"},{"location":"research/fmri/fmri-nimare.html#full-code-example","title":"Full code example","text":"<p>Here you can find the full code example:</p> <pre><code>#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nNiMARE Meta-Analysis\n\nThis guide demonstrates how to download and process the Neurosynth database using NiMARE,\na Python-based framework designed for neuroimaging meta-analyses.\n\nThis step-by-step script covers:\n1. Downloading the Neurosynth data\n2. Converting it into a NiMARE-compatible dataset\n3. Adding article abstracts\n4. Filtering studies based on specific abstract keywords.\n5. Extracting and plotting significant voxels clusters.\n\nBefore starting, ensure NiMARE and required dependencies are installed in your environment. Open your terminal and install the required packages with:\n\npip install nimare biopython pprint scipy nibabel numpy pandas nilearn\n\nCreated on Tue Oct 29 13:45:16 2024\n\n@authors: Emma Goris, Andrea Costantino\n\"\"\"\n\n# --- Step 1: Import All Necessary Packages ---\n\nimport os\nimport shutil\nimport numpy as np\nfrom scipy.stats import norm\nfrom nimare.dataset import Dataset\nfrom nimare.extract import download_abstracts, fetch_neurosynth\nfrom nimare.io import convert_neurosynth_to_dataset\nfrom nimare.meta.cbma.ale import ALE  # ALE algorithm for meta-analysis\nfrom nimare.correct import FWECorrector  # Family-wise error correction\nfrom pprint import pprint\nfrom nilearn.plotting import plot_stat_map\nfrom nilearn import datasets  # Neuroimaging datasets\nfrom nilearn.reporting import get_clusters_table\nimport nibabel as nib\n\n# --- Step 2: Downloading the Neurosynth Database ---\n\n# Set the output directory for storing downloaded Neurosynth data\n# Make sure to change this to a directory path on your own machine\nout_dir = os.path.abspath(\"./nimare_data\")\nos.makedirs(out_dir, exist_ok=True)  # Create the directory if it doesn't already exist\n\n# Download Neurosynth data using NiMARE\u2019s `fetch_neurosynth` function\n# Set version to the latest available (version \"7\" as of October 2024)\nfiles = fetch_neurosynth(\n    data_dir=out_dir,  # Specify directory to save data\n    version=\"7\",  # Neurosynth data version to download\n    overwrite=False,  # Set to True to re-download data if it already exists\n    source=\"abstract\",  # Use article abstracts as the source\n    vocab=\"terms\",  # Use \"terms\" as the vocabulary type\n)\nfiles = files[0]\n\n# Print details of the downloaded files to understand the data structure\npprint(files)\n\n# Expected output:\n# {'coordinates': './nimare_data/neurosynth/data-neurosynth_version-7_coordinates.tsv.gz',\n#  'features': [{'features': './nimare_data/neurosynth/data-neurosynth_version-7_vocab-terms_source-abstract_type-tfidf_features.npz',\n#                'vocabulary': './nimare_data/neurosynth/data-neurosynth_version-7_vocab-terms_vocabulary.txt'}],\n#  'metadata': './nimare_data/neurosynth/data-neurosynth_version-7_metadata.tsv.gz'}\n\n# --- Step 3: Converting Neurosynth Data to a NiMARE-Compatible Dataset ---\n\n# Define the path for the NiMARE-compatible dataset file\ndataset_path = os.path.join(out_dir, \"neurosynth_dataset.pkl.gz\")\n\n# Convert and save Neurosynth data to NiMARE dataset format if it doesn\u2019t already exist\nif not os.path.exists(dataset_path):\n    neurosynth_dset = convert_neurosynth_to_dataset(\n        coordinates_file=files[\"coordinates\"],  # Path to Neurosynth coordinates\n        metadata_file=files[\"metadata\"],  # Path to metadata file\n        annotations_files=files[\"features\"],  # Path to feature files\n        target=\"mni152_2mm\",\n    )\n    neurosynth_dset.save(dataset_path)  # Save the converted dataset\n    print(f\"Dataset saved to: {dataset_path}\")\nelse:\n    neurosynth_dset = Dataset.load(\n        dataset_path\n    )  # Load existing dataset if already created\n    print(f\"Dataset loaded from: {dataset_path}\")\n\n# Define path for saving the dataset with abstracts\nabstracts_path = os.path.join(out_dir, \"neurosynth_dataset_with_abstracts.pkl.gz\")\n\n# Download and add abstracts to the dataset if it doesn\u2019t already include them\nif not os.path.exists(abstracts_path):\n    # Note: Replace \"example@example.edu\" with your email address for access to PubMed\n    neurosynth_dset = download_abstracts(neurosynth_dset, \"example@example.edu\")\n    neurosynth_dset.save(abstracts_path)  # Save dataset with abstracts\n    print(f\"Dataset with abstracts saved to: {abstracts_path}\")\nelse:\n    neurosynth_dset = Dataset.load(\n        abstracts_path\n    )  # Load dataset with abstracts if already created\n    print(f\"Dataset with abstracts loaded from: {abstracts_path}\")\n\n# Verify dataset by printing a sample of the abstracts and counting entries\npprint(neurosynth_dset.texts.head())  # Display the first few entries of abstracts\nprint(f\"Number of abstracts: {len(neurosynth_dset.texts)}\")\n\n# --- Step 4: Creating a Subset of Data ---\n\n# Define a function to search abstracts for specific keywords, which helps create a filtered subset\ndef search_abstracts(dataset, keywords):\n    \"\"\"\n    Search for abstracts containing specified keywords.\n\n    Parameters:\n    - dataset (NiMARE Dataset): Dataset with abstracts.\n    - keywords (list of str): Keywords to search for in abstracts.\n\n    Returns:\n    - list of str: Study IDs of abstracts containing specified keywords.\n    \"\"\"\n    keyword_ids = []  # Initialize list for storing IDs of matching studies\n    abstracts = dataset.get_texts(text_type=\"abstract\")  # Retrieve abstracts\n    study_ids = dataset.ids.tolist()  # Retrieve study IDs for each abstract\n\n    for idx, abstract in enumerate(abstracts):\n        try:\n            # Check if abstract is a string and contains any keyword\n            if isinstance(abstract, str) and any(\n                keyword.lower() in abstract.lower() for keyword in keywords\n            ):\n                study_id = study_ids[idx]  # Get corresponding study ID\n                keyword_ids.append(study_id)  # Add matching study ID to list\n        except Exception as e:\n            # Print error details if an exception occurs\n            print(f\"Error processing abstract ID {idx}: {str(e)}\")\n            print(f\"Abstract content (partial): {str(abstract)[:100]}...\")\n            print(\"Skipping this abstract.\")\n\n    return keyword_ids\n\n# Define keywords to search for in the abstracts\nkeywords = [\"face recognition\"]\n\n# Execute the search function and print matching study IDs\nmatching_ids = search_abstracts(neurosynth_dset, keywords)\nprint(f\"Number of abstracts containing {keywords}: {len(matching_ids)}\")\nprint(f\"IDs of matching abstracts: {matching_ids}\")\n\n# Filter dataset to include only studies with keywords in abstracts\ndset_filtered_by_abstract = neurosynth_dset.slice(\n    matching_ids\n)  # Keep only selected studies\npprint(dset_filtered_by_abstract.metadata)  # Verify metadata of the filtered dataset\n\n# --- Step 5: Running Meta-Analysis ---\n\n# Create dir to store cache (useful for low memory)\ncache_dir = os.path.join(out_dir, \"cache\")\n\nif os.path.exists(cache_dir):\n    shutil.rmtree(cache_dir) # Delete the old cache folder if it exists\n\nos.makedirs(cache_dir, exist_ok=True)\n\n# Set up the ALE meta-analysis algorithm for activation likelihood estimation\nale = ALE(\n    kernel__sample_size=10,\n    memory=cache_dir,\n    n_cores=-1,\n    memory_level=100,\n)  # Set sample size to control for experiment variability\nale_results = ale.fit(dset_filtered_by_abstract)  # Run ALE on the filtered dataset\n\n# Apply Monte Carlo correction to the ALE results to control for family-wise error\ncorr = FWECorrector(\n    method=\"montecarlo\", n_iters=100, n_cores=-1, voxel_thresh=0.05, vfwe_only=True\n)  # Set up Monte Carlo correction, 10k iterations for publication level\ncres = corr.transform(ale_results)  # Apply correction\n\n# Save corrected maps, such as z-scores and p-values, to the output directory\ncres.save_maps(\"./nimare_data\")  # Save results in the specified directory\n\n# --- Step 6: Plot Peak Activation Coordinates in MNI ---\n\n# Get z-statistics map\nz_img = cres.get_map(\"z\")\nstat_threshold = norm.isf(0.001)\n\nplot_stat_map(\n    z_img,\n    display_mode=\"mosaic\",\n    draw_cross=False,\n    cmap=\"RdBu_r\",\n    symmetric_cbar=True,\n    threshold=stat_threshold,\n    title=\"Uncorrected z maps (p&lt;.001)\",\n)\n\n# Get FWE-corrected z-statistics map\nz_corr_img = cres.get_map(\"z_level-voxel_corr-FWE_method-montecarlo\")\nstat_threshold = norm.isf(0.05)\n\n# Display the thresholded z-statistics map in MNI space\nplot_stat_map(\n    z_corr_img,\n    display_mode=\"mosaic\",\n    draw_cross=False,\n    cmap=\"RdBu_r\",\n    symmetric_cbar=True,\n    threshold=stat_threshold,\n    title=\"Montecarlo corrected z maps (p &lt; .05)\",\n)\n\n# Generate cluster table and image based on the corrected z-statistics map\ncluster_table = get_clusters_table(\n    z_corr_img, stat_threshold=stat_threshold, cluster_threshold=20\n)\n\n# --- Step 7: Extracting Peak Activation Region Names ---\n# Load the Harvard-Oxford atlas\n\n# All the Harvard-Oxford atlases can be visualized here:\n# https://neurovault.org/collections/262/\n# The nomenclature for these atlases follow this structure:\n# HarvardOxford &lt;area&gt; maxprob thr&lt;threshold&gt; &lt;resolution&gt;mm, where:\n# &lt;area&gt; can be \"cort\" for cortex, \"sub\" for sub-cortical\n# &lt;threshold&gt; is the probability threshold: 0 (bigger ROIs), 25, 50 (smaller ROIs)\n# &lt;resolution&gt; the resolution of the atlas. Can be 1 or 2 mm (same as your images).\natlas = datasets.fetch_atlas_harvard_oxford(\"cort-maxprob-thr0-2mm\")\n\n# Function to map MNI coordinates to the corresponding ROI name\ndef get_region_name(x, y, z, atlas):\n\n    # Get data from the atlas\n    atlas_img = atlas.maps  # Load the atlas map\n    atlas_data = atlas_img.get_fdata()  # Get the atlas data\n    affine = atlas_img.affine  # The affine matrix for the atlas\n    atlas_labels = atlas.labels  # The list of atlas labels (region names)\n\n    # Convert MNI coordinates to voxel indices\n    voxel_indices = np.round(\n        nib.affines.apply_affine(np.linalg.inv(affine), [x, y, z])\n    ).astype(int)\n\n    # Get the label index at the voxel location\n    label_index = atlas_data[tuple(voxel_indices)]\n\n    # Map label index to region name, with a check for out-of-bound or unknown regions\n    return (\n        atlas_labels[int(label_index)]\n        if label_index &gt; 0 and int(label_index) &lt; len(atlas_labels)\n        else \"Unknown\"\n    )\n\n# Retrieve the ROI name for each MNI coordinate\ncluster_table[\"Region\"] = cluster_table.apply(\n    lambda row: get_region_name(row[\"X\"], row[\"Y\"], row[\"Z\"], atlas),\n    axis=1,\n)\n\n# Display the updated table with voxel values\nprint(cluster_table)\n\ncluster_table.to_csv(\n    os.path.join(out_dir, \"peak_activation_coordinates.csv\"), index=False\n)  # Save CSV without row index\nprint(\"Peak information saved to peak_activation_coordinates.csv\")\n</code></pre>"},{"location":"research/fmri/fmri-procedure.html","title":"Practical scanning protocol","text":"<p>This page outlines the procedures followed by our lab at MR8.</p> <p>For a quick overview of all the steps on the day of a scanning section, please consult the MRI checklist</p>"},{"location":"research/fmri/fmri-procedure.html#general-information","title":"General Information","text":"<p>The MR8 suite houses a Philips Ingenia scanner with a 32-channel head coil, located in MR suite E408 (map). Detailed scanner specifications can be found in the manual.</p> <p>Important Notes</p> <ul> <li>There is no cell phone service inside the MR suite. Use the control room phone for external calls (pick up the phone, dial <code>0</code> and wait a bit, then write the full number).</li> <li>After 6 pm and on weekends, two certified MR users (MRRUs) are required to run a session. More details are available in the Safety Rules &amp; Procedures.</li> <li>You can find relevant phone numbers to call for urgent questions as well as usernames and passwords of the PCs in the scan console room in this file in the Hoplab Teams folder.</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#mr8-equipment","title":"MR8 Equipment","text":"<p>The diagram above provides a (non-exhaustive) overview of the MR8 suite's equipment that we normally use and how instruments are connected, which we hope will help in understanding the setup and troubleshooting various issues. A more thorough description of the equipment is available in the manual.</p> <p>Systems are color-coded, and can be read as follow:</p> <ul> <li>Red lines and boxes indicate connections from the scanner:<ul> <li>TTL Pulse a.k.a. the \"trigger\", which is used among other things to synchronize the fMRI task with the scan.</li> <li>Data connections from the scanner to the PC.</li> </ul> </li> <li>Blue lines and boxes indicate button boxes, and double lines represent optical fiber connections. Specifically:<ul> <li>Nata box, 5 buttons. Important: if you use this box, make sure your code can differentiate between the <code>5</code> button code and the <code>5</code> trigger code. This can be done programmatically, and it is addressed in recent versions of our scripts. An alternative workaround would be to relay the trigger thorugh the diamond box, which sends the <code>T</code> trigger code instead of the usual <code>5</code>. If you need to do this, make sure you switch back to the original set-up at the end of your scanning session.</li> <li>Diamond box, 4 buttons. This box is marked with a red tape. When the trigger box is connected to this box (which should not be the case), this box relays the trigger <code>T</code> to the stim PC.</li> <li>2-buttons box. When the trigger box is connected to this box (which is the expected and usual set-up), this box relays the trigger <code>5</code> from the scanner to the Stim PC.</li> </ul> </li> <li>Green lines and boxes indicate Eye-tracking instruments and connections<ul> <li>Eyelink 1000 long range system, including the camera and infrared light source</li> <li>ET box to convert analog input from the EL-1000 to digital.</li> <li>ET PC to run the Eyelink software and control recordings and settings.</li> <li>Power for the ET system is located in the back room.</li> </ul> </li> <li>Purple lines and boxes connect the MRI control PC (old system, which we use)</li> <li>Gold lines and boxes connect the MRI control PC (new system, which we DO NOT use)</li> <li>Dotted lines mostly relay audio/video </li> <li>Dashed lines are power lines.</li> </ul> <p>Other instruments we use are:</p> <ul> <li>the 3NB light filter, which should be placed between the projector and the MRI compatible screen at  the back of the scanner.</li> <li>the Headphones, which are used as a safety measure for the participant, and to deliver audio when needed. These headphones are MRI compatible and work using the magnetic field produced by the scanner. That means that they will not work outside of the scanner, and the sounds will get increasingly strong as you move inside the scanner. The sound for these (yellow) headphones is sent from the stim PC through the Lindy box, passes through the amplifier located next to the stim PC, and it finally reaches the HP. Volume can be controlled through the amplifier or the stim PC general volume level.</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#required-forms","title":"Required Forms","text":"<p>Before going to the hospital, ensure that you have the following forms ready:</p> <ul> <li> <p>Consent Form: Must be signed by both the participant and yourself before the session. Keep it safe post-session.</p> </li> <li> <p>MR Safety Checklist: This form should be filled out by the participant prior to the session. You can send it in advance to avoid delays on the day of the scan. When the radiology desk is open, this form should be handed to the desk personnel, which will stamp it and confirm your scanning session. Link</p> </li> <li> <p>Check-In/Check-Out Form: Fill this form when entering and leaving the MR suite. The timings you indicate on there will be used to record our usage of the scanner and the amount of hours to pay for. Leave this form on the dedicated tray, next to that of the screening questionnaires. Link</p> </li> </ul>"},{"location":"research/fmri/fmri-procedure.html#participant-arrival-and-registration","title":"Participant Arrival and Registration","text":""},{"location":"research/fmri/fmri-procedure.html#participant-registration","title":"Participant Registration","text":"<p>Upon arriving at the hospital, participants must register at the main entrance. The registration procedure varies depending on whether they hold a Belgian ID:</p> <ul> <li>Belgian eID-card: Use the self-service kiosks or the Mynexuzhealth app for fast registration.</li> <li>No Belgian eID-card: Participants must visit the registration desks (open from 7:00 AM to 6:30 PM).</li> <li>Children under 12: Register using a Kids-ID or ISI+ card (for those with Belgian Social Security but without a Belgian ID).</li> </ul> <p>Participants must wait at the main entrance at least 30 minutes before their scheduled scanning session. A researcher will meet them and guide them through the check-in process.</p> Weekdays (before 5:30 PM)After 5:30 PM or Weekends/Holidays <p>The researcher will guide the participant through the process, ensuring that:</p> <ul> <li>The MR Safety Checklist and Consent Form are completed at the hospital entrance. Family members or anyone accompanying the participant into the Inner Controlled Area must also complete the MR Safety Checklist.</li> <li>The MR Safety Checklist is submitted at the \"Wachtzaal Radiologie\" registration desk.</li> <li>The secretary scans the participant's screening questionnaire into their medical file.</li> <li>The researcher brings the stamped questionnaire to the MRI scanner.</li> </ul> <p>Important</p> <p>The MR Safety Checklist must be submitted at the Radiology desk. Failure to do so will prevent the participant from being listed in the Radiology Information System (RIS), which could cause delays.</p> <ul> <li>The MR Safety Checklist and Consent Form are completed at the hospital entrance.</li> <li>The researcher and participant will proceed directly to the MR8 suite together.</li> </ul> <p>Controlled Areas Access</p> <p>Access to the Controlled Areas (MRI suite) is restricted and requires a KU Leuven card. Participants are not permitted to enter these areas until the MR Safety Checklist and Consent Form are signed.</p>"},{"location":"research/fmri/fmri-procedure.html#at-the-scanner-control-room","title":"At the Scanner Control Room","text":"<p>Upon arriving in the scanner control room, follow these steps to ensure smooth preparation and transition to the scanning session:</p> <ol> <li> <p>Task Explanation:</p> <ul> <li>Clearly explain the task and any relevant stimuli to the participant. If applicable, show them examples of the stimuli or allow them to practice a few trials to help them understand the task before scanning begins.</li> </ul> </li> <li> <p>Form Submission:</p> <ul> <li>Confirm that the Consent Form and MR Safety Checklist have been signed. These forms must be signed before entering any Controlled Area.</li> <li>Ensure that the check-in section of the Check-in/out form is completed.</li> <li>Place the signed forms in the designated trays in the scanner control room.</li> </ul> </li> <li> <p>Pre-scan Preparation:</p> <ul> <li>Remind the participant to use the bathroom if needed. The bathroom is located behind the orange door, across from the control room.</li> <li>Check that neither you nor the participant have any metal or magnetic items on your body (e.g., watches, hair clips, bank cards, festival bracelets, belts, phones). If the participant prefers not to remove a festival bracelet, you can cover the metal portion with tape.</li> </ul> </li> <li> <p>Personal Belongings:</p> <ul> <li>Store the participant's personal belongings in the lockers (refer to locker no. 1 on the map of the MR8 suite).</li> </ul> </li> <li> <p>Final Check:</p> <ul> <li>Double-check that all necessary forms have been completed and submitted.</li> <li>Confirm that the participant and all accompanying individuals are prepared to enter the scanning suite with no metal or magnetic items.</li> </ul> </li> <li> <p>Optional Comfort:</p> <ul> <li>If the participant is expected to be in the scanner for an extended period, offer them water or a snack beforehand to ensure they are comfortable.</li> </ul> </li> </ol>"},{"location":"research/fmri/fmri-procedure.html#preparation-of-the-scanner-area","title":"Preparation of the Scanner Area","text":"<p>Warning</p> <p>After leaving the scanner room, always lock the door.  If the door isn\u2019t locked properly, the scan console will display an error when you try to start scanning.</p>"},{"location":"research/fmri/fmri-procedure.html#stimulus-pc","title":"Stimulus PC","text":"<p>The stimulus computer's desktop is located in the control room. It is the second-last computer from the right, between the eye-tracking computer (last) and MRI control computer.</p> <ol> <li> <p>Logging In:</p> <ul> <li>Use the provided username and password. Login details can be found here.</li> </ul> <p>Password Not Accepted?</p> <p>If the password is not accepted, check for a qwerty-azerty keyboard mismatch. Press <code>alt+shift</code> and ensure EN is selected on the login screen.</p> </li> <li> <p>Storing Experiment Files:</p> <ul> <li>Store your experiment folders under: <code>C:\\Research\\Psychology\\</code>  (Create your own folder within this directory.)</li> </ul> </li> <li> <p>Installed Software:</p> <ul> <li>Matlab 2011b, 2015a, and Psychtoolbox 3.0.123 are installed.</li> </ul> <p>Tip</p> <p>If Matlab freezes or shows a JAVA error, restarting Matlab should fix the issue.</p> </li> <li> <p>Screen Resolution:</p> <ul> <li>The resolution is set to 1920 x 1080 (landscape).</li> <li>To flip the screen, adjust the projector settings, not the computer.</li> <li>Screen Width: 28.35 visual degrees</li> </ul> </li> </ol>"},{"location":"research/fmri/fmri-procedure.html#trigger-boxes","title":"Trigger Boxes","text":"<p>The scanner sends a trigger \"5\" to the stimulus computer. Different setups are used for static and dynamic stimuli:</p> Static StimuliDynamic Stimuli (e.g., movies) <p>A single wire connects two button boxes, each with 2 buttons:</p> <ul> <li>Box 1: <ul> <li>Blue button = Trigger 1</li> <li>Yellow button = Trigger 2</li> </ul> </li> <li>Box 2: <ul> <li>Green button = Trigger 3</li> <li>Red button = Trigger 4</li> </ul> </li> </ul> <p>A response box with 4 buttons:</p> <ul> <li>Blue button = Trigger \"b\"</li> <li>Yellow button = Trigger \"y\"</li> <li>Green button = Trigger \"g\"</li> <li>Red button = Trigger \"r\"</li> </ul> <p>Check Trigger Outputs</p> <p>Before starting the experiment, verify that the buttons provide the expected outputs on the stimulus PC screen. If no triggers are working:</p> <ul> <li>Restart Matlab and/or the stimulus computer.</li> <li>Check if any cables have been left disconnected. The response box is on top of the stimulus desktop PC in the control room. Ensure both cables are properly connected.</li> </ul> <p></p>"},{"location":"research/fmri/fmri-procedure.html#common-issues","title":"Common Issues","text":"Button Box Not Responding <ol> <li>Restart Matlab.</li> <li>Reset the button boxes in the technical room by unplugging and reconnecting the power cables.</li> <li>If the problem persists, restart the stimulus computer.</li> </ol> Trigger Not Working <ol> <li>Restart Matlab and check for responses from the button box.</li> <li>Ensure the trigger passes through the static stimuli box (check if the boxes are responsive).</li> <li>Verify that all cables are connected properly. The response box is on the table next to the desktop PC in the technical room.</li> </ol> Restarting the Scanner <p>Do not do this without the approval of Ron or Stefan. If the trigger still doesn\u2019t work, you may need to restart the scanner:</p> <ol> <li>Ensure the volunteer is out of the scanner first.</li> <li>Go to the technical room and locate the box with the red stop and green start buttons.</li> <li>Press the red button to stop the scanner. Wait 10 seconds, then press the green button to restart it.</li> <li>Log back into the scanner computer using MRService credentials.</li> <li>Wait until all components are ready and restart the software.    Confirm any errors, such as helium pressure alerts, by pressing OK.</li> </ol>"},{"location":"research/fmri/fmri-procedure.html#scanner-table-setup","title":"Scanner table setup","text":"<ol> <li> <p>Cover Cushions:      Always cover the cushions with paper towels before use.</p> </li> <li> <p>Keep Equipment Off the Floor:      Do not place cushions or equipment on the floor. If any are found on the floor, place them on the shelves.</p> </li> <li> <p>Patient Table Setup:</p> <ul> <li>The 32-channel coil should be placed ~10 cm from the edge of the table.</li> <li>Coil connections:  <ul> <li>Left lower plug and right upper plug.</li> </ul> </li> <li>Headphones:   Plug into the upper left connector at the top of the table.</li> <li>Panic Button:   Plug into the lower left connector at the bottom of the table.</li> </ul> </li> </ol> <p>Running Low on Supplies?</p> <p>If you run out of supplies (e.g., paper towels), you can find new ones in the closet right in front of you when entering MR suite E408. Paper rolls are stored on top.</p>"},{"location":"research/fmri/fmri-procedure.html#projection-screen","title":"Projection Screen","text":"<ul> <li> <p>Correct Position:     Ensure the back of the screen is aligned with the black marks on the scanner table.</p> </li> <li> <p>Handling:     Never touch the projection side of the screen. Use the plastic stand at the bottom if you need to move it.</p> </li> </ul>"},{"location":"research/fmri/fmri-procedure.html#projector-filter","title":"Projector Filter","text":"<p>Ensure that filter 3NB (1.34% light transmission) is placed in front of the projector tunnel for consistency across scan sessions.</p> <p>MR8 offers four filter options, each with different light transmission levels:</p> Filter Light Transmission 3NB 1.34% A+B 4.27% A+C 4.86% Unnamed (grey tape) 69.3% <p>You can combine filters to adjust the luminance.</p> <p>Handle Filters with Care</p> <p>Filters are fragile. Always hold them by the frame to avoid damage. Filters are stored in the top left drawer of the cabinet in the scanner room.</p>"},{"location":"research/fmri/fmri-procedure.html#projector-usage","title":"Projector Usage","text":"<ol> <li> <p>Powering On:    The projector brand is NEC. Use the remote (button on the top right) to turn it on.</p> </li> <li> <p>Adjusting the Lens:    If the lens is out of position, use the buttons next to the lens on the projector to adjust \u2014 do not touch the lens directly.</p> </li> </ol>"},{"location":"research/fmri/fmri-procedure.html#common-issues_1","title":"Common Issues","text":"<p>If the screen is showing a blue window or incorrect display:</p> <ul> <li>Check that the projector cable is properly connected to the stimulus computer.</li> <li>Ensure the source is set to DisplayPort.      Press the DisplayPort button on the remote to reset the projector to standard settings.</li> </ul> <p>Viewing Projector Menu</p> <p>To view the projector menu, you'll need to be inside the scanner room with the remote. Remove the filter, then use the remote inside the scanner to see the menu options on the projection screen.</p>"},{"location":"research/fmri/fmri-procedure.html#getting-the-volunteer-ready-for-the-scanner","title":"Getting the Volunteer Ready for the Scanner","text":""},{"location":"research/fmri/fmri-procedure.html#earplugs-and-headphones","title":"Earplugs and Headphones","text":"<p>Volunteers must wear earplugs and white earphones. New white earpad covers can be found:</p> <ul> <li>On top of the cabinet to the right</li> <li>Inside the cabinet in the hallway (enter, go straight, then turn right)</li> </ul> <p>Mandatory Hearing Protection</p> <p>Volunteers who refuse to wear the provided hearing protection cannot be scanned.</p>"},{"location":"research/fmri/fmri-procedure.html#coil-setup","title":"Coil Setup","text":"<ol> <li> <p>Positioning the Volunteer:</p> <ul> <li>Ask the volunteer to lie down on the patient table with their head fitted inside the coil.</li> <li>Ensure they don\u2019t hit their head when positioning.</li> <li>Place a knee cushion under the volunteer's legs for comfort.</li> </ul> </li> <li> <p>Head Stability:</p> <ul> <li>Secure the volunteer's head using washcloths placed between the sides of the head and the coil.</li> <li>Make sure the volunteer is comfortable, ensuring the washcloths do not apply too much pressure.</li> </ul> </li> <li> <p>Panic Button:</p> <ul> <li>Attach the panic button to the volunteer's clothing (within easy reach).</li> <li>Do not place it in their hand.  </li> <li>Explain its operation and test the alarm before starting the session.</li> </ul> </li> <li> <p>Response Button Box:</p> <ul> <li>Hand the response button box to the volunteer and repeat instructions on which buttons to use.</li> </ul> </li> <li> <p>Positioning the Coil:</p> <ul> <li>Place the top of the coil onto the bottom part, ensuring the volunteer's nose/head is not touching the coil.</li> <li>Align the volunteer\u2019s eyebrows with the calibration line on the coil. This ensures the head is positioned at the center of the magnet.</li> <li>Secure the coil by clipping it with the gray handle.</li> </ul> </li> </ol>"},{"location":"research/fmri/fmri-procedure.html#mirror-and-screen-alignment","title":"Mirror and Screen Alignment","text":"<p>Preferred Mirror</p> <p>Use the single square-shaped mirror (preferred for better resolution over the double mirror).</p> <ol> <li> <p>Attach the Mirror:</p> <ul> <li>Slide the mirror on top of the head coil.</li> <li>There are two ways to align it easily:</li> <li>Click the mirror on and slide it forward to block the calibration light. Then, adjust and insert the volunteer.</li> <li>Align the mirror, click it on, adjust the position, and insert the volunteer.</li> </ul> </li> <li> <p>Volunteer Adjustment:</p> <ul> <li>The volunteer can adjust the mirror themselves if needed by sliding it backward or forward to get a clear view of the screen.</li> </ul> </li> </ol>"},{"location":"research/fmri/fmri-procedure.html#table-calibration","title":"Table calibration","text":"<p>The control panels on the left and right of the scanner have identical functions. Use the same switch to move the table up/down and backward/forward.</p> <ol> <li> <p>Table Height:</p> <ul> <li>If the table is down, raise it by pressing the up button until it stops at the maximum height.</li> </ul> </li> <li> <p>Moving the Table Inside:</p> <ul> <li>Use the up button to move the table inside the scanner.</li> </ul> </li> <li> <p>Eye Protection:</p> <ul> <li>Ask the volunteer to close their eyes and gently cover their eyes with your hand.</li> </ul> </li> <li> <p>Calibration Laser:</p> <ul> <li>Press the button with the light bulb icon to activate the red calibration laser.</li> <li>Line up the laser with the volunteer\u2019s eyebrows.  </li> </ul> <p>Tip</p> <p>If the laser turns off, press the button again to reactivate it.</p> </li> <li> <p>Confirm Coil Position:</p> <ul> <li>Press the button below the light bulb button to confirm the coil's position (a green light will indicate confirmation).</li> <li>The table will move automatically into the scanner when the button is held for a few seconds.</li> </ul> <p>Note</p> <p>Place the mirror on top of the coil before sliding the volunteer into the scanner.</p> </li> </ol>"},{"location":"research/fmri/fmri-procedure.html#common-issues_2","title":"Common Issues","text":"Table calibration failed <ul> <li>If the table moves too far inside the scanner, calibration may have failed. Slide the table out of the scanner, recalibrate, and try again.</li> <li>If the scanner light remains on, use the control buttons to switch it off.</li> <li>For fMRI studies, maintain consistent lighting throughout the session by using the outer circle on the control panel to switch the light on or off.</li> </ul> Green calibration light already on <ul> <li>If the green calibration light is already on before positioning the table correctly, move the table out of the scanner to reset the calibration. The light will turn off, allowing you to restart the calibration process.</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#inserting-the-volunteer-into-the-scanner","title":"Inserting the Volunteer into the Scanner","text":"<ol> <li> <p>Move the Table:</p> <ul> <li>Use the backward/forward switch to move the table into the scanner. The table will stop automatically when it reaches the correct position according to the calibration.</li> </ul> </li> <li> <p>Cable Management:</p> <ul> <li>Hold the cables while moving the volunteer to prevent them from stretching.</li> </ul> </li> <li> <p>Comfort Check:</p> <ul> <li>Ensure the volunteer is comfortable in the scanner, without crossed arms or legs (to avoid forming current loops).</li> <li>Make sure no metal or wires are touching the volunteer\u2019s skin or the bore of the scanner.</li> <li>No wires should be looped within the scanner bore.</li> </ul> </li> </ol>"},{"location":"research/fmri/fmri-procedure.html#common-issues_3","title":"Common Issues","text":"Mirror doesn\u2019t fit in the bore <ul> <li>Check if the washcloths are stuck between the edges of the coil, as this could lift the coil's top.</li> <li>Ensure the bottom part of the coil is properly slotted into the grooves on the table.</li> </ul> Table doesn\u2019t move <ul> <li>The table might be disconnected. Press the button located at the bottom right, next to the red button (Button 2), to reconnect the table.</li> </ul> Table Moves Too Far Inside Scanner <p>If the table moves too far inside the scanner, it indicates a calibration failure:</p> <ol> <li>Slide the table out of the scanner.</li> <li>Recalibrate the patient\u2019s position using the calibration laser.</li> </ol>"},{"location":"research/fmri/fmri-procedure.html#screen-visibility","title":"Screen Visibility","text":"<p>Before leaving the room, check the following:</p> <ul> <li>Ask the volunteer if the screen is fully visible and centered.</li> <li>Check the screen yourself to ensure it is aligned with the black marks on the scanner table.</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#eyetracker-setup","title":"Eyetracker Setup","text":""},{"location":"research/fmri/fmri-procedure.html#positioning-the-participant","title":"Positioning the Participant","text":"<ol> <li>Use the square mirror with front reflection.</li> <li>Position the participant\u2019s head as high as possible in the coil (to reduce shadows on the face).</li> <li>Support the participant\u2019s neck with washcloths to tilt the head back for better eye visibility.</li> </ol>"},{"location":"research/fmri/fmri-procedure.html#eyetracker-startup","title":"Eyetracker Startup","text":"<ol> <li> <p>Scanner area:</p> <ul> <li>Connect the eyetracker plug to the power supply (marked with a white tag: \u201ceyetracking\u201d).</li> <li>Ensure the screen is aligned with the EYE line.</li> <li>Check if the eyetracker setup is aligned with the floor marks.</li> </ul> </li> <li> <p>Control room - Eyetracker PC:</p> <ul> <li>Boot the Eyelink software (default option in the Windows Boot Manager).</li> <li>If Eyelink doesn\u2019t start, press <code>t</code> followed by Enter to launch it manually.</li> </ul> </li> <li> <p>Control room - Stimulus PC:</p> <ul> <li>Open the track2popup to view the eye on the screen and adjust the sharpness.</li> </ul> </li> </ol>"},{"location":"research/fmri/fmri-procedure.html#eyelink-camera-setup","title":"Eyelink camera setup","text":"<ol> <li>Press <code>ENTER</code> to begin Camera Setup.</li> <li>Adjust the camera position by holding a finger in front of it to check where it\u2019s pointing.</li> <li>Ensure both the pupil and corneal reflex (CT) are well-detected.</li> <li>Adjust the pupil threshold using the up/down arrows for the clearest possible image.</li> </ol>"},{"location":"research/fmri/fmri-procedure.html#calibration-validation","title":"Calibration &amp; Validation","text":"<ol> <li> <p>Calibration:</p> <ul> <li>Before starting Smartbrain, ensure the participant is in the optimal head position for calibration.</li> <li>Press <code>C</code> to start calibration, and guide the participant to focus on the dots.</li> <li>When the word \"stable\" appears, press <code>SPACEBAR</code> 9 times for each point.</li> </ul> </li> <li> <p>Validation:</p> <ul> <li>Press <code>V</code> to start validation, guiding the participant to focus on the dots.</li> <li>If successful, press ACCEPT. If validation fails, recalibrate if needed.</li> </ul> </li> <li> <p>Recording:</p> <ul> <li>Open a new file before each functional run and press RECORD at the start of the run.</li> <li>Stop recording by pressing CLOSE FILE at the end of the run.</li> </ul> </li> </ol> <p>Controlling EyeLink</p> <p>It is advisable to control the calibration, validation, recordings and data collection from the script you use for your fMRI task.</p>"},{"location":"research/fmri/fmri-procedure.html#common-issues_4","title":"Common Issues","text":"CalPopUp2 Issues <p>If CalPopUp2 does not start properly (errors or failure to create a new file), restarting the stimulus PC should solve the issue.</p> Tracking Issues <p>If the eye is not being tracked during camera setup: - Ask the participant to adjust the mirror for better light. - Add washcloths under their neck to tilt the head back for a clearer view.</p>"},{"location":"research/fmri/fmri-procedure.html#prepare-and-start-scanning","title":"Prepare and Start Scanning","text":""},{"location":"research/fmri/fmri-procedure.html#communication-with-the-volunteer","title":"Communication with the Volunteer","text":"<ul> <li>Ask if they are OK before starting.</li> <li>Test the response buttons: Ask the volunteer to press each relevant button one by one, and check the responses on the screen.</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#prepare-the-scanner","title":"Prepare the Scanner","text":"<ol> <li> <p>New Examination:</p> <ul> <li>Select \"Patient \u2013 New Examination\".</li> <li>Choose RIS to load the volunteer's details. Their data (name, ID, birthdate, sex, and exam) will be automatically filled in.</li> </ul> </li> <li> <p>Volunteer Information:</p> <ul> <li>Enter the volunteer\u2019s weight (from the MR Safety Checklist).</li> <li>For women: Answer no for pregnancy.</li> <li>For both men and women: Answer no for implants.</li> <li>Anatomical images are sent automatically to PACS with the correct identification.</li> </ul> </li> <li> <p>Proceed:</p> <ul> <li>Click Proceed to start setting up the exam.</li> </ul> </li> </ol> <p>Lighting Change</p> <p>Starting a new exam will automatically switch on the outer light circle of the scanner. You may want to switch it off manually.</p> <p>Lighting for fMRI Studies</p> <p>Keep the same light settings throughout the entire fMRI study. - The center button turns the light on or off. - Swipe around the center button to dim the lights.</p>"},{"location":"research/fmri/fmri-procedure.html#select-your-exam-card","title":"Select your exam card","text":"<ol> <li> <p>Loading the Exam Card:</p> <ul> <li>To set up an exam card, contact Ron Peeters.</li> <li>Exam cards are stored under <code>/hospital/Research/</code> with names in the format: S-number and Researcher\u2019s Name.</li> <li>Press the + next to the exam card name or drag it to the left of the window.</li> <li>To copy sequences, right-click on the sequence and choose copy. Right-click again to paste (or use shortcuts: <code>Ctrl+C</code> and <code>Ctrl+V</code>).</li> </ul> </li> <li> <p>Standard Exam Setup:</p> <ul> <li>Load the following sequences:</li> <li>Smartbrain</li> <li>Check_fMRI</li> <li>Then your own sequences, such as:<ul> <li>fMRI protocol N=4 (use this to check slice position and timing)</li> <li>fMRI protocol N=X (number of dynamic scans in the actual study)</li> </ul> </li> </ul> </li> </ol>"},{"location":"research/fmri/fmri-procedure.html#common-issues_5","title":"Common Issues","text":"Participant Can\u2019t Be Found in CP <ol> <li>Start a new examination by going to Patients &gt; New Examination.</li> <li>Update the patient list by clicking RIS Configuration, then Proceed.</li> <li>Close the New Examination window and reopen it via Patients &gt; New Examination &gt; RIS.</li> </ol> <p>If no one is available to help, manually fill in the participant's details and send an email to Ron with the following information:</p> Field Value Patient Name Participant's name Registration ID Same as patient name Birthday 01-01-(year of birth) Sex Participant's sex Exam Name Same as patient name Weight Weight from MR Safety Checklist"},{"location":"research/fmri/fmri-procedure.html#start-scanning","title":"Start Scanning","text":"<ol> <li> <p>Smartbrain:</p> <ul> <li>Double-click Smartbrain \u2192 Proceed \u2192 Start scan.</li> </ul> </li> <li> <p>Check fMRI:</p> <ul> <li>After Smartbrain finishes, start Check fMRI.</li> <li>The scan frame should contain the whole brain.</li> <li>Accept \u2192 Proceed.</li> </ul> </li> <li> <p>fMRI Protocol:</p> <ul> <li>Double-click your fMRI sequence.</li> <li>Ensure the scan frame covers the area of interest (whole brain or region of interest).</li> <li>Accept the frame and proceed to start the experiment on the Stimulus PC.</li> </ul> </li> </ol> <p>Don\u2019t Forget</p> <ul> <li>Always press Proceed before starting the scan. Any changes made in the tabs won\u2019t take effect unless Proceed is clicked.</li> <li>Ensure consistent TR, slices, and settings within and between participants.</li> <li>Start the experiment on the Stimulus PC before proceeding on the scanner, to avoid missing the trigger.</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#common-issues_6","title":"Common Issues","text":"Door Not Closed Properly <p>Ensure both the door to the technical room and the scanner door are securely closed. Improperly closed doors will trigger error messages and prevent scanning from starting.</p> Patient position on the table is unknown <p>If you encounter the error message \"Patient position on the table is unknown\":</p> <ul> <li>Return to the scanner room and recalibrate the patient\u2019s position using the laser alignment system.</li> <li>Ensure that the calibration laser is correctly aligned and restart the scan.</li> </ul> Scanner light still on <p>If the scanner light remains on, adjust it using the control buttons.</p> <p>For fMRI studies, ensure the lighting remains consistent throughout the session. Use the outer circle on the control panel to turn the light off or dim it as needed.    </p> Pixelated image after reference scan <p>If the reference scan shows a pixelated image with only the skull contours visible, the top of the coil may not be properly mounted.</p> <ul> <li>Slide the volunteer out of the scanner.</li> <li>Ensure that the coil is securely closed before attempting to recalibrate.</li> </ul> Ventilation too low <p>If the ventilation error appears during Smartbrain, the system requires a minimum ventilation setting of 3.</p> <ul> <li>You can either proceed without adjusting the setting or adjust ventilation via Examination &gt; Adjust Ventilation in the scan console.</li> </ul> Participant leaving the scanner during the task <p>It might happen that your participant feels uncomfortable during the scanning, and has to get out of the scanner to strech their legs, go to the toilet or so. If time allows it and they are ok with it, you can consider re-starting the scanning session where it was left off. </p> <p>To do so, follow these steps to adapt your sequence at the MRI control computer: - Right-click and duplicate both Smartbrain and Check_fMRI in your sequence. - If necessary, duplicate any run you left interrupted that you would want to re-do. - Re-order the events of your sequence so that Smartbrain and Check_fMRI are followed by any runs you wish to complete.</p> <p>Once they are ready, place your participant in the scanner as you did to begin the scanning, and re-start the sequence from the events you just created.</p> No key press seems to be recorded <p>The good practice is always to have someone press the button boxes you're going to use before the participant enters the scanner. That allows to see if the button boxes are connected &amp; all the buttons functional. If you're not sure the key presses of your participant actually send a key press to the computer and you're in the middle of a run, the first thing to do is to have a look at the logging coming into the MatLab console after the run. You can also ask your participant to press the response buttons during the break, with the cursor in the console, and see what comes out.</p> <p>In any case, if no key press is being recorded, try the following steps: - Check in the projector room if the transmitter box connected to the cables of the button boxes you are using is connected to the stimulus computer. - Check if the transmittor box connected to the cables of the button boxes you are using is turned on. Some boxes require a to be switched on to actually transmit signals.</p> <p>Tip: A good way to make sure participants can send key presses from the button boxes they are using is to have an instruction screen that requires a key press to continue. Assuming that screen contains information relevant to the participant, they will be able to read and press whenever they are ready. Such a screen can allow to ensure the stimulus computer receives at least some kind of key press.</p>"},{"location":"research/fmri/fmri-procedure.html#anatomical-scan","title":"Anatomical Scan","text":"<p>If you need to collect only an anatomical scan, you still must run the standard scans first (refer to the section above).</p> <p>You have two options for scheduling the anatomical scan:</p> <ul> <li>At the end of the session</li> <li>Interleaved with fMRI sequences (to give participants a break)</li> </ul> <p>During the anatomical run, participants can:</p> <ul> <li>Close their eyes</li> <li>Watch a movie (You can present a YouTube video via the stimulus computer).</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#checking-for-movement","title":"Checking for Movement","text":"<p>After an fMRI run finishes:</p> <ol> <li>Drag the sequence name to one of the boxes on the right of the scan console screen.</li> <li>An image of the run will appear.</li> <li>Scroll through the slices and play the time series of images (use the play button at the top of the window).</li> <li>Review the time series to check for any participant movement.</li> </ol>"},{"location":"research/fmri/fmri-procedure.html#exporting-data","title":"Exporting Data","text":"<p>You can export data using either Nifti/PAR-REC or DICOM formats. Choose the method that suits your needs.</p> <p>Faster Export Method</p> <p>For faster and more reliable data transfer, insert your USB key or hard drive into the computer in the technical room, rather than the scan console.</p> <p>Pseudonymization required</p> <p>To comply with the GDPR and related data protection regulations, your dataset must be pseudonymized by assigning a code name to the exported data \u2014 for example, using a subject identifier in BIDS format such as <code>sub-01</code>.</p>"},{"location":"research/fmri/fmri-procedure.html#nifti-or-par-rec-export","title":"Nifti or PAR-REC Export","text":"<ol> <li>Go to Patients &gt; Administration in the scan program.</li> <li>Locate your participant\u2019s name in the patient administration window.</li> <li>Double-click the name and select the runs to export.</li> <li>Click Disk Files.</li> <li>Navigate to the Non-Dicom Export tab.</li> <li>Choose an export file name and format. Please make sure you have checked the box that omits all identifying information from the exported files!</li> <li>(Optional) Check the Sort box, but ensure consistency (either always check it or never).</li> <li>Press Proceed to start the export.</li> <li>Verify the exported files in the export folder on the FTP drive. Double-check the file sizes to ensure all volumes were exported correctly.</li> </ol> <p>To monitor export progress, navigate to:</p> <ul> <li>Patients &gt; Administration &gt; Manage Job Queue</li> </ul> <p>Avoid Exporting Incomplete Runs</p> <p>Never export data from a run that is still in progress. The export list will not refresh automatically. Press the Refresh button, or reopen the window after part of the data has been exported</p>"},{"location":"research/fmri/fmri-procedure.html#dicom-export","title":"DICOM Export","text":"<ol> <li>Go to Patients &gt; Administration in the scan program.</li> <li>In the patient administration window, locate the participant\u2019s name.</li> <li>Double-click the name and select the runs to export.</li> <li>Click Disk Files.</li> <li>Select a directory to save the data.</li> <li>Choose between Nifti, Enhanced (4D DICOM) or Classic (2D DICOM) format (each slice saved as a separate file).</li> <li>Press Proceed to start the export. Please make sure you have checked the box that omits all identifying information from the exported files!</li> </ol> <p>Handling Export Delays</p> <p>It may take some time before the export starts. If you see a warning about exporting a large number of images, simply press Proceed to confirm.</p> <p>To track export progress, navigate to:</p> <ul> <li>Patients &gt; Administration &gt; Manage Job Queue</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#common-issues_7","title":"Common Issues","text":"Not enough free disk space <p>If files do not export properly (e.g., incorrect sizes), it may indicate that the export drive has insufficient space.</p> <ol> <li>Check the available disk space by right-clicking the Export drive and selecting Properties.</li> <li>If the drive is full, delete old data such as PAR-REC, Nifti, or DICOM files. These files can be re-exported later, so no data will be permanently lost.</li> </ol> Local patient database near full capacity <p>If the local patient database is nearly full (90-100% capacity), scanning may not proceed.</p> <ol> <li>Navigate to Patients &gt; Administration in the scan program.</li> <li>Check the percentage of disk space used (displayed in the top-right corner).</li> </ol> <p>Freeing Space</p> <ul> <li>Delete previous participant data only after confirming that it has been exported and transferred without corruption.</li> <li>Alternatively, ask for help from an MR technician to delete unnecessary data.</li> </ul> Missing export window <p>If the export window doesn\u2019t appear, press the Windows key to reveal the taskbar and locate the hidden export window.</p> Export Progress Stalled <p>If DICOM exports seem to have stalled, navigate to Manage Job Queue and check that the dropdown menu is set to Enabled.</p> Safe Ejection of External Drives <p>If you are unable to safely eject your external drive:</p> <ol> <li>Log off the scanner computer to shut down the scanning software.</li> <li>Log back in using MRService credentials.</li> <li>Restart the scanning software and acknowledge any helium pressure alarms.</li> <li>Once done, you can safely eject the external drive.</li> </ol> External Drive Not Detected <p>If the external drive is not detected in the Devices and Drives window:</p> <ol> <li>Log off and log back in using the MRService credentials.</li> <li>Check again in the Devices and Drives window to see if the external drive appears.</li> </ol>"},{"location":"research/fmri/fmri-procedure.html#after-scanning","title":"After Scanning","text":""},{"location":"research/fmri/fmri-procedure.html#scanner-area","title":"Scanner Area","text":"<p>When finishing a session, ensure the scanner area is returned to its original state. Follow these steps:</p> <ol> <li> <p>Move the volunteer out of the scanner:</p> <ul> <li>Use the up/down and backward/forward buttons to lower the table, allowing the volunteer to comfortably get off the table.</li> </ul> </li> <li> <p>Organize the equipment:</p> <ul> <li>Button box: Place the button box back in the cabinet to the right of the scanner.</li> <li>Panic button: Leave it at the end of the patient table.</li> <li>Mirror: Return the square mirror to the shelf and replace it with the curved mirror (do not touch the mirror itself).</li> <li>Top of the coil: Leave it inside the scanner (keep the bottom part plugged in and on the table).</li> <li>Washcloths: Dispose of the used washcloths in the transparent white bag near the door.</li> <li>Headphones: Hang the white headphones back on the hook at the front of the scanner.</li> <li>Earplugs: Dispose of them in the blue bag near the door.</li> <li>Paper towels: Throw away any used paper towels in the blue bag.</li> </ul> </li> <li> <p>Cleaning:</p> <ul> <li>Use the disinfectant provided in the MRI magnet room to wipe down the MRI system table.</li> <li>Cushions and equipment: Ensure no equipment is left on the floor. If you find anything on the floor, place it on a shelf.</li> </ul> </li> <li> <p>Turn off the projector:</p> <ul> <li>Go to the technical room and use the remote control to switch off the projector.</li> </ul> </li> <li> <p>If clinical scans follow your session:</p> <ul> <li>Remove the projection screen from the scanner bore and place it safely inside the scanner room (e.g., against the wall).</li> <li>Change the filter back to the default setting (3NB).</li> </ul> </li> </ol>"},{"location":"research/fmri/fmri-procedure.html#control-room","title":"Control Room","text":""},{"location":"research/fmri/fmri-procedure.html#stimulus-pc_1","title":"Stimulus PC","text":"<ul> <li>Collect your data from the stimulus PC.</li> <li>Turn off the screen (but do not shut down the computer).</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#scan-console","title":"Scan Console","text":"<ol> <li> <p>Verify data export:</p> <ul> <li>Ensure all data has been exported.</li> <li>Check the file sizes to confirm all files were exported correctly (files should have similar sizes; if some are smaller, they may still be exporting).</li> </ul> </li> <li> <p>Clear the screen:</p> <ul> <li>Select Patient &gt; Close \"participant name\" to clear the participant's data from the screen.</li> <li>Turn off the screen (do not shut down the computer).</li> </ul> </li> </ol>"},{"location":"research/fmri/fmri-procedure.html#check-out-procedures","title":"Check-out Procedures","text":"<p>Before leaving the department, complete the check-out part of the check-in/out form:</p> <ol> <li>Fill out the check-out section of the form.</li> <li>Leave the form with the other documents, along with the MRI safety questionnaire.</li> <li>Report any minor technical incidents on the form and email the MRI Safety Officer: Dr. Ronald Peeters.</li> <li>Incidental abnormalities: Do not inform the participant. Contact Prof. Dr. Stefan Sunaert immediately.</li> </ol>"},{"location":"research/fmri/fmri-procedure.html#if-you-are-the-last-person-scanning-experiments-that-day","title":"If you are the last person scanning experiments that day","text":""},{"location":"research/fmri/fmri-procedure.html#scanner-area_1","title":"Scanner Area","text":"<ul> <li>Use alcohol wipes to clean the patient table.</li> <li>Put the projection screen on the side, against the wall.</li> <li>Remove the coil from the table and store it in the cabinet.</li> <li>Lock the scanner area (the key is in the control room).</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#control-room_1","title":"Control Room","text":"<ul> <li>Do not shut down the console or stimulus PC!</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#scanning-of-children","title":"Scanning of Children","text":"<ol> <li> <p>Preparation:</p> <ul> <li>Bring biscuits and drinks for the child.</li> <li>Make sure the time slot is long enough so the session is not rushed.</li> <li>Limit scanning sessions to 50 minutes of active tasks with plenty of breaks.</li> </ul> </li> <li> <p>During the session:</p> <ul> <li>Show the child the control room.</li> <li>Go through the MR screening form with the parents and double-check that the child has no metal on their clothes.</li> <li>Ask the child to use the toilet before scanning.</li> </ul> </li> <li> <p>Parental presence:</p> <ul> <li>Parents must stay in the control room during scanning. Once the scanning has started, they should wait in the waiting area outside the scanner.</li> <li>If the child is too scared, they can be accompanied by a parent into the scanner room (ensure the parent removes all metal and wears earplugs and headphones).</li> <li>Once the child is calm, the parent can leave the scanner room.</li> </ul> </li> <li> <p>Communicating with the child:</p> <ul> <li>Explain that you will talk to them via the intercom.</li> <li>Let them know they can speak when prompted and need to stay quiet otherwise.</li> <li>Use simple, reassuring language:<ul> <li>Call the coil a \u201chelmet.\u201d</li> <li>Explain that the table movement is like being on a ride.</li> <li>Tell them about the lights being off during scanning and that they need to stay very still.</li> </ul> </li> </ul> </li> <li> <p>Head positioning:</p> <ul> <li>Fixate the child\u2019s head with washcloths on the sides, but avoid too much pressure.</li> <li>Optionally, use tape across the child\u2019s forehead to further secure the head.</li> </ul> </li> <li> <p>Calibration:</p> <ul> <li>Explain that the child must keep their eyes closed during calibration.</li> <li>Cover their eyes with your hand for extra comfort.</li> <li>Ensure the laser is positioned between the child\u2019s eyebrows.</li> </ul> </li> <li> <p>Lighting for children:</p> <ul> <li>Leave a bit of light on during scanning to reduce fear. If needed, use the dimmer switch in the control room.</li> </ul> </li> <li> <p>During scanning:</p> <ul> <li>For structural scans (where functional information isn\u2019t needed), you can play a YouTube video or DVD to keep the child entertained.</li> </ul> </li> <li> <p>Breaks:</p> <ul> <li>Take a break after each run and ask the child how they are feeling.</li> <li>If the tasks change between runs, give the child a short reminder of the instructions.</li> </ul> </li> <li> <p>Post-scan:</p> <ul> <li>After scanning, let the child sit up slowly to avoid dizziness.</li> </ul> </li> </ol>"},{"location":"research/fmri/fmri-procedure.html#auditory-stimuli","title":"Auditory Stimuli","text":""},{"location":"research/fmri/fmri-procedure.html#scanner-room-yellow-headphones","title":"Scanner Room - Yellow Headphones","text":"<ul> <li>The yellow headphones are stored on the left side of the storage space (against the wall).</li> <li>The headphones will present sound at full level only when placed inside the scanner bore.</li> <li>Disconnect the white headphones from the head coil and replace them with the yellow headphones.</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#control-room-microphone","title":"Control Room - Microphone","text":"<p>The microphone is always on, but goes into standby mode after a few seconds.</p> Button Function + / - Buttons Increase/decrease the volume. Menu Button Access various options. Hold it and press the + button to navigate the menu. Grey Button Speak to the participant. <p>Activate the fMRI settings by holding the Menu button and pressing + to navigate to the fMRI option.</p>"},{"location":"research/fmri/fmri-procedure.html#control-room-amplifier-and-converter","title":"Control Room - Amplifier and Converter","text":"<ul> <li>Check that the red and white plugs (audio cables to the headphones) are connected to the converter.</li> <li>Ensure the power cable is plugged in next to the red and white plugs.</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#common-issues_8","title":"Common Issues","text":"Participant Can't Hear You <ul> <li>Reboot the amplifier by unplugging the power cable underneath the desk.</li> <li>Reboot the converter by unplugging its power cable.</li> </ul> Volume Imbalance (left/right) <ul> <li>Adjust the balance via the Menu button. Hold it and use \u00b1 to adjust levels separately.</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#in-case-of-emergency","title":"In Case of Emergency","text":""},{"location":"research/fmri/fmri-procedure.html#seeing-an-abnormality-on-brain-images","title":"Seeing an Abnormality on Brain Images","text":"<p>Do not disclose concerns to the volunteer</p> <p>Avoid causing unnecessary distress to the volunteer. False alarms can arise from misinterpretations.</p> <p>If you detect a potential abnormality:</p> <ol> <li>Report the concern immediately to Dr. Stefan Sunaert. If unavailable, contact the radiologist on call.</li> <li>Supply a copy of the image showing the suspected abnormality.</li> <li>Make a note of the finding in the Check-out part of the Check-In/Out form.</li> </ol>"},{"location":"research/fmri/fmri-procedure.html#emergency-procedures","title":"Emergency Procedures","text":"<p>In case of an emergency involving the MRI system or facility:</p>"},{"location":"research/fmri/fmri-procedure.html#equipment-malfunction","title":"Equipment Malfunction","text":"<ul> <li>If the use of facility equipment results in an accident, immediately notify certified MR personnel (MR technicians or radiologist) and the MRI Safety Officer.</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#system-failure","title":"System Failure","text":"<ul> <li> <p>If part of the system fails and poses a danger to the volunteer:</p> </li> <li> <p>Remove the volunteer from the scanner.</p> </li> <li>Seek help from certified MR personnel.</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#emergency-magnet-shutdown-quenching","title":"Emergency magnet shutdown (quenching)","text":"Magnet Quenching <p>Emergency shutdown of the magnet (quenching) must only be performed by authorized personnel in the following life-threatening situations:</p> <ul> <li>If the magnetic field poses an immediate danger to a person.</li> <li>If emergency services need access to the Inner Controlled Area with ferromagnetic equipment.</li> </ul>"},{"location":"research/fmri/fmri-procedure.html#in-case-of-fire","title":"In Case of Fire","text":"<p>Danger</p> <p>Do not bring ferromagnetic fire extinguishers into the Inner Controlled Area (risk of projectile hazard). Use fire extinguishers marked with a YELLOW ribbon, which are non-ferromagnetic (aluminum).</p> <ol> <li>Call emergency number 2580. Clearly indicate the location: \"MRI Suite MR8\".</li> <li>Do not take unnecessary risks. Only perform one extinguishing attempt.</li> <li>Close doors to the endangered area once all persons are evacuated.</li> <li>Evacuate the area, using emergency exits if necessary.</li> <li>Follow instructions from the fire crew without taking further risks.</li> </ol>"},{"location":"research/fmri/fmri-procedure.html#in-case-of-reanimation","title":"In Case of Reanimation","text":"<ol> <li>Seek immediate assistance from the ASU.</li> <li>Call emergency number 1000. Clearly state the location: \"MRI Suite MR8\".</li> </ol>"},{"location":"research/fmri/analysis/index.html","title":"fMRI Analysis Overview","text":"<p>This section of the wiki provides a comprehensive guide for analyzing fMRI data. Follow the steps below to ensure smooth data processing, from environment setup to advanced multi-variate pattern analysis (MVPA).</p>"},{"location":"research/fmri/analysis/index.html#analysis-steps-overview","title":"Analysis steps overview","text":"<ul> <li> <p> General Information   Overview of the pipeline, including key terminology and data structure. Start here to get a sense of the entire workflow.</p> </li> <li> <p> Setting Up the Environment   Install essential tools and software like Docker, fMRIPrep, and SPM to prepare your analysis environment.</p> </li> <li> <p> Converting Data to BIDS   Organize your raw data in the BIDS format for compatibility with neuroimaging tools. Includes step-by-step instructions for converting and validating your dataset.</p> </li> <li> <p> Preprocessing &amp; QA   Perform quality control and preprocess your data using tools like fMRIPrep and MRIQC to ensure it's ready for statistical analysis.</p> </li> <li> <p> First-Level Analysis   Model brain activity using the General Linear Model (GLM) in SPM. Set up contrasts and extract statistical values.</p> </li> <li> <p> Regions of Interest (ROIs)   Create and analyze regions of interest for targeted brain analysis. ROIs are crucial for advanced analyses like MVPA.</p> </li> <li> <p> Multi-Variate Pattern Analysis (MVPA)   Decode complex neural patterns using machine learning methods like SVM. Analyze brain activity across different conditions.</p> </li> <li> <p> Complete Workflow Example   An end-to-end guide that ties everything together, providing an example of a full fMRI analysis pipeline.</p> </li> </ul>"},{"location":"research/fmri/analysis/index.html#additional-resources","title":"Additional Resources","text":"<ul> <li> <p>BIDS Starter Kit   Learn more about the BIDS format and how to structure your datasets.</p> </li> <li> <p>SPM Documentation   Dive into SPM resources to get the most out of your GLM analyses.</p> </li> <li> <p>CoSMoMVPA Documentation   Explore multi-variate pattern analysis techniques with CoSMoMVPA.</p> </li> </ul>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html","title":"fMRI workflow example","text":"<p>This page is a work in progress and is based on my (Andrea) fMRI pipeline. This information may change once we agree on shared practices.</p> <p>The code to reproduce these analyses can be found here.</p> <p>For information on how to set up the working environment, install, and configure the packages mentioned in this document, refer to Set-up your fMRI environment and Coding practices</p>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#from-raw-data-to-bids","title":"From raw data to BIDS","text":""},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#collecting-raw-data","title":"Collecting raw data","text":"<p>Note</p> <p>It is essential to ensure that no personal identifiers are present in any of the files that leave the hospital. Use my script for anonymizing filenames and data.</p> <p>At the hospital:</p> <ul> <li>Take fMRI scans from the hospital computer. Specify which buttons to select in the GUI.</li> <li>Anonymize the filenames using my script to ensure subject privacy.</li> <li>Extract behavioral (bh) and eye-tracking (et) data from the output folders on the experiment PC.</li> <li>Copy all files into a <code>sourcedata/</code> folder organized with subfolders: <code>bh</code>, <code>nifti</code>, and <code>et</code>.</li> </ul>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#converting-fmri-data-to-bids","title":"Converting fMRI Data to BIDS","text":""},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#step-1-preparing-for-the-first-subject","title":"Step 1: Preparing for the First Subject","text":"<p>Info</p> <p>This step should only be performed for the first subject in your dataset. It can be skipped if anatomical and functional JSON templates are already available in <code>code/misc/</code>. See this guide for more info on the template files.</p> <ol> <li> <p>Convert DICOM to BIDS (NIfTI):</p> <ol> <li> <p>Prerequisites:</p> <ul> <li><code>dcm2nii</code></li> <li>Raw data should be organized as: <code>/sourcedata/sub-&lt;xx&gt;/dicom</code></li> <li>MATLAB</li> </ul> </li> <li> <p>Download <code>dicm2nii</code> from dicm2nii, unzip, and add to the MATLAB path.</p> </li> <li> <p>Open MATLAB and type <code>anonymize_dicm</code> in the console. Select the folder where the files are and the output folder: <code>/sourcedata/sub-&lt;xx&gt;/dicom_Anon</code>.</p> </li> <li> <p>Run <code>dicm2nii</code> in MATLAB. Select the DICOM folder and result folder (e.g., <code>dicom_converted</code>). Untick the compress box and ensure to save the JSON file.</p> </li> </ol> <p>The output folder structure should be as follows:</p> <pre><code>dicom_converted\n\u251c\u2500\u2500 sub-01\n\u2502   \u251c\u2500\u2500 anat\n\u2502   \u2502   \u251c\u2500\u2500 sub-01_T1w.json\n\u2502   \u2502   \u2514\u2500\u2500 sub-01_T1w.nii.gz\n\u2502   \u251c\u2500\u2500 func\n\u2502   \u2502   \u251c\u2500\u2500 sub-01_task-exp_run-1_bold.json\n\u2502   \u2502   \u2514\u2500\u2500 sub-01_task-exp_run-1_bold.nii.gz\n\u2502   \u2514\u2500\u2500 dcmHeaders.mat\n\u2514\u2500\u2500 participants.tsv\n</code></pre> <ul> <li>Copy the <code>sub-01</code> folder from <code>dicom_converted</code> into the BIDS folder.</li> </ul> </li> <li> <p>Validate the BIDS Directory:</p> <ul> <li>Use the BIDS Validator to check for any errors.</li> </ul> </li> </ol>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#step-2-processing-subsequent-subjects","title":"Step 2: Processing Subsequent Subjects","text":"<p>If the raw data is organized in a <code>sourcedata/sub-xx</code> folder, and JSON templates are already created:</p> <ul> <li>Anonymize/deface the images.</li> <li>Run <code>script01</code> to move and rename the raw files into the BIDS folder, creating a <code>sub-xx</code> folder for each subject.</li> </ul>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#converting-behavioral-data-to-bids","title":"Converting behavioral data to BIDS","text":"<ul> <li>Run <code>script02</code> to convert behavioral <code>.mat</code> data into <code>events.tsv</code> files following the BIDS specification. This will parse the trial data from the <code>.mat</code> file and create new <code>.tsv</code> files for each subject and run.</li> </ul> <p>The fMRI task script should output two files per run:</p> <ol> <li> <p><code>&lt;timestamp&gt;_log_&lt;subID&gt;-&lt;run&gt;-&lt;buttonMapping&gt;_&lt;taskName&gt;.tsv</code> : This is the human-readable log file produced by the task. Here is an extract from the file:</p> EVENT_TYPE EVENT_NAME DATETIME EXP_ONSET ACTUAL_ONSET DELTA EVENT_ID START - 2024-05-03 10:49:43.099 - 0 - - FLIP Instr 2024-05-03 10:50:11.399 - 28.300201 - - RESP KeyPress 2024-05-03 10:50:34.160 - 51.063114 - 51 FLIP TgrWait 2024-05-03 10:50:34.216 - 51.117046 - - PULSE Trigger 2024-05-03 10:50:40.000 - 56.904357 - 53 </li> <li> <p><code>&lt;timestamp&gt;_log_&lt;subID&gt;_&lt;run&gt;_&lt;taskName&gt;.mat</code>: This MATLAB file contains all the parameters to reproduce the experimental run, and stores input parameters and results.</p> </li> </ol> <p>Ensure that each resulting TSV file has at least three columns: <code>onset</code>, <code>duration</code>, and <code>trial_type</code>.</p> <p>If the behavioural data is stored in a sourcedata/sub-xx/bh/ folder consistent to the one described above, you can run the script02_behavioural-to-BIDS.m script, after editing the parameters at the top of the script. This script iterates through subject-specific directories targeting behavioral .mat files, then processes and exports trial-related info into BIDS-compliant TSV event files in the BIDS folder provided as parameters.</p>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#converting-eye-tracking-data-to-bids","title":"Converting Eye-Tracking Data to BIDS","text":"<p>Pre-requisite: Install the EyeLink Developers Kit/API to convert EDF files into ASC files. Refer to the official setup guide:</p> <ul> <li> <p>EyeLink Developers Kit/API</p> </li> <li> <p>Run the eye-tracking (ET) conversion script to convert the data to ASC in BIDS format.</p> <p>Warning</p> <p>BEP020 has not been approved yet. Consider whether event messages should be included in the BIDS structure.</p> </li> </ul>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#pre-processing-fmri-data-in-bids-format","title":"Pre-processing fMRI Data in BIDS Format","text":""},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#quality-control-with-mriqc","title":"Quality Control with MRIQC","text":"<p>Use MRIQC to perform quality control checks on your fMRI data:</p> mriqc_batch.sh<pre><code>#!/bin/bash\nfor i in {0..40}; do\n    if [ $i -eq 0 ] || [ $i -eq 5 ] || [ $i -eq 14 ] || [ $i -eq 31 ]; then\n        continue\n    fi\n    subID=$(printf \"sub-%02d\" $i)\n    echo \"Processing $subID\"\n    docker run -it --rm \\\n        -v /data/BIDS:/data:ro \\\n        -v /data/BIDS/derivatives/mriqc:/out \\\n        -v /temp_mriqc:/scratch \\\n        nipreps/mriqc:latest /data /out participant \\\n        --participant-label ${subID} \\\n        --nprocs 16 --mem-gb 40 --float32 \\\n        --work-dir /scratch \\\n        --verbose-reports --resource-monitor -vv\n    sleep 0.5\ndone\n\necho \"Running group analysis\"\ndocker run -it --rm \\\n    -v /data/BIDS:/data:ro \\\n    -v /data/BIDS/derivatives/mriqc:/out \\\n    -v /temp_mriqc:/scratch \\\n    nipreps/mriqc:latest /data /out group \\\n    --nprocs 16 --mem-gb 40 --float32 \\\n    --work-dir /scratch \\\n    --verbose-reports --resource-monitor -vv\n\nsleep 0.5\n\necho \"Running classifier\"\ndocker run \\\n    -v /temp_mriqc:/scratch \\\n    -v /data/BIDS/derivatives/mriqc:/resdir \\\n    -w /scratch --entrypoint=mriqc_clf poldracklab/mriqc:latest \\\n    --load-classifier -X /resdir/group_T1w.tsv\n</code></pre> <p>Warning</p> <p>JSON files may include <code>NaN</code> values that are incompatible with MRIQC. Use <code>./utils/sanitize_json.py</code> to fix this issue before running MRIQC.</p>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#minimal-preprocessing-with-fmriprep","title":"Minimal Preprocessing with fMRIprep","text":"<p>With your BIDS data organized, the next step is preprocessing using fMRIprep:</p> <ul> <li>Install Docker (and WSL if on Windows) and configure it for use with fMRIprep.</li> <li>Install <code>fmriprep-docker</code> with <code>pip install fmriprep-docker</code>.</li> <li>Ensure Docker has access to the folders you will be using (e.g., BIDS folder, temporary work directory).</li> </ul> <p>To run <code>fmriprep</code> for a single subject, use the following command:</p> <pre><code>fmriprep-docker /data/projects/chess/data/BIDS /data/projects/chess/data/BIDS/derivatives/fmriprep participant \\\n    --work-dir //data/projects/chess/data/temp_fmriprep --mem-mb 10000 --n-cpus 16 \\\n    --output-spaces MNI152NLin2009cAsym:res-2 anat fsnative \\\n    --fs-license-file /data/projects/chess/misc/.license \\\n    --bold2t1w-dof 9 --task exp --dummy-scans 0 \\\n    --fs-subjects-dir /data/projects/chess/data/BIDS/derivatives/fastsurfer \\\n    --notrack --participant-label 41\n</code></pre> Use CIFTI output for surface data <p>If you plan to run analysis on surface data, consider using CIFTI output images from fMRIPrep. While this approach hasn't been directly tested here, CIFTI outputs can provide several advantages:</p> <ul> <li>Surface analysis in SPM (see this conversation on Neurostars).</li> <li>CIFTI images include cortical BOLD time series projected onto the surface using templates like the Glasser2016 parcellation (which is also used for MVPA).</li> <li>This method allows for direct analysis of surface data in formats like <code>.gii</code>, which can be compatible with SPM for further analysis.</li> <li>Using CIFTI outputs could simplify the process of obtaining surface-based parcellations and make the data more directly usable in subject space, potentially eliminating the need for complex and time-consuming transformations.</li> <li>It may also provide a more accurate representation of cortical activity by avoiding interpolation errors that can occur when mapping from volume to surface space.</li> </ul> <p>If you decide to explore this option, make sure to include the cifti falg in <code>--output-spaces</code> when running <code>fmriprep-docker</code>. This setup will produce CIFTI files (<code>.dtseries.nii</code>) along with standard volumetric outputs, giving you flexibility in how you proceed with your analysis.</p> Allocating resources to fMRIprep <p>Running fMRIPrep is resource and time intensive, especially with high-resolution data. Here are some practical tips to optimize the process:</p> <ul> <li>Time Estimate: Processing a single subject can take between 4-8 hours depending on your system's specifications (e.g., CPU, RAM). Plan accordingly if you have many subjects.</li> <li> <p>Optimize Resource Allocation: Adjust the <code>--n-cpus</code> and <code>--mem-mb</code> arguments to make the best use of your available hardware:</p> <ul> <li>n-cpus: Allocate about 70-80% of your CPU cores to avoid system slowdowns (e.g., <code>--n-cpus 12</code> on a 16-core system).</li> <li>mem-mb: Use around 80-90% of your total RAM, leaving some free for the operating system (e.g., <code>--mem-mb 32000</code> on a 40 GB system).</li> </ul> </li> <li> <p>Monitor Resource Usage: While running fMRIPrep, open a system monitor like Task Manager (Windows), Activity Monitor (Mac), or htop (Linux) to observe CPU and memory usage:</p> <ul> <li>Aim for high CPU usage (close to maximum) and RAM usage that is slightly below your system\u2019s capacity.</li> <li>If memory usage exceeds available RAM, the process might crash due to Out of Memory (OOM) errors or cause disk space issues if using a <code>--work-dir</code> that fills up.</li> </ul> </li> <li> <p>Adjust Settings if Necessary: If you encounter OOM errors or the process is slower than expected:</p> <ul> <li>Lower <code>--mem-mb</code>: Decrease memory allocation incrementally (e.g., by 2-4 GB at a time).</li> <li>Reduce <code>--n-cpus</code>: Using fewer cores can help balance the load and prevent crashes.</li> <li>Use a dedicated <code>--work-dir</code>: Specify a work directory on a high-speed SSD or similar to reduce I/O bottlenecks and ensure there's enough disk space for temporary files.</li> </ul> </li> </ul> <p>If the run finishes successfully (check the last line of your terminal output), you should have a new <code>BIDS/derivatives/fmriprep/sub-xx</code> folder. See here for a complete list of outputs generated by fMRIPrep. Make sure that inside your <code>anat</code> and <code>func</code> folders you have all the scans (anatomical and functional for all runs) in the specified spaces. Since we specified <code>fsnative</code> as a space and did not use the <code>--no-recon-all</code> flag, fMRIPrep will also produce surface data in <code>BIDS/derivatives/fastsurfer/sub-xx</code>.</p>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#interpreting-fmriprep-visual-reports","title":"Interpreting fMRIPrep Visual Reports","text":"<p>Check the <code>sub-xx.html</code> report to ensure everything ran smoothly. Pay particular attention to:</p> <ul> <li>Registrations: Verify the alignment between functional and anatomical images.</li> <li>Framewise Displacement (FD) Values: Look for runs with unusually high FD values, as these may indicate motion artifacts or poor data quality.</li> </ul> <p>For more details, refer to the general guidelines outlined here, and to the following links:</p> <ul> <li>fMRIPrep Output Confounds</li> <li>Video on Reviewing fMRIPrep Outputs</li> </ul>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#processing-eye-tracking-data-with-bidsmreye","title":"Processing Eye-Tracking Data with <code>bidsmreye</code>","text":"<p>To process eye-tracking data using <code>bidsmreye</code>, run the following Docker command:</p> <pre><code>docker run -it --rm \\\n    -v /data/projects/chess/data/BIDS/derivatives/fmriprep:/data \\\n    -v /data/projects/chess/temp_bidsmreye:/out \\\n    cpplab/bidsmreye:0.5.0 \\\n    /data /out participant all \\\n    --space T1w \\\n    --reset_database \\\n    --verbose\n</code></pre> <p>Note</p> <p>In my experience, <code>bidsmreye</code> worked only when using the <code>T1w</code> fMRIPrep output space.</p>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#first-level-analysis-general-linear-model-glm","title":"First-Level Analysis \u2013 General Linear Model (GLM)","text":"<p>After preprocessing, proceed to the first-level analysis with the GLM. Running the GLM and setting contrasts is straightforward using <code>script03</code>. Make sure to adjust the following parameters:</p> <ul> <li>Paths:</li> <li><code>fmriprepRoot</code>: Path to the fMRIPrep folder.</li> <li><code>BIDSRoot</code>: Path to your BIDS folder.</li> <li><code>outRoot</code>: Path to save GLM results (ideally in the derivatives folder, in a <code>fmriprep-spm</code> folder).</li> <li> <p><code>tempDir</code>: Directory for temporary files, such as uncompressed or smoothed files.</p> </li> <li> <p>Subject Selection:</p> <p>Leave a new line before listing subjects.</p> </li> <li> <p><code>selectedSubjectsList</code>: A list of integers like <code>[41, 42, 43, 44]</code> or use <code>'*'</code> to analyze all subjects.</p> </li> <li> <p><code>selectedRuns</code>: List of runs to analyze.</p> </li> <li> <p>Contrasts Setup:</p> <pre><code>selectedTasks(1).name = 'exp';  % The name of the task. Must match the task name in your BIDS filenames.\nselectedTasks(1).contrasts = {'Check &gt; No-Check'};  % Name of the contrast.\nselectedTasks(1).weights(1) = struct('C_WILDCARD___WILDCARD_', 1, 'NC_WILDCARD___WILDCARD_', -1);  % Weights for each regressor.\nselectedTasks(1).smoothBool = false;  % Whether to smooth images before GLM. Useful for localizers.\n</code></pre> </li> </ul> <p>If everything is configured correctly, the script will generate new <code>sub-xx</code> folders in your output directory. These folders will contain subdirectories for each analysis task, with <code>beta_000x.nii</code> files for each regressor (including confounds and conditions).</p>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#verifying-the-design-matrix","title":"Verifying the Design Matrix","text":"<p>It is advisable to verify that the design matrix is set up correctly:</p> <ol> <li>Open the SPM GUI by typing <code>spm fmri</code> in the MATLAB Command Window.</li> <li>Click on Results and select the <code>SPM.mat</code> file located in your <code>BIDS/fmriprep-spm/{space}/sub-xx/{task_name}/</code> directory.</li> <li> <p>This will open the SPM Contrast Manager, showing the design matrix and assigned contrasts. Ensure the following:</p> <ul> <li>No overlapping or unusually long conditions.</li> <li>The correct number of runs.</li> <li>Confound regressors are positioned at the end of each run.</li> <li>Contrast weights are assigned correctly.</li> </ul> </li> <li> <p>Select your contrast and click Done.</p> </li> <li>In the next window, set the following options:<ul> <li>Apply masking: None</li> <li>P-value adjustment: None</li> <li>Threshold: <code>0.001</code></li> <li>Extent threshold: <code>0</code></li> </ul> </li> </ol> <p>This will display the results of the contrast (thresholded t-map) on the top right, along with a list of significantly active clusters in the bottom right panel.</p>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#visualizing-activations","title":"Visualizing Activations","text":"<p>To visualize activations on a volume or surface:</p> <ol> <li>Click Display -&gt; overlays... in the SPM GUI.</li> <li>Select sections for volume plotting or render for surface plotting.</li> <li>Choose the subject's anatomical image from <code>BIDS/derivatives/fmriprep/sub-xx/anat</code>.<ul> <li>For volume plots, select the <code>.nii</code> file corresponding to the same space as your GLM (usually MNI).</li> <li>For surface plots, select the pial or inflated brain image.</li> </ul> </li> </ol> <p>Warning</p> <p>SPM cannot read <code>.nii.gz</code> files directly, so you must decompress them into <code>.nii</code> files. This can be done with any decompression tool by right-clicking on the file in your file explorer. Once decompressed, use the SPM GUI to select the <code>.nii</code> file.</p>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#generating-and-organizing-regions-of-interest-rois","title":"Generating and Organizing Regions of Interest (ROIs)","text":""},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#rois-from-functional-localizers","title":"ROIs from Functional Localizers","text":"<p>To run MVPA, you need Regions of Interest (ROIs) to select your voxels. You can obtain ROIs through:</p> <ul> <li>Functional Localizers: Perform a localizer task in the scanner, then run a GLM on the preprocessed and smoothed data. For example, <code>Faces &gt; Objects</code> to identify the FFA.</li> <li>Pre-defined Anatomical Masks: Use anatomical masks in the same space as your subjects (e.g., MNI). Ensure the mask resolution matches the resolution of your data (e.g., resample/reslice if necessary using tools like ANTs, SPM, or Python libraries like nilearn or nibabel).</li> </ul>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#hcp-glasser-parcellation","title":"HCP Glasser Parcellation","text":"<p>In my pipeline, I use the Glasser2016 parcellation projected on <code>fsaverage</code>, which includes 180 ROIs per hemisphere. This process involves converting Glasser parcellation annotation files to labels and mapping them from <code>fsaverage</code> to the subject's T1 and MNI spaces. Use the <code>HPC-to-subject.sh</code> script for automation (see the top of the script file for usage notes).</p>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#multi-variate-pattern-analysis-mvpa","title":"Multi-Variate Pattern Analysis (MVPA)","text":""},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#running-decoding-with-svm","title":"Running Decoding with SVM","text":"<p>After organizing your ROIs, proceed with the MVPA analysis:</p> <ul> <li>Use <code>script04</code> to perform independent cross-validated SVM classification on each subject and ROI.</li> <li>The script outputs decoding accuracy for each ROI of the HCP parcellation.</li> </ul> <p>The results are saved in a <code>BIDS/derivatives/mvpa</code> folder organized according to the BIDS structure. Each subject's folder will contain a <code>.tsv</code> file with the decoding accuracy results.</p>"},{"location":"research/fmri/analysis/fmri-andrea-workflow.html#plotting-and-reporting","title":"Plotting and Reporting","text":"<p>The Glasser parcellation includes parcels at three levels, with each higher level grouping several ROIs into a single ROI. In my pipeline, the analysis is performed at the lowest level (180 parcels per hemisphere), then averaged across ROIs within a larger ROI using <code>script06</code>.</p> <ul> <li>The script computes the significance of each decoding accuracy against chance.</li> <li>It generates plots of significant accuracies on an inflated brain for each grouping level.</li> </ul> <p>For example:</p> <p></p>"},{"location":"research/fmri/analysis/fmri-bids-conversion.html","title":"Convert your fMRI data into BIDS format","text":"<p>To organize our fMRI dataset, we follow the BIDS Specification.</p> <p>If you are not familiar with the BIDS Specification, the BIDS Starter Kit provides all the information needed to get started, along with example BIDS datasets, Talks and Slides, and most importantly Tutorials.</p> <p>It is crucial that you get familiar with BIDS folders/files naming convention and structure. Most, if not all, the tools we are going to use in the next steps are BIDS Apps, and they rely on data organized following the BIDS Specification. Following this structure will make it easier to use these tools, share your code and data, and communicate with other scientists.</p> <p>The BIDS Specification provides guidelines on how to organize all your data formats, including (f/d)MRI, EEG, eye-tracking, Task events associated with Neuro-Imaging recordings or not, and Derivatives (e.g., pre-processed files, Regions Of Interest mask files, GLM files, etc.).</p> <p>At any moment, you can check your dataset for BIDS compliance. To do so, you can use the BIDS dataset validator.</p>"},{"location":"research/fmri/analysis/fmri-bids-conversion.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Convert your fMRI data into BIDS format</li> <li>Table of Contents</li> <li>BIDS Conversion Overview<ul> <li>1. Create the raw data directory</li> <li>2. Create the subject's directory</li> <li>3. Organize your source files</li> <li>4. Convert DICOM files</li> <li>5. Create the BIDS directory</li> <li>6. Organise the BIDS directory</li> <li>7. Create JSON sidecar files</li> <li>8. Create event files</li> <li>9. Create additional BIDS files</li> <li>10. Validating Your BIDS Structure</li> </ul> </li> </ul>"},{"location":"research/fmri/analysis/fmri-bids-conversion.html#bids-conversion-overview","title":"BIDS Conversion Overview","text":"<p>Here's a high-level overview of the steps involved in arranging your data in a BIDS-compatible way. While this provides a general understanding, most of these steps should be performed using the code provided in each sub-section to minimize errors. After scanning participants, you'll obtain data from two primary sources:</p> <ol> <li>The scanner: functional and structural outputs (<code>DICOM</code> files).</li> <li>The stimulus presentation computer: behavioural outputs (mainly <code>log</code> files and <code>mat</code> files) and potentially eye-tracking data (<code>edf</code> files or <code>tsv</code> files).</li> </ol> <p>As you turn your raw data into a BIDS-compatible format, your project directory will change considerably. The folder trees below show you how each steps will affect your working directory, with changing folders and file in bold for each step.</p>"},{"location":"research/fmri/analysis/fmri-bids-conversion.html#1-create-the-raw-data-directory","title":"1. Create the raw data directory","text":"<pre><code>\nmyproject\n\u2514\u2500\u2500 sourcedata\n</code></pre> <p>Your first step is to organize your files in a <code>sourcedata</code> folder. Follow the structure outlined in How to store raw data: have one main project folder (e.g. <code>myproject</code>), and a <code>sourcedata</code> folder in it.</p>"},{"location":"research/fmri/analysis/fmri-bids-conversion.html#2-create-the-subjects-directory","title":"2. Create the subject's directory","text":"<pre><code>\nmyproject\n\u2514\u2500\u2500 sourcedata\n    \u2514\u2500\u2500 sub-01\n        \u251c\u2500\u2500 bh\n        \u251c\u2500\u2500 dicom\n        \u251c\u2500\u2500 eye\n        \u251c\u2500\u2500 dicom_anon\n        \u2514\u2500\u2500 nifti\n</code></pre> <p>Create the relevant sub-folders within the <code>sourcedata</code> folder: for each participant you collected data from, create a <code>sub-xx</code> folder (e.g. <code>sub-01</code>). Within the folder of each participant, create a <code>bh</code> (behaviour), <code>eye</code> (i.e. eye-tracking data), <code>dicom</code> (i.e. dicom files collected from the scanner), <code>dicom_anon</code> (i.e. anonymized dicom files collected from the scanner), and <code>nifti</code> (i.e. nifti, the format of the files after the DICOM conversion).</p> <p>To create these folders, open your terminal (or PowerShell if you are on Windows) and type:</p> <pre><code>cd /path/to/myproject/sourcedata\nmkdir sub-01\nmkdir sub-01/bh\nmkdir sub-01/eye\nmkdir sub-01/dicom\nmkdir sub-01/dicom_anon\nmkdir sub-01/nifti\n</code></pre>"},{"location":"research/fmri/analysis/fmri-bids-conversion.html#3-organize-your-source-files","title":"3. Organize your source files","text":"<pre><code>\nmyproject\n\u2514\u2500\u2500 sourcedata\n    \u2514\u2500\u2500 sub-01\n        \u251c\u2500\u2500 bh\n        \u2502   \u251c\u2500\u2500 yyyy-mm-dd-sub-01_run-01_task-{taskname}_log.tsv\n        \u2502   \u251c\u2500\u2500 yyyy-mm-dd-sub-01_run-01_task-{taskname}.mat\n        \u2502   \u251c\u2500\u2500 ...\n        \u2502   \u251c\u2500\u2500 yyyy-mm-dd-sub-01_run-{runnumber}_task-{taskname}_log.tsv\n        \u2502   \u2514\u2500\u2500 yyyy-mm-dd-sub-01_run-{runnumber}_task-{taskname}.mat\n        \u251c\u2500\u2500 dicom\n        \u2502   \u251c\u2500\u2500 IM_0001\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 IM_0005\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 PS_0002\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 PS_0006\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 XX_0003\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 XX_0004\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 XX_0007\n        \u251c\u2500\u2500 dicom_anon\n        \u251c\u2500\u2500 nifti\n        \u2514\u2500\u2500 eye\n\n</code></pre> <p>Place the files you collected in this <code>sourcedata</code> structure: data collected from your experimental task goes into <code>bh</code> (e.g. <code>.mat</code> files and log files if you used the fMRI task template), data collected from the scanner itself (DICOM) goes in <code>dicom</code>, eye-tracking data (generally, EDF or csv files) goes in <code>eye</code>.</p>"},{"location":"research/fmri/analysis/fmri-bids-conversion.html#4-convert-dicom-files","title":"4. Convert DICOM files","text":"<pre><code>\nmyproject\n\u2514\u2500\u2500 sourcedata\n    \u2514\u2500\u2500 sub-01\n        \u251c\u2500\u2500 bh\n        \u251c\u2500\u2500 dicom\n        \u251c\u2500\u2500 dicom_anon\n        \u2502   \u251c\u2500\u2500 IM_0001\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 IM_0005\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 PS_0002\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 PS_0006\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 XX_0003\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 XX_0004\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 XX_0007\n        \u251c\u2500\u2500 nifti\n        \u2502   \u251c\u2500\u2500 dcmHeaders.mat\n        \u2502   \u251c\u2500\u2500 sub-01_run-01.json\n        \u2502   \u251c\u2500\u2500 sub-01_run-01.nii.gz\n        \u2502   \u251c\u2500\u2500 sub-01_struct.json\n        \u2502   \u2514\u2500\u2500 sub-01_struct.nii.gz\n        \u251c\u2500\u2500 dicom_anon\n        \u251c\u2500\u2500 nifti\n        \u2514\u2500\u2500 eye\n</code></pre> <p>If you have collected DICOM files from the scanner, you need to anonymise and convert them so that you can use them properly. There are several tools available that can help with this. One recommended option is <code>dicm2nii</code>, a lightweight and flexible toolbox for handling DICOM-to-NIfTI conversion.</p> <p>Why <code>dicm2nii</code> and not <code>dcm2niix</code>?</p> <p>Although <code>dcm2niix</code> is widely used and robust, especially for modern enhanced DICOMs and vendor-specific edge cases (like Philips), <code>dicm2nii</code> is often suggested.</p> <p>For data acquired with Philips scanners, or if your DICOMs have missing metadata (e.g., <code>PhaseEncodingDirection</code>), see this Rorden Lab guide and this NITRC forum thread See also Missing fields in JSON files for more information.</p> <p>To convert your data:</p> <ol> <li> <p>Navigate to your sourcedata folder</p> <pre><code>cd /path/to/myproject/sourcedata\n</code></pre> </li> <li> <p>Clone the repository from GitHub:</p> <pre><code>git clone https://github.com/xiangruili/dicm2nii.git\n</code></pre> </li> <li> <p>Add the <code>dicm2nii</code> folder to your MATLAB path:     In MATLAB, run:</p> <pre><code>addpath('/path/to/dicm2nii')  % Adjust this to the actual folder path\n</code></pre> <p>Tip</p> <p>You can also use <code>uigetdir</code> to interactively select the folder:</p> <pre><code>addpath(uigetdir)\n</code></pre> </li> <li> <p>Anonymize your DICOM files</p> <p>Use the <code>anonymize_dicm</code> function. This removes identifying fields and creates a safe copy for conversion:</p> <pre><code>anonymize_dicm('sub-01/dicom', 'sub-01/dicom_anon', 'sub-01')\n</code></pre> <ul> <li>First argument = path to raw DICOM folder</li> <li>Second argument = path to output anonymized DICOM folder</li> <li>Third argument = subject ID string used in metadata fields (optional but recommended)</li> </ul> <p>This will create <code>dicom_anon</code> and log any changes made.</p> </li> <li> <p>Convert anonymized DICOMs to NIfTI</p> <p>Now convert the anonymized files:</p> <pre><code>dicm2nii('sub-01/dicom_anon', 'sub-01/nifti', 'nii.gz')\n</code></pre> <ul> <li>First argument = path to anonymized DICOMs</li> <li>Second argument = output directory</li> <li>Third argument = output format (<code>nii</code>, <code>nii.gz</code>)</li> </ul> <p>This will:</p> <ul> <li>Generate one <code>.nii.gz</code> file per series</li> <li>Produce accompanying <code>.json</code> metadata files</li> <li>Create a <code>dcmHeaders.mat</code> with all parsed metadata</li> </ul> </li> </ol>"},{"location":"research/fmri/analysis/fmri-bids-conversion.html#5-create-the-bids-directory","title":"5. Create the BIDS directory","text":"<pre><code>\nmyproject\n\u251c\u2500\u2500 BIDS\n\u2502   \u2514\u2500\u2500 sub-01\n\u2502       \u251c\u2500\u2500 anat\n\u2502       \u2502   \u2514\u2500\u2500 sub-01_T1w.nii\n\u2502       \u2514\u2500\u2500 func\n\u2502           \u251c\u2500\u2500 sub-01_task-{taskname}_run-01_bold.nii\n\u2502           \u251c\u2500\u2500 ...\n\u2502           \u2514\u2500\u2500 sub-01_task-{taskname}_run-{runnumber}_bold.nii\n\u2514\u2500\u2500 sourcedata\n    \u2514\u2500\u2500 sub-01\n        \u251c\u2500\u2500 bh\n        \u251c\u2500\u2500 dicom\n        \u251c\u2500\u2500 dicom_anon\n        \u2514\u2500\u2500 nifti\n</code></pre> <p>Create a <code>BIDS</code> folder in your main project directory, alongside the <code>sourcedata</code> folder. For each participant, create a sub folder (e.g. <code>BIDS/sub-01</code>). In the BIDS folder of each participant, place a <code>func</code> folder for functional files and a <code>anat</code> folder for anatomical files. Copy-paste your functional <code>.nii</code> files from <code>sourcedata</code> to their corresponding <code>func</code> folder, renaming them if necessary to follow BIDS format (e.g. <code>sub-01_task-{taskname}_run-01_bold.nii</code>), and similarly copy-paste your structural <code>.nii</code> files to the <code>anat</code> folder, renaming them if necessary (e.g. <code>sub-01_T1w.nii</code>). </p>"},{"location":"research/fmri/analysis/fmri-bids-conversion.html#6-organise-the-bids-directory","title":"6. Organise the BIDS directory","text":"<pre><code>\nmyproject\n\u251c\u2500\u2500 BIDS\n\u2502   \u2514\u2500\u2500 sub-01\n\u2502       \u251c\u2500\u2500 anat\n\u2502       \u2502   \u2514\u2500\u2500 sub-01_T1w.nii\n\u2502       \u2514\u2500\u2500 func\n\u2502           \u251c\u2500\u2500 sub-01_task-{taskname}_run-01_bold.nii\n\u2502           \u251c\u2500\u2500 ...\n\u2502           \u2514\u2500\u2500 sub-01_task-{taskname}_run-{runnumber}_bold.nii\n\u2514\u2500\u2500 sourcedata\n    \u2514\u2500\u2500 sub-01\n        \u251c\u2500\u2500 bh\n        \u251c\u2500\u2500 dicom\n        \u251c\u2500\u2500 dicom_anon\n        \u2514\u2500\u2500 nifti\n</code></pre> <ol> <li>Navigate to your <code>sourcedata/sub-xx/nifti/</code> folder.</li> <li>Identify the functional and structural NIfTI files.</li> <li>Rename the files following BIDS conventions:<ul> <li>Functional: <code>sub-&lt;label&gt;_task-&lt;label&gt;_run-&lt;label&gt;_bold.nii</code></li> <li>Structural: <code>sub-&lt;label&gt;_T1w.nii</code></li> </ul> </li> <li>Move the renamed files to their respective folders in <code>BIDS/sub-xx/</code>:<ul> <li>Functional files go to <code>BIDS/sub-xx/func/</code></li> <li>Structural files go to <code>BIDS/sub-xx/anat/</code></li> </ul> </li> </ol>"},{"location":"research/fmri/analysis/fmri-bids-conversion.html#7-create-json-sidecar-files","title":"7. Create JSON sidecar files","text":"<pre><code>\nmyproject\n\u251c\u2500\u2500 BIDS\n\u2502   \u2514\u2500\u2500 sub-01\n\u2502       \u251c\u2500\u2500 anat\n\u2502       \u2502   \u251c\u2500\u2500sub-01_T1w.nii\n\u2502       \u2502   \u2514\u2500\u2500 sub-01_T1w.json\n\u2502       \u2514\u2500\u2500 func\n\u2502           \u251c\u2500\u2500 sub-01_task-{taskname}_run-01_bold.json\n\u2502           \u251c\u2500\u2500 sub-01_task-{taskname}_run-01_bold.nii\n\u2502           \u251c\u2500\u2500 ...\n\u2502           \u251c\u2500\u2500 sub-01_task-{taskname}_run-{runnumber}_bold.json\n\u2502           \u2514\u2500\u2500 sub-01_task-{taskname}_run-{runnumber}_bold.nii\n\u2514\u2500\u2500 sourcedata\n    \u2514\u2500\u2500 sub-01\n        \u251c\u2500\u2500 bh\n        \u251c\u2500\u2500 dicom\n        \u251c\u2500\u2500 dicom_anon\n        \u2514\u2500\u2500 nifti\n</code></pre> <p>Create <code>.json</code> sidecar files for each functional run <code>.nii</code> file, using the output from the dicom conversion step.</p> <p>Each <code>nii</code> file must have a sidecar JSON file. Make sure you anonymised and converted your DICOM files and go through the following steps:</p> <ol> <li>Locate the JSON sidecar files in <code>sourcedata/sub-xx/nifti/</code>.</li> <li>Open each JSON file and Complete the <code>PhaseEncodingDirection</code> and <code>SliceTiming</code> fields (see Missing fields in JSON files for more information).</li> <li>Copy-paste the updated JSON files to accompany each NIfTI file in the <code>BIDS/sub-xx/func</code> folder: each run should have its accompanying <code>sub-xx_task-{taskname}_run-{runnumber}_bold.json</code> sidecar file.</li> </ol>"},{"location":"research/fmri/analysis/fmri-bids-conversion.html#8-create-event-files","title":"8. Create event files","text":"<pre><code>\nmyproject\n\u251c\u2500\u2500 BIDS\n\u2502   \u2514\u2500\u2500 sub-01\n\u2502       \u251c\u2500\u2500 anat\n\u2502       \u2502   \u251c\u2500\u2500sub-01_T1w.nii\n\u2502       \u2502   \u2514\u2500\u2500 sub-01_T1w.json\n\u2502       \u2514\u2500\u2500 func\n\u2502           \u251c\u2500\u2500 sub-01_task-{taskname}_run-01_bold.json\n\u2502           \u251c\u2500\u2500 sub-01_task-{taskname}_run-01_bold.nii\n\u2502           \u251c\u2500\u2500 sub-01_task-{taskname}_run-01_events.tsv\n\u2502           \u251c\u2500\u2500 ...\n\u2502           \u251c\u2500\u2500 sub-01_task-{taskname}_run-{runnumber}_bold.json\n\u2502           \u251c\u2500\u2500 sub-01_task-{taskname}_run-{runnumber}_bold.nii\n\u2502           \u2514\u2500\u2500 sub-01_task-{taskname}_run-{runnumber}_events.tsv\n\u251c\u2500\u2500 code\n\u2514\u2500\u2500 sourcedata\n    \u2514\u2500\u2500 sub-01\n        \u251c\u2500\u2500 bh\n        \u251c\u2500\u2500 dicom\n        \u251c\u2500\u2500 dicom_anon\n        \u251c\u2500\u2500 eye\n        \u2514\u2500\u2500 nifti\n</code></pre> <p>Create one <code>events.tsv</code> file for each function run <code>.nii</code> file, using the output from your experimental task. If you used the fMRI task template), output log files can be used to create event files quite easily. More info on events files can be found here.</p> <p>Event files are crucial for analyzing fMRI data. They contain information about the timing and nature of stimuli or tasks during the scan. To create your event files manually:</p> <ol> <li>Navigate to your <code>sourcedata/sub-xx/bh/</code> folder.</li> <li>Locate the behavioral output files (<code>.mat</code> or <code>.log</code>) for each run.</li> <li>Create a corresponding <code>events.tsv</code> file for each run in the <code>BIDS/sub-xx/func/</code> folder.</li> </ol> <p>Each <code>events.tsv</code> file must contain at least three columns: <code>onset</code>, <code>duration</code>, and <code>trial_type</code>, and can include additional as needed for your specific analysis. It also must contain one row per trial (stimulus) in your experiment.</p> <p>If you use the fMRI task template, the log files you get as output contain all the information needed to build event files in a few steps. Below is a quick overview of the steps to take to make event files from log files. Note that it might not apply perfectly to all cases, and that other approaches can be more practical to you. It can be a good idea to create your own utility script to create event files from your behavioural results.</p> <p>To create event files from log files, here is what you need to do (see the example below for an example transformation):</p> <ul> <li>Create the <code>onset</code> column from the onset values in the log files: in the latter, onset times (usually stored in a column called <code>ACTUAL_ONSET</code>) are aligned to the start of the run. In BIDS event files, events need to be aligned to the start of the scanning. To obtain correct onset values, one can simply shift the onset of each line from a log file so that the onset <code>0.0</code> corresponds to the first TR trigger.</li> <li>Create the <code>duration</code> column from the <code>onset</code> values. Log files typically don't record the exact duration of events, as that would put some extra calculation load onto MatLab (which struggles enough already as it is). A good approach is to calculate these post-hoc from the onset values, by simply taking the difference in between successive event onsets.</li> <li>Create the <code>trial_type</code> column with the condition names. Fill this column by extracting the information that is relevant for your experimental design. In the example below, we extract the condition names <code>face</code> and <code>building</code> from the <code>EVENT_ID</code> column, as these are the conditions we're interested in.</li> <li>Add or keep any extra column you might need. In the example below, we keep the <code>event_id</code> columns as it might still be useful later on in the pipeline. Note that you should make a reference to these extra column in your <code>events.json</code> file.</li> </ul> <p>For example, this is what a log file looks like:</p> <pre><code>EVENT_TYPE  EVENT_NAME  DATETIME                EXP_ONSET     ACTUAL_ONSET  DELTA     EVENT_ID\nSTART       -           yyyy-mm-dd-hh-mm-ss  -               0.000000  -         -\nFLIP        Instr       yyyy-mm-dd-hh-mm-ss  -               0.099950  -         -\nRESP        KeyPress    yyyy-mm-dd-hh-mm-ss  -               7.663277  -         7\nFLIP        TgrWait     yyyy-mm-dd-hh-mm-ss  -               7.697805  -         -\nPULSE       Trigger     yyyy-mm-dd-hh-mm-ss  -              12.483778  -         5\nPULSE       Trigger     yyyy-mm-dd-hh-mm-ss  -              24.452093  -         5\nFLIP        Pre-fix     yyyy-mm-dd-hh-mm-ss  -              24.462263  -         -\nPULSE       Trigger     yyyy-mm-dd-hh-mm-ss  -              26.452395  -         5\nPULSE       Trigger     yyyy-mm-dd-hh-mm-ss  -              28.452362  -         5\nPULSE       Trigger     yyyy-mm-dd-hh-mm-ss  -              30.451807  -         5\nPULSE       Trigger     yyyy-mm-dd-hh-mm-ss  -              32.451339  -         5\nPULSE       Trigger     yyyy-mm-dd-hh-mm-ss  -              34.451376  -         5\nFLIP        Stim        yyyy-mm-dd-hh-mm-ss  34.462263      34.474302  0.012039  building_image.png\nRESP        KeyPress    yyyy-mm-dd-hh-mm-ss  -              35.566808  -         9\nFLIP        Fix         yyyy-mm-dd-hh-mm-ss  -              34.521628  -         -\nPULSE       Trigger     yyyy-mm-dd-hh-mm-ss  -              36.451615  -         5\nFLIP        Stim        yyyy-mm-dd-hh-mm-ss  37.462263      37.524439  0.062177  face_image.png\nFLIP        Fix         yyyy-mm-dd-hh-mm-ss  -              37.572648  -         -\nPULSE       Trigger     yyyy-mm-dd-hh-mm-ss  -              38.453535  -         5\nRESP        KeyPress    yyyy-mm-dd-hh-mm-ss  -              38.806193  -         1\nPULSE       Trigger     yyyy-mm-dd-hh-mm-ss  -              40.451415  -         5\n...\n</code></pre> <p>This is what the corresponding event file should look like:</p> <pre><code>onset           duration        trial_type     event_id\n10.022209       0.0473259       building       building_image.png\n13.072346       0.0482089       face           face_image.png\n</code></pre>"},{"location":"research/fmri/analysis/fmri-bids-conversion.html#9-create-additional-bids-files","title":"9. Create additional BIDS files","text":"<pre><code>\nmyproject\n\u251c\u2500\u2500 BIDS\n\u2502   \u251c\u2500\u2500 dataset_description.json\n\u2502   \u251c\u2500\u2500 events.json\n\u2502   \u251c\u2500\u2500 participants.json\n\u2502   \u251c\u2500\u2500 participants.tsv\n\u2502   \u251c\u2500\u2500 sub-01\n\u2502   \u2502   \u251c\u2500\u2500 anat\n\u2502   \u2502   \u2514\u2500\u2500 func\n\u2502   \u2514\u2500\u2500 task-taskname_bold.json\n\u2514\u2500\u2500 sourcedata\n    \u2514\u2500\u2500 sub-01\n        \u251c\u2500\u2500 bh\n        \u251c\u2500\u2500 dicom\n        \u251c\u2500\u2500 dicom_anon\n        \u251c\u2500\u2500 eye\n        \u2514\u2500\u2500 nifti\n</code></pre> <ol> <li> <p>Create the following modality agnostic BIDS files files in your <code>BIDS/</code> folder:</p> <ul> <li><code>dataset_description.json</code></li> <li><code>participants.tsv</code></li> <li><code>participants.json</code></li> <li><code>task-&lt;taskname&gt;_bold.json</code></li> </ul> </li> <li> <p>Fill in the required information for each file according to the BIDS specification.</p> </li> </ol> <p>And set up additional components:</p> <ol> <li>Create a <code>derivatives/</code> folder in your <code>BIDS/</code> directory.</li> <li>If needed, create a <code>.bidsignore</code> file in your <code>BIDS/</code> root folder to exclude any non-BIDS compliant files.</li> </ol> Why should I use a .bidsignore file? <p>A `.bidsignore' file is useful to communicate to the BIDS validator which files should not be indexed, because they are not part of the standard BIDS structure. More information can be found here.</p>"},{"location":"research/fmri/analysis/fmri-bids-conversion.html#10-validating-your-bids-structure","title":"10. Validating Your BIDS Structure","text":"<p>By following these steps systematically, you'll ensure your data is properly organized in BIDS format, facilitating easier analysis and collaboration.</p> <p>Make sure all the steps have been followed successfully by validating your BIDS folder. To do so, use the BIDS validator.</p> <ol> <li>Use the online BIDS Validator to check your BIDS structure.</li> <li>Upload your entire <code>BIDS/</code> folder and review any errors or warnings.</li> <li>Make necessary corrections based on the validator's output.</li> </ol> <p>By following these detailed steps, you'll ensure your data is properly organized in BIDS format, facilitating easier analysis and collaboration.</p> <p>Now that you have your data in BIDS format, we can proceed to data pre-processing and quality assessment. See the next guide for instructions. \u2192 Pre-processing and QA</p>"},{"location":"research/fmri/analysis/fmri-general.html","title":"General Notes","text":"<p>You should land on this page after collecting your fMRI data and converting it to BIDS. Here, you\u2019ll find all the general information and FAQs about fMRI protocols.</p> <p>Data Storage Suggestion</p> <p>Bring a dedicated hard drive to the hospital for storing the output data. This will ensure that you have a reliable medium to transfer and secure the raw data from the scanner.</p>"},{"location":"research/fmri/analysis/fmri-general.html#quick-links-to-resources","title":"Quick Links to Resources","text":"<p>These resources provide essential information and tutorials that can be consulted before or during your journey into SPM and fMRI analysis:</p> <ul> <li>SPM Online Documentation - fMRI Tutorials: A very nice set of tutorials for functional MRI analysis using Statistical Parametric Mapping (SPM).</li> <li>fMRI Prep and Analysis with Andrew Jahn: A ( The) YouTube channel offering tutorials on fMRI preprocessing and analysis, covering various aspects of neuroimaging.</li> <li>SPM Programming Introduction: An introduction to programming with SPM.</li> <li>SPM Manual: The official SPM manual, providing in-depth information on all aspects of SPM, from installation to advanced analysis techniques.</li> <li>Introduction to SPM by Karl Friston: A brief guide to statistical parametric mapping, adapted from K. Friston\u2019s 2003 introductory notes.</li> </ul>"},{"location":"research/fmri/analysis/fmri-general.html#how-to-store-raw-data","title":"How to Store Raw Data","text":"<p>To avoid errors during BIDS conversion, store the raw data (e.g., data collected from the scanner, behavioral measures, eye-tracking) with the following folder structure:</p> <pre><code>sourcedata\n\u2514\u2500\u2500 sub-41\n    \u251c\u2500\u2500 bh\n    \u2502   \u251c\u2500\u2500 20240503104938_log_41-1-2_exp.tsv\n    \u2502   \u251c\u2500\u2500 20240503105558_41_1_exp.mat\n    \u2502   \u251c\u2500\u2500 20240503105640_log_41-2-1_exp.tsv\n    \u2502   \u251c\u2500\u2500 20240503110226_41_2_exp.mat\n    \u2502   \u251c\u2500\u2500 20240503110241_log_41-3-2_exp.tsv\n    \u2502   \u251c\u2500\u2500 20240503110825_41_3_exp.mat\n    \u2502   \u251c\u2500\u2500 20240503110851_log_41-4-1_exp.tsv\n    \u2502   \u251c\u2500\u2500 20240503111433_41_4_exp.mat\n    \u2502   \u251c\u2500\u2500 20240503111450_log_41-5-2_exp.tsv\n    \u2502   \u2514\u2500\u2500 20240503112032_41_5_exp.mat\n    \u2514\u2500\u2500 nifti\n        \u251c\u2500\u2500 sub-41_WIP_CS_3DTFE_8_1.nii\n        \u251c\u2500\u2500 sub-41_WIP_Functional_run1_3_1.nii\n        \u251c\u2500\u2500 sub-41_WIP_Functional_run2_4_1.nii\n        \u251c\u2500\u2500 sub-41_WIP_Functional_run3_5_1.nii\n        \u251c\u2500\u2500 sub-41_WIP_Functional_run4_6_1.nii\n        \u2514\u2500\u2500 sub-41_WIP_Functional_run5_7_1.nii\n</code></pre> <p>Folder Structure</p> <p>Ensure each subject\u2019s data is organized as shown above to minimize errors during BIDS conversion. Store all behavioral and NIfTI files under <code>sourcedata</code>.</p>"},{"location":"research/fmri/analysis/fmri-general.html#how-to-get-images-from-the-scanner","title":"How to Get Images from the Scanner","text":"<p>For optimal BIDS conversion of fMRI data, it is recommended to initially collect DICOM files (not NIfTI or PAR/REC) at the scanner. Although this adds an extra conversion step, it ensures accurate conversion into BIDS format. Follow these steps:</p> <ol> <li> <p>Initial DICOM Collection:</p> <ul> <li>Collect DICOM files for each modality (e.g., T1 and BOLD) for one subject.</li> <li>Convert these DICOM files to NIfTI format using <code>dcm2nii</code>, which will generate JSON sidecar files. Refer to this section for more details on the conversion process.</li> </ul> </li> <li> <p>Template Creation:</p> <ul> <li>Rename the JSON files for T1 and BOLD images to <code>sub-xx_T1w.json</code> and <code>sub-xx_task-exp_run-x_bold.json</code>.</li> <li>Move the JSON files into the <code>misc/</code> folder.</li> </ul> </li> <li> <p>Subsequent Data Collection:</p> <ul> <li>After creating the template JSON files, collect future data directly in NIfTI format to save time. The <code>script01_nifti-to-BIDS.m</code> script will use the JSON templates to populate the BIDS folders, as long as the fMRI sequence remains unchanged. If the sequence changes, generate new templates from the DICOM files.</li> </ul> </li> </ol>"},{"location":"research/fmri/analysis/fmri-general.html#missing-fields-in-json-files","title":"Missing Fields in JSON Files","text":"<p>Despite these steps, some BIDS fields in the sidecar JSON files may remain empty due to limitations of the Philips scanner. The most relevant fields that may be left empty are <code>SliceTiming</code> and <code>PhaseEncodingDirection</code>.</p> <ul> <li>SliceTiming:</li> <li>This field is required by fMRIPrep during slice timing correction.</li> <li> <p>Populate it using the <code>get_philips_MB_slicetiming.py</code> script, assuming you have access to a DICOM file and know the multiband factor (default is 2, as used in our lab).     !!! warning         The script assumes an interleaved, foot-to-head acquisition and will not work for other acquisition types.</p> </li> <li> <p>PhaseEncodingDirection:</p> </li> <li>This BIDS tag helps tools undistort images.</li> <li>Philips DICOM headers specify the phase encoding axis (e.g., A-P or L-R) but not the polarity (A \u2192 P or P \u2192 A).</li> <li>Check the scanner settings or consult with Ron to determine whether the polarity is A \u2192 P or P \u2192 A, and update the <code>?</code> in the JSON file with <code>j</code> (P \u2192 A) or <code>j-</code> (A \u2192 P).</li> <li>More info here and here</li> </ul> <p>Handling NaNs in JSON Files</p> <p>NaN values in JSON files can cause errors during the MRIQC workflow. To address NaN values, see the discussions in this post, this GitHub issue, and this NeuroStars thread.</p> <p>For more details on Philips DICOM conversion, refer to the following resources:</p> <ul> <li>Philips DICOM Missing Information - dcm2niix</li> <li>PARREC Conversion - dcm2niix</li> </ul>"},{"location":"research/fmri/analysis/fmri-general.html#where-to-find-additional-info-on-the-fmri-sequence","title":"Where to Find Additional Info on the fMRI Sequence","text":"<p>Additional information on the fMRI sequence can be found directly at the scanner. Here\u2019s a step-by-step guide:</p> <ol> <li> <p>Start the Examination:</p> <ul> <li>Go to Patients -&gt; New Examination -&gt; RIS.</li> <li>Select your subject and fill out the required fields:<ul> <li>Weight: Enter the subject's weight.</li> <li>Implants: Specify if the subject has any implants.</li> <li>Pregnant: Indicate if the subject is pregnant.</li> </ul> </li> </ul> </li> <li> <p>Load the Scanning Sequence:</p> <ul> <li>Drag and drop your scanning sequence from the bottom panel to the left panel.</li> </ul> </li> <li> <p>Select a Run:</p> <ul> <li>Click on either a functional or anatomical run from the available list.</li> </ul> </li> <li> <p>Expand the Tabs:</p> <ul> <li>Click on the <code>&gt;&gt;</code> symbol in the bottom panel, below the sagittal, coronal, and horizontal views, to expand additional tabs.</li> </ul> </li> <li> <p>Access Geometry Settings:</p> <ul> <li>Navigate to the Geometry tab to access important scan parameters:<ul> <li>MB factor: Indicates the number of slices recorded simultaneously, used for slice timing correction.</li> <li>Slices: Total number of horizontal slices.</li> <li>Fold-over direction: Required for correcting the phase encoding direction in the BIDS field.</li> <li>Slice scan order: Typically Foot to Head (FH), used for slice timing correction.</li> </ul> </li> </ul> </li> <li> <p>Check Additional Fields:</p> <ul> <li>Visit the Coils tab for details about the head coils used during the scan.</li> <li>In the Contrast tab, note the following fields:<ul> <li>TE (Echo Time): Usually a single echo of 30 ms by default.</li> <li>TR (Repetition Time): Typically set to 2000 ms by default.</li> </ul> </li> </ul> </li> </ol> <p>Next Step \u2192 Set-up your environment</p>"},{"location":"research/fmri/analysis/fmri-glm-script.html","title":"Introduction to SPM Scripting with <code>matlabbatch</code>","text":"<p>You should land on this page after having collected your fMRI data, converted it to BIDS and preprocessed it. Your goal now is to model the BOLD activity with a Generalised Linear Model (GLM), in order to obtain the beta values on which to apply further analyses. Here, we do so using SPM (Statistical Parametric Mapping), and by scripting the steps taken by SPM. While the SPM GUI makes analysis accessible, scripting with <code>matlabbatch</code> in MATLAB provides a reproducible, automated workflow. This guide introduces you to SPM scripting, from using the GUI to generate code snippets to creating a full multi-step <code>matlabbatch</code> job for SPM.</p> <p>You should start here if you are familiar with SPM and what it does. If you feel like the analysis steps are still unclear to you, take some time to learn them by using the GUI first. Check out First-level analysis - GUI.</p>"},{"location":"research/fmri/analysis/fmri-glm-script.html#1-start-with-the-spm-gui","title":"1. Start with the SPM GUI","text":"<p>SPM's GUI is a valuable resource for beginners. It allows you to set analysis parameters interactively and generate corresponding MATLAB code. This method helps you learn which settings are required for each analysis step and understand how they translate into code.</p>"},{"location":"research/fmri/analysis/fmri-glm-script.html#steps-in-the-gui","title":"Steps in the GUI","text":"<ol> <li>Open SPM by typing <code>spm fmri</code> in the MATLAB command window, then click <code>Batch</code></li> <li>Navigate through each analysis step you want to script. For example, start with <code>fMRI Model Specification</code> under the <code>SPM</code> \u2192 <code>Stats</code> menu.</li> <li>Set parameters such as:<ul> <li>Timing (e.g., Repetition Time, microtime resolution).</li> <li>Session information (e.g., scans, conditions, and high-pass filter).</li> </ul> </li> <li> <p>See the code by clicking on <code>View</code> \u2192 <code>Show .m code</code>.</p> <p>This should show something like this:</p> <pre><code>matlabbatch{1}.spm.stats.fmri_spec.dir = '&lt;UNDEFINED&gt;';\nmatlabbatch{1}.spm.stats.fmri_spec.timing.units = '&lt;UNDEFINED&gt;';\nmatlabbatch{1}.spm.stats.fmri_spec.timing.RT = '&lt;UNDEFINED&gt;';\nmatlabbatch{1}.spm.stats.fmri_spec.timing.fmri_t = 16;\nmatlabbatch{1}.spm.stats.fmri_spec.timing.fmri_t0 = 8;\nmatlabbatch{1}.spm.stats.fmri_spec.sess = struct('scans', {}, 'cond', {}, 'multi', {}, 'regress', {}, 'multi_reg', {}, 'hpf', {});\nmatlabbatch{1}.spm.stats.fmri_spec.fact = struct('name', {}, 'levels', {});\nmatlabbatch{1}.spm.stats.fmri_spec.bases.hrf.derivs = [0 0];\nmatlabbatch{1}.spm.stats.fmri_spec.volt = 1;\nmatlabbatch{1}.spm.stats.fmri_spec.global = 'None';\nmatlabbatch{1}.spm.stats.fmri_spec.mthresh = 0.8;\nmatlabbatch{1}.spm.stats.fmri_spec.mask = {''};\nmatlabbatch{1}.spm.stats.fmri_spec.cvi = 'AR(1)';\n</code></pre> </li> </ol> <p>Tip</p> <p>To use this feature to learn scripting, save several batches with different parameters. Open the generated code to see how each parameter is defined in <code>matlabbatch</code>.</p> <p>Here\u2019s an example of how to use the GUI to extract the respective code, here using fMRI Model Specification and Estimation.</p> <ol> <li> <p>Model Specification:</p> <ul> <li>Go to SPM &gt; Stats &gt; fMRI Model Specification.</li> <li>Set timing parameters (e.g., TR, microtime resolution).</li> <li>Add conditions and high-pass filter settings.</li> <li>Save the batch via File &gt; Save Batch and Script.</li> <li> <p>Open the generated script and review settings like:</p> <pre><code>matlabbatch{1}.spm.stats.fmri_spec.dir = {'/path/to/output'};\nmatlabbatch{1}.spm.stats.fmri_spec.timing.RT = 2;  % Repetition Time in seconds\n</code></pre> </li> </ul> </li> <li> <p>Model Estimation:</p> <ul> <li>In the GUI, go to Stats &gt; Model Estimation.</li> <li>Select your <code>SPM.mat</code> file and save the settings.</li> <li> <p>Observe that model estimation is added as <code>matlabbatch{2}</code> in the saved script:</p> <pre><code>matlabbatch{2}.spm.stats.fmri_est.spmmat = {'/path/to/SPM.mat'};\nmatlabbatch{2}.spm.stats.fmri_est.method.Classical = 1; % Classical estimation\n</code></pre> </li> </ul> </li> </ol>"},{"location":"research/fmri/analysis/fmri-glm-script.html#2-understand-the-basics-of-matlabbatch-jobs","title":"2. Understand the Basics of <code>matlabbatch</code> Jobs","text":"<p>The <code>matlabbatch</code> structure in SPM is a versatile container for storing all settings and steps for your analysis. Each field in <code>matlabbatch</code> corresponds to a parameter in the SPM GUI.</p>"},{"location":"research/fmri/analysis/fmri-glm-script.html#key-concepts-of-matlabbatch-jobs","title":"Key Concepts of <code>matlabbatch</code> Jobs","text":"<ul> <li>Jobs and Batches: In SPM scripting, \"job\" refers to a specific processing step (e.g., model specification or estimation), while \"batch\" is a collection of jobs executed sequentially.</li> <li>Fields and Indexing: Each field in <code>matlabbatch</code> corresponds to a specific setting. For example, <code>matlabbatch{1}.spm.stats.fmri_spec.timing.RT</code> sets the repetition time (TR).</li> <li>Sessions: Sessions in <code>matlabbatch</code> typically refer to fMRI runs and are indexed using <code>sess(runIdx)</code> for each run in an analysis.</li> </ul> <p>SPM Batch Workflow</p> <p>In SPM scripting, all parameters are set first in <code>matlabbatch</code>, then executed together in a single command:</p> <pre><code>spm_jobman('run', matlabbatch);\n</code></pre>"},{"location":"research/fmri/analysis/fmri-glm-script.html#3-assembling-matlabbatch-jobs-in-matlab","title":"3. Assembling <code>matlabbatch</code> Jobs in MATLAB","text":"<p>With the basics covered, let\u2019s construct a full <code>matlabbatch</code> job in MATLAB. This example demonstrates setting up a simple fMRI model specification, estimation, and contrast definition.</p> Code Block: Full <code>matlabbatch</code> Example <pre><code>% Define output directory and timing parameters\nmatlabbatch{1}.spm.stats.fmri_spec.dir = {'/path/to/output'};\nmatlabbatch{1}.spm.stats.fmri_spec.timing.RT = 2;\nmatlabbatch{1}.spm.stats.fmri_spec.timing.fmri_t = 16;\nmatlabbatch{1}.spm.stats.fmri_spec.timing.fmri_t0 = 8;\n\n% Define sessions (runs) and scans\nmatlabbatch{1}.spm.stats.fmri_spec.sess(1).scans = {'/path/to/run1.nii'};\nmatlabbatch{1}.spm.stats.fmri_spec.sess(1).cond(1).name = 'Condition A';\nmatlabbatch{1}.spm.stats.fmri_spec.sess(1).cond(1).onset = [10, 20, 30];\nmatlabbatch{1}.spm.stats.fmri_spec.sess(1).cond(1).duration = [5, 5, 5];\n\n% Model estimation\nmatlabbatch{2}.spm.stats.fmri_est.spmmat = {'/path/to/output/SPM.mat'};\nmatlabbatch{2}.spm.stats.fmri_est.method.Classical = 1;\n\n% Contrast specification\nmatlabbatch{3}.spm.stats.con.spmmat = {'/path/to/output/SPM.mat'};\nmatlabbatch{3}.spm.stats.con.consess{1}.tcon.name = 'Condition A &gt; Baseline';\nmatlabbatch{3}.spm.stats.con.consess{1}.tcon.weights = 1;\nmatlabbatch{3}.spm.stats.con.consess{1}.tcon.sessrep = 'none';\n</code></pre> <p>Each block in <code>matlabbatch</code> represents one step in the analysis, identical to configuring them in the GUI.</p> <p>Important</p> <p>This code will run all steps sequentially when you call <code>spm_jobman('run', matlabbatch);</code>. In this example: - <code>matlabbatch{1}</code> specifies the model. - <code>matlabbatch{2}</code> estimates the model. - <code>matlabbatch{3}</code> defines the contrasts.</p>"},{"location":"research/fmri/analysis/fmri-glm-script.html#4-executing-the-job-in-matlab","title":"4. Executing the Job in MATLAB","text":"<p>With all steps defined in <code>matlabbatch</code>, execute the entire analysis workflow:</p> <pre><code>spm('defaults', 'fmri');\nspm_jobman('initcfg');\nspm_jobman('run', matlabbatch);\n</code></pre> <ul> <li><code>spm('defaults', 'fmri');</code>: Initializes SPM with fMRI defaults.</li> <li><code>spm_jobman('initcfg');</code>: Prepares the job manager for execution.</li> <li><code>spm_jobman('run', matlabbatch);</code>: Executes all defined steps in sequence, producing output in the specified directory.</li> </ul>"},{"location":"research/fmri/analysis/fmri-glm-script.html#5-organizing-code-in-functions","title":"5. Organizing Code in Functions","text":"<p>Using functions in MATLAB effectively improves code readability, reduces redundancy, and simplifies maintenance.</p> <p>Here\u2019s a breakdown of best practices for managing functions in MATLAB, including where to place them and how to add their directory to the MATLAB path.</p>"},{"location":"research/fmri/analysis/fmri-glm-script.html#where-to-place-functions","title":"Where to Place Functions","text":"Reusable FunctionsScript-Specific Functions <p>If a function is intended for use across multiple scripts, it's best to save it as a separate <code>.m</code> file in a designated folder (e.g., <code>scripts/functions</code>). This keeps your code organized and ensures that changes to the function are applied consistently across all scripts.</p> <p>To make sure MATLAB can locate your function, add the functions folder to the MATLAB path. This can be done in two ways:</p> <ol> <li> <p>Command Line: Run the following command in the MATLAB command window:</p> <pre><code>addpath('/path/to/scripts/functions');\n</code></pre> </li> <li> <p>Using the MATLAB GUI:</p> <ul> <li>Go to Home &gt; Set Path.</li> <li>Click Add Folder\u2026 and navigate to the folder containing your functions.</li> <li>Click Save to add this path permanently or Close to add it temporarily for the session.</li> </ul> </li> </ol> <p>By adding the folder to your path, the function is accessible across all scripts, ensuring consistency and reducing redundancy.</p> <p>If a function is specifically tailored for a single script and not intended for reuse, include it at the end of that script. This keeps the function accessible only within the script, reducing dependencies on external files.</p> <p>Here\u2019s an example of including a function at the end of a script.</p> <pre><code>% Main part of the script\nmyVar = setup_variable(10, 5);\n\n% Function definition at the end of the script\nfunction result = setup_variable(a, b)\n    % This function takes two numbers and multiplies them\n    result = a * b;\nend\n</code></pre>"},{"location":"research/fmri/analysis/fmri-glm-script.html#example-creating-a-reusable-function","title":"Example: Creating a Reusable Function","text":"<p>If <code>setup_variable</code> is a function you\u2019ll use frequently, save it as a separate <code>.m</code> file in your <code>functions</code> directory. The function file <code>setup_variable.m</code> might look like this:</p> <pre><code>function result = setup_variable(a, b)\n    % This function takes two numbers and multiplies them\n    result = a * b;\nend\n</code></pre> <p>Then, in your main script, you can call the function after adding the functions folder to the path. For example:</p> <pre><code>addpath('/path/to/functions');\nmyVar = setup_variable(10, 5);\n</code></pre>"},{"location":"research/fmri/analysis/fmri-glm-script.html#6-full-code-example","title":"6. Full code example","text":"<p>Once you have a general understanding of using the SPM GUI and <code>matlabbatch</code> scripting, you\u2019re ready to try more advanced scripting techniques. This final section provides a complete example script that integrates everything covered so far. Specifically, the script below:</p> <ul> <li>Sets up essential parameters and paths for analyzing fMRI data,</li> <li>Loads event and confound files for each run,</li> <li>Specifies, estimates, and applies contrasts for a General Linear Model (GLM) in SPM,</li> <li>Saves the final model and a copy of the script for reproducibility.</li> </ul> <p>Below, you\u2019ll find the main script, <code>performGLMAutoContrast.m</code>, which performs all these steps, and all required functions to execute the script. Most of the  functions are completely stand-alone, and can be used in your own projects (e.g., <code>smoothNiftiFile</code>, <code>gunzipNiftiFile</code>, <code>fMRIprepConfounds2SPM</code>, <code>eventsBIDS2SPM</code>).</p> <p>These functions can either be saved as standalone <code>.m</code> files in a <code>functions</code> folder (e.g., <code>setup_model_specification.m</code>) or simply copy-pasted at the end of the script for direct use. If you choose to save functions as standalone files, make sure the filename exactly matches the function name (e.g., <code>setup_model_specification.m</code> for a function called <code>setup_model_specification</code>). This ensures MATLAB can locate and run the function correctly.</p> performGLMAutoContrast.m <pre><code>% GLM Analysis Script for fMRI Data using SPM\n%\n% This script performs a General Linear Model (GLM) analysis on fMRI data that has been preprocessed using fMRIprep and\n% is organized in BIDS format. It is designed to run in MATLAB using SPM, specifically for researchers who may be\n% unfamiliar with SPM scripting but have fMRI data analysis needs. The script follows standard steps for GLM setup,\n% specification, estimation, and contrast definition, which are essential in understanding task-related brain activity.\n%\n% Overview of What This Script Does:\n% - Sets up paths and parameters.\n% - Loads event and confound data for each run and prepares it for GLM analysis.\n% - Specifies the GLM parameters, including timing, high-pass filter, and basis function.\n% - Runs model specification and estimation to fit the GLM to the fMRI data.\n% - Defines contrasts to test specific hypotheses.\n% - Saves the final model and a copy of this script for future reference.\n%\n% Steps in this Script:\n% 1. **Set Parameters**: Define paths to data, fMRI timing parameters, selected tasks, contrasts, and subjects.\n% 2. **Load Events and Confounds**: Each run`s event (trial timing) and confound (noise regressors) data are loaded\n%    from BIDS-compliant TSV and JSON files produced by fMRIprep. This prepares each run for GLM analysis.\n% 3. **SPM Model Specification**: Sets up a structure (called`matlabbatch`) that stores all necessary parameters\n%    for defining the GLM model. Each parameter here mirrors options in the SPM GUI.\n% 4. **Run Model Specification and Estimation**: Once all parameters are set in`matlabbatch`, we run batches 1\n%    and 2. Batch 1 specifies the model, and batch 2 estimates the model using the data and settings we\u2019ve defined.\n% 5. **Check SPM.mat File**: Verifies the existence of the SPM.mat file after running the model, confirming the\n%    model specification and estimation steps were successful.\n% 6. **Define and Run Contrasts**: Sets up contrasts based on hypotheses about brain activity, applies them, and\n%    stores results. This is done in batch 3.\n% 7. **Save Script**: Saves a copy of this script in the output folder for reproducibility and future reference.\n\n% Working with matlabbatch and SPM Batches:\n% - `matlabbatch` is a structure used by SPM to store each step of an analysis, organized as separate fields for \n%   each processing stage (e.g., model specification, model estimation, contrast definition). Each entry in \n%   `matlabbatch` corresponds to a batch operation you could run manually in the SPM GUI. SPM\u2019s batch system is \n%   powerful because it allows us to set parameters programmatically in a way that is identical to the GUI.\n% - **Setting Parameters**: Each field in `matlabbatch` represents a setting or action, matching options in the \n%   SPM GUI. For example, in this script, `matlabbatch{1}.spm.stats.fmri_spec.timing.RT` corresponds to setting \n%   the Repetition Time (RT) in the SPM GUI.\n% - **Retrieving GUI Code**: To see the code for any parameter set in the SPM GUI, configure the GUI settings and \n%   then select **File &gt; Save Batch and Script**. SPM will generate MATLAB code that mirrors the GUI settings, \n%   which you can use to understand how `matlabbatch` fields align with GUI options.\n%\n% Understanding SPM Batches and Sessions:\n% - In SPM, **sessions** represent individual runs in a study. Each run (session) is specified separately within \n%   `matlabbatch`, which enables you to specify different conditions or timing settings per run if necessary.\n% - We use indexing within `matlabbatch` to indicate which session or contrast we\u2019re defining. For instance, \n%   `matlabbatch{1}.spm.stats.fmri_spec.sess(runIdx).scans` refers to the scans (images) in session `runIdx`.\n%\n% How SPM Executes Batches:\n% - SPM uses a deferred execution approach: parameters are specified first by configuring `matlabbatch`, and \n%   then `spm_jobman('run', matlabbatch)` initiates all specified processing steps in sequence, similar to \n%   clicking \"Run\" in the GUI. In this script, we define `matlabbatch` elements in sequence (batches 1 to 3) \n%   and run them together in one command to automate the workflow.\n%\n% Assumptions about Input Data:\n% - Data must be preprocessed using fMRIprep and follow the BIDS format.\n% - Event files (in TSV format) must adhere to BIDS specifications.\n% - Confound files (in JSON and TSV formats) must adhere to fMRIprep output standards.\n%\n% Requirements:\n% - SPM12 (or a compatible version) must be installed and added to the MATLAB path.\n%\n% This script automates the process typically done in the SPM GUI.\n\n% Parameters:\n% - selectedSubjectsList: List of subject IDs to include in the analysis. Can be a list of integers or '*' to include all subjects.\n% - selectedRuns: List of run numbers to include in the analysis. Can be a list of integers or '*' to include all runs.\n% - selectedTasks: Structure array with information about the task(s) to analyze.\n%     Each task must have the following fields:\n%     - name: String. The name of the task.\n%     - contrasts: Cell array of strings. The name(s) of the contrast(s).\n%     - weights: Cell array of structs. The weights for the contrast(s).\n%     - smoothBool: Boolean. Whether to smooth the data before the GLM.\n%\n% Example of defining tasks, weights, and contrasts:\n% selectedTasks(1).name = 'loc';\n% selectedTasks(1).contrasts = {'Faces &gt; Objects', 'Objects &gt; Scrambled', 'Scenes &gt; Objects'};\n% selectedTasks(1).weights(1) = struct('Faces', 1, 'Objects', -1, 'Scrambled', 0, 'Scenes', 0);\n% selectedTasks(1).weights(2) = struct('Faces', 0, 'Objects', 1, 'Scrambled', -1, 'Scenes', 0);\n% selectedTasks(1).weights(3) = struct('Faces', 0, 'Objects', -1, 'Scrambled', 0, 'Scenes', 1);\n% selectedTasks(1).smoothBool = true;\n%\n% Paths:\n% - fmriprepRoot: Path to the root folder of the fMRIprep output.\n% - BIDSRoot: Path to the root folder of the BIDS dataset.\n% - outRoot: Path to the output root folder for the analysis results.\n%\n% Preprocessing Options (uses fMRIprep confounds table):\n% - pipeline: Denoising pipeline strategy for SPM confound regression.\n%     It should be a cell array of strings specifying the strategies to use.\n%     Possible strategies:\n%         HMP - Head motion parameters (6,12,24)\n%         GS - Brain mask global signal (1,2,4)\n%         CSF_WM - CSF and WM masks global signal (2,4,8)\n%         aCompCor - aCompCor (10,50)\n%         MotionOutlier - motion outliers FD &gt; 0.5, DVARS &gt; 1.5, non-steady volumes\n%         Cosine - Discrete cosine-basis regressors (low frequencies) -&gt; HPF\n%         FD - Raw framewise displacement\n%         Null - Returns a blank data frame\n%\n% Example:\n% pipeline = {'HMP-6','GS-1','FD'};\n% This selects the 6 head motion parameters, the global signal, and the raw Framewise Displacement value.\n%\n% Author: Andrea Ivan Costantino\n% Date: 5 July 2023\n\n% This section of the script defines parameters and settings necessary for a GLM analysis in SPM, using fMRI data\n% preprocessed with fMRIPrep. We first set up the required paths, select specific subjects, tasks, and contrasts, \n% and define analysis parameters like repetition time and timing information. The final block iterates through \n% subjects and tasks, setting up each run of the GLM and checking for the existence of the required data before\n% beginning the analysis. This setup ensures a consistent pipeline for different subjects and tasks.\n\nclc\nclear\n\n%% PARAMETERS\n\n% Define the space of the images ('T1w' or 'MNI'); in this case, we use MNI space at 2mm resolution.\nniftiSpace = 'MNI152NLin2009cAsym_res-2';\n\n% Define the root directory for the BIDS dataset (where the data is stored).\nBIDSRoot = '/data/projects/chess/data/BIDS';\n\n% Define paths relative to the BIDS root:\nfmriprepRoot = fullfile(BIDSRoot, 'derivatives', 'fmriprep'); % Path to fMRIprep output data.\noutRoot = fullfile(BIDSRoot, 'derivatives', 'fmriprep-SPM-TEST', niftiSpace); % Directory for storing analysis results.\ntempDir = fullfile(BIDSRoot, 'derivatives', 'fmriprep-preSPM'); % Temporary storage for intermediate files.\n\n% Define fMRI timing parameters.\nRT = 2;           % Repetition time (TR) in seconds, the time between consecutive volume acquisitions.\nfmri_t = 60;      % Microtime resolution (number of slices).\nfmri_t0 = fmri_t / 2;     % Microtime onset (reference slice for alignment).\n\n% Define the subjects and runs to include in the analysis.\nselectedSubjectsList = [1,2,3]; % List of subject IDs to include (or use '*' for all subjects).\nselectedRuns = '*'; % Runs to include, with '*' indicating all available runs.\n\n% Define tasks with contrasts and associated weights for GLM analysis.\n% Each task contains fields: name, contrasts, weights, and smoothBool.\n% Task 1: Localizer Task\nselectedTasks(1).name = 'loc'; % Name of task as used in BIDS.\nselectedTasks(1).contrasts = {'Faces &gt; Objects', 'Objects &gt; Scrambled', 'Scenes &gt; Objects'}; % Contrasts of interest.\nselectedTasks(1).weights(1) = struct('Faces', 1, 'Objects', -1, 'Scrambled', 0, 'Scenes', 0); % Weights for contrast 1.\nselectedTasks(1).weights(2) = struct('Faces', 0, 'Objects', 1, 'Scrambled', -1, 'Scenes', 0); % Weights for contrast 2.\nselectedTasks(1).weights(3) = struct('Faces', 0, 'Objects', -1, 'Scrambled', 0, 'Scenes', 1); % Weights for contrast 3.\nselectedTasks(1).smoothBool = true; % Apply smoothing before GLM for this task.\n\n% Task 2: Experimental Task\nselectedTasks(2).name = 'exp';\nselectedTasks(2).contrasts = {'Check &gt; No-Check'};\nselectedTasks(2).weights(1) = struct('check', 1, 'nocheck', -1); % Weights for contrast 1.\nselectedTasks(2).smoothBool = false; % Do not apply smoothing for this task.\n\n% Specify confound regressors to include in the GLM.\npipeline = {'HMP-6','GS-1','FD'}; % Use head motion parameters, global signal, and framewise displacement.\n\n% Find folders for each subject based on the selected subjects list.\nsub_paths = findSubjectsFolders(fmriprepRoot, selectedSubjectsList);\n\n%% RUN THE GLM FOR EACH SUBJECT AND TASK\n\n% Loop through each subject in the list.\nfor sub_path_idx = 1:length(sub_paths)\n\n    % Loop through each defined task for analysis.\n    for selected_task_idx = 1:length(selectedTasks)\n\n        % Clear previous configurations in matlabbatch to avoid conflicts.\n        clearvars matlabbatch\n\n        %% SUBJECT AND TASK INFO\n\n        % Get the current task details (name, contrasts, smoothing preference).\n        selectedTask = selectedTasks(selected_task_idx).name; % Task name.\n        contrasts = selectedTasks(selected_task_idx).contrasts; % Task contrasts.\n        smoothBool = selectedTasks(selected_task_idx).smoothBool; % Boolean: smooth images before GLM?\n\n        % Extract the subject ID from the folder name (formatted as 'sub-01').\n        subPath = sub_paths(sub_path_idx);\n        subName = subPath.name; % e.g., 'sub-01'\n        sub_id = strsplit(subName,'-'); % Splits 'sub-01' into {'sub', '01'}\n        selectedSub = str2double(sub_id{2}); % Extracts and converts the subject ID to a number.\n\n        % Define the output path for storing GLM results for this subject and task.\n        outPath = fullfile(outRoot, 'GLM', subName, selectedTask);\n\n        % Define paths to the functional data for this subject.\n        funcPathSub = fullfile(fmriprepRoot, subName, 'func'); % Functional data path.\n        bidsPathSub = fullfile(BIDSRoot, subName, 'func'); % BIDS formatted path.\n\n        % Check if the specified task exists in the subject\u2019s data.\n        files = dir(funcPathSub); % List files in the functional data directory.\n        fileNames = {files.name}; % Extract file names.\n        containsTask = any(contains(fileNames, ['task-', selectedTask])); % Check for the task in file names.\n\n        % If the task is missing for this subject, skip to the next iteration.\n        if ~containsTask\n            warning(['Task ', selectedTask, ' not found for ' subName ' in ' funcPathSub '. Skipping...']);\n            continue;\n        end\n\n        % Print a status update showing the subject and task being processed.\n        fprintf('############################### \\n# STEP: running %s - %s #\\n############################### \\n', subName, selectedTask);\n\n        % This first section finds and loads the event and confound files for each run, based on selections specified earlier. \n        % We retrieve and organize the event data (which includes onset times and durations) and confound data \n        % (like motion parameters or physiological noise) for each run of a task. These inputs are essential for setting up the \n        % GLM model, as they provide information needed for modeling the BOLD signal in relation to task events and controlling \n        % for noise.\n\n        %% FIND AND LOAD EVENTS AND CONFOUNDS FROM FMRIPREP FOLDER\n\n        % Retrieve event and confound files for selected runs.\n        if ismember('*', selectedRuns)\n            % If '*' is specified, include all available runs for this subject and task.\n            eventsTsvFiles = dir(fullfile(bidsPathSub, strcat(subName, '_task-', selectedTask, '_run-*_events.tsv')));\n            json_confounds_files = dir(fullfile(funcPathSub, strcat(subName, '_task-', selectedTask, '_run-*_desc-confounds_timeseries.json')));\n            tsv_confounds_files = dir(fullfile(funcPathSub, strcat(subName, '_task-', selectedTask, '_run-*_desc-confounds_timeseries.tsv')));\n        else\n            % Include only the specific runs listed in `selectedRuns`.\n            eventsTsvFiles = arrayfun(@(x) dir(fullfile(bidsPathSub, strcat(subName, '_task-', selectedTask, '_run-', sprintf('%01d', x), '_events.tsv'))), selectedRuns, 'UniformOutput', true);\n            json_confounds_files = arrayfun(@(x) dir(fullfile(funcPathSub, strcat(subName, '_task-', selectedTask, '_run-', sprintf('%01d', x), '_events.json'))), selectedRuns, 'UniformOutput', true);\n            tsv_confounds_files = arrayfun(@(x) dir(fullfile(funcPathSub, strcat(subName, '_task-', selectedTask, '_run-', sprintf('%01d', x), '_events.tsv'))), selectedRuns, 'UniformOutput', true);\n        end\n\n        % Sort the retrieved files alphabetically by name for consistent ordering.\n        eventsTsvFiles = table2struct(sortrows(struct2table(eventsTsvFiles), 'name'));\n        json_confounds_files = table2struct(sortrows(struct2table(json_confounds_files), 'name'));\n        tsv_confounds_files = table2struct(sortrows(struct2table(tsv_confounds_files), 'name'));\n\n        % Ensure the number of event and confound files matches across all run files.\n        assert(numel(eventsTsvFiles) == numel(json_confounds_files) &amp;&amp; numel(json_confounds_files) == numel(tsv_confounds_files), ...\n            'Mismatch in number of TSV events, TSV confounds, and JSON confounds files in %s', funcPathSub)\n\n\n        % This second section sets non-run-specific SPM model parameters, defining the core structure of the GLM model. \n        % Here we specify settings such as the repetition time, microtime resolution, basis functions, and whether global \n        % normalization should be applied. These parameters remain consistent across different runs and subjects, creating \n        % a standardized foundation for the GLM analysis.\n\n        %% SPM MODEL PARAMETERS (NON RUN DEPENDENT)\n\n        % Define non-run-specific parameters for the GLM in SPM.\n        matlabbatch{1}.spm.stats.fmri_spec.dir = cellstr(outPath); % Directory where SPM output will be saved.\n        matlabbatch{1}.spm.stats.fmri_spec.timing.units = 'secs'; % Units for event timing (set to seconds).\n        matlabbatch{1}.spm.stats.fmri_spec.timing.RT = RT; % Repetition time (TR) defines time between successive scans.\n        matlabbatch{1}.spm.stats.fmri_spec.timing.fmri_t = fmri_t; % Microtime resolution - number of time bins within one TR.\n        matlabbatch{1}.spm.stats.fmri_spec.timing.fmri_t0 = fmri_t0; % Microtime onset - reference slice for alignment.\n        matlabbatch{1}.spm.stats.fmri_spec.fact = struct('name', {}, 'levels', {}); % No factorial design applied here.\n        matlabbatch{1}.spm.stats.fmri_spec.bases.hrf.derivs = [0 0]; % Canonical HRF without derivatives (time/dispersion).\n        matlabbatch{1}.spm.stats.fmri_spec.volt = 1; % Volterra interactions disabled (linear model).\n        matlabbatch{1}.spm.stats.fmri_spec.global = 'None'; % No global normalization applied.\n        matlabbatch{1}.spm.stats.fmri_spec.mthresh = 0.8; % Masking threshold excludes voxels below 80% of mean signal.\n        matlabbatch{1}.spm.stats.fmri_spec.mask = {''}; % No explicit mask specified.\n        matlabbatch{1}.spm.stats.fmri_spec.cvi = 'AR(1)'; % AR(1) autocorrelation correction for temporal dependencies.\n\n        % Set up parameters for estimating the GLM in the second stage.\n        matlabbatch{2}.spm.stats.fmri_est.spmmat(1) = cfg_dep('fMRI model specification: SPM.mat File', ...\n            substruct('.','val', '{}',{1}, '.','val', '{}',{1}, '.','val', '{}',{1}), substruct('.','spmmat'));\n        matlabbatch{2}.spm.stats.fmri_est.write_residuals = 0; % Do not save residuals.\n        matlabbatch{2}.spm.stats.fmri_est.method.Classical = 1; % Use classical estimation method (ReML).\n\n        % This section iterates over each run of the task for the current subject, setting up the required SPM parameters \n        % for each run. We load event and confound data, identify the correct NIfTI files, apply smoothing if required, \n        % and configure session-specific settings in the SPM model specification. Each step is verified to ensure data \n        % integrity, and any issues with missing or multiple files are flagged for user review.\n\n        %% SPM RUN SETTINGS (E.G., EVENTS, CONFOUNDS, IMAGES)\n\n        % Loop over each run of the task.\n        for runIdx = 1:numel(eventsTsvFiles)\n\n            % Load event information into a structure compatible with SPM.\n            events_struct = eventsBIDS2SPM(fullfile(eventsTsvFiles(runIdx).folder, eventsTsvFiles(runIdx).name));\n            selectedRun = findRunSubstring(eventsTsvFiles(runIdx).name); % Identify the current run (e.g., 'run-01').\n\n            fprintf('## STEP: TASK %s - %s\\n', selectedTask, selectedRun)\n\n            % Select the corresponding confound files for the current run.\n            jsonRows = filterRowsBySubstring(json_confounds_files, selectedRun);\n            confoundsRows = filterRowsBySubstring(tsv_confounds_files, selectedRun);\n\n            % Check for a single JSON confounds file, and flag errors if multiple or none are found.\n            if length(jsonRows) ~= 1\n                error('More than one JSON file found for the specified run. Please check the dataset.');\n            elseif isempty(jsonRows)\n                error('No JSON file found for the specified run. Please check the dataset.');\n            else\n                jsonRow = jsonRows{1, :};\n            end\n\n            % Check for a single TSV confounds file, and handle errors similarly.\n            if length(confoundsRows) ~= 1\n                error('More than one TSV confounds file found for the specified run. Please check the dataset.');\n            elseif isempty(confoundsRows)\n                error('No TSV confounds file found for the specified run. Please check the dataset.');\n            else\n                confoundsRow = confoundsRows{1, :};\n            end\n\n            % Extract confound data based on the specified pipeline, loading it into an array.\n            confounds_array = fMRIprepConfounds2SPM(fullfile(jsonRow.folder, jsonRow.name),...\n                fullfile(confoundsRow.folder, confoundsRow.name), pipeline);\n\n            % Define the pattern to locate the NIfTI file and check for its existence.\n            filePattern = strcat(subName, '_task-', selectedTask, '_', selectedRun, '_space-', niftiSpace, '_desc-preproc_bold');\n            niiFileStruct = dir(fullfile(fmriprepRoot, subName, 'func', strcat(filePattern, '.nii')));\n\n            % Handle cases with missing or multiple NIfTI files.\n            if size(niiFileStruct, 1) &gt; 1\n                error('Multiple NIFTI files found for %s.', selectedRun)\n            elseif isempty(niiFileStruct)\n                % If no NIfTI files are found, check for compressed (.nii.gz) files and decompress if necessary.\n                niiGzFilePattern = fullfile(funcPathSub, strcat(filePattern, '.nii.gz'));\n                niiGzFileStruct = dir(niiGzFilePattern);\n\n                if isempty(niiGzFileStruct)\n                    warning('No NIFTI file found for run %s. SKIPPING!', selectedRun)\n                    continue\n                elseif size(niiGzFileStruct, 1) &gt; 1\n                    error('Multiple NIFTI.GZ files found for this run.')\n                else\n                    % Decompress the .nii.gz file if found.\n                    niiGzFileString = fullfile(niiGzFileStruct.folder, niiGzFileStruct.name);\n                    gunzippedNii = gunzipNiftiFile(niiGzFileString, fullfile(tempDir, 'gunzipped', subName));\n                    niiFileStruct = dir(gunzippedNii{1});\n                end\n            end\n\n            % Get the full path of the (possibly decompressed) NIfTI file.\n            niiFileString = fullfile(niiFileStruct.folder, niiFileStruct.name);\n\n            % Smooth the NIfTI file if smoothing is enabled for this task.\n            if smoothBool\n                niiFileString = smoothNiftiFile(niiFileString, fullfile(tempDir, 'smoothed', subName));\n            else\n                fprintf('SMOOTH: smoothBool set to false for this task. Skipping...\\n')\n            end\n\n            % Specify functional images in SPM model specification for the current run.\n            niiFileCell = {niiFileString};\n            matlabbatch{1}.spm.stats.fmri_spec.sess(runIdx).scans = spm_select('expand', niiFileCell); % Expanded list of images.\n\n            % Set high-pass filter (HPF) to a value higher than the run duration to avoid filtering.\n            matlabbatch{1}.spm.stats.fmri_spec.sess(runIdx).hpf = (matlabbatch{1}.spm.stats.fmri_spec.timing.RT * ...\n                size(matlabbatch{1}.spm.stats.fmri_spec.sess(runIdx).scans, 1)) + 100;\n\n            % Add conditions (events) to the model for the current run.\n            for cond_id = 1:length(events_struct.names)\n                matlabbatch{1}.spm.stats.fmri_spec.sess(runIdx).cond(cond_id).name = events_struct.names{cond_id};\n                matlabbatch{1}.spm.stats.fmri_spec.sess(runIdx).cond(cond_id).onset = events_struct.onsets{cond_id};\n                matlabbatch{1}.spm.stats.fmri_spec.sess(runIdx).cond(cond_id).duration = events_struct.durations{cond_id};\n            end\n\n            % Add confound regressors to the model for the current run.\n            for reg_id = 1:size(confounds_array, 2)\n                matlabbatch{1}.spm.stats.fmri_spec.sess(runIdx).regress(reg_id).name = confounds_array.Properties.VariableNames{reg_id};\n                matlabbatch{1}.spm.stats.fmri_spec.sess(runIdx).regress(reg_id).val = confounds_array{:, reg_id};\n            end\n        end\n\n\n        % This final section initializes the SPM defaults, executes the GLM model specification and estimation,\n        % verifies that the model ran successfully by checking for the SPM.mat file, and then defines and\n        % applies contrasts. Finally, the script is copied to the output folder for replicability. Each step\n        % is verified with status messages, providing clear feedback for the user.\n\n        %% RUN BATCHES 1 AND 2\n\n        % Initialize SPM defaults for fMRI analysis.\n        spm('defaults','fmri'); % Load SPM defaults for fMRI.\n        spm_jobman('initcfg');  % Initialize the SPM job configuration.\n\n        % Execute the model specification (batch 1) and model estimation (batch 2).\n        fprintf('GLM: Running GLM for: %s - TASK:  %s\\n', subName, selectedTask)\n        spm_jobman('run', matlabbatch(1:2));\n        fprintf('GLM: DONE!\\n')\n\n        %% FIND SPM.mat\n\n        % After running the GLM, check for the existence of the SPM.mat file in the output path.\n        % The SPM.mat file is essential for contrast specification.\n        spmMatPath = fullfile(outPath, 'SPM.mat');\n        if ~exist(spmMatPath, 'file')\n            error('SPM.mat file not found in the specified output directory.');\n        end\n\n\n        %% Save Boxcar plot and design matrix of estimated model\n        SPMstruct = load(spmMatPath);\n\n        plotBoxcarAndHRFResponses(SPMstruct, outPath)\n        saveSPMDesignMatrix(SPMstruct, outPath)\n\n        %% RUN BATCH 3 (CONTRASTS)\n\n        % Define the path to the SPM.mat file in the batch structure for contrast estimation.\n        matlabbatch{3}.spm.stats.con.spmmat(1) = {spmMatPath};\n\n        % Loop through each contrast defined for the current task.\n        for k = 1:length(contrasts)\n            % Adjust contrast weights to fit the current design matrix structure.\n            weights = adjust_contrasts(spmMatPath, selectedTasks(selected_task_idx).weights(k));\n\n            % Define each contrast and its properties in the SPM batch structure.\n            matlabbatch{3}.spm.stats.con.consess{k}.tcon.weights = weights; % Contrast weights.\n            matlabbatch{3}.spm.stats.con.consess{k}.tcon.name = contrasts{k}; % Name of the contrast.\n            matlabbatch{3}.spm.stats.con.consess{k}.tcon.sessrep = 'none'; % No session-specific replication.\n        end\n\n% Execute the contrast batch (batch 3).\nfprintf('Setting contrasts...\\n');\nspm_jobman('run', matlabbatch(3));\nfprintf('DONE!\\n');\n\n%% Save Contrast Plots\n% Thresholds for statistical analysis\nthresholds = {\n    0.001, ...\n    0.01, ...\n    0.05 ...\n    };\n\n% Iterate over contrasts to generate plots\nfor constrastIdx = 1:length(selectedTasks(taskIndex).contrasts)\n\n    % Iterate over thresholds to generate plots\n    for thresholdIndex = 1:length(thresholds)\n\n        % Generate and Save Contrast Overlay Images\n\n        % Set crosshair coordinates (modify if needed)\n        crossCoords = [40, -52, -18]; % FFA. Change as needed.\n\n        % Set the index of the contrast to display (modify if needed)\n        spmContrastIndex = constrastIdx;\n\n        % Call the function to generate and save contrast overlay images\n        generateContrastOverlayImages(spmMatPath, outPath, fmriprepRoot, subjectName, pipelineStr, thresholds{thresholdIndex}, spmContrastIndex, crossCoords);\n\n    end\nend\nend\n%% SAVE SCRIPT\n\n% Save a copy of this script in the output directory for documentation and replicability.\nFileNameAndLocation = [mfilename('fullpath')]; % Get full path to this script.\nscript_outdir = fullfile(outPath,'spmGLMautoContrast.m'); % Set destination path for script copy.\ncurrentfile = strcat(FileNameAndLocation, '.m'); % Get filename with extension.\ncopyfile(currentfile, script_outdir); % Copy script to output directory.\nend\n</code></pre> generateContrastOverlayImages.m <pre><code>function generateContrastOverlayImages(spmMatPath, outputPath, fmriprepRoot, subjectName, pipelineStr, thresholdValue, spmContrastIndex, crossCoords)\n% GENERATECONTRASTOVERLAYIMAGES Generates and saves contrast overlay images for specified thresholds\n%\n% This function loads the SPM.mat file, sets up the xSPM structure for the specified contrast,\n% and generates overlay images on the anatomical image. The overlay images are saved to the output directory.\n%\n% Inputs:\n%   spmMatPath        - String, path to the SPM.mat file\n%   outputPath        - String, output directory where images will be saved\n%   fmriprepRoot      - String, root directory of fmriprep outputs\n%   subjectName       - String, subject identifier (e.g., 'sub-01')\n%   pipelineStr       - String, representation of the denoising pipeline\n%   thresholdValue         - Numeric threshold to use for generating images\n%   spmContrastIndex  - Integer, index of the contrast in SPM.xCon to use (default is 1)\n%   crossCoords       - Vector [x, y, z], coordinates to set the crosshair (default is [40, -52, -18])\n%\n% Outputs:\n%   None (overlay images are saved to the output directory)\n%\n% Usage:\n%   generateContrastOverlayImages(spmMatPath, outputPath, fmriprepRoot, subjectName, pipelineStr, thresholds);\n%\n% Notes:\n%   - The function assumes that SPM and SPM12 toolboxes are properly set up.\n%   - The function handles any errors during the generation of xSPM and provides informative messages.\n%\n% Example:\n%   generateContrastOverlayImages('/path/to/SPM.mat', '/output/dir', '/fmriprep/root', 'sub-01', 'GS-1_HMP-6', {0.001, 0.01}, 1, [40, -52, -18]);\n\nif nargin &lt; 8\n    crossCoords = [40, -52, -18]; % Default crosshair coordinates\nend\nif nargin &lt; 7\n    spmContrastIndex = 1; % Default contrast index\nend\n\n% Load SPM.mat to access contrast data\nfprintf('Loading SPM.mat from %s to process contrasts...\\n', spmMatPath);\nload(spmMatPath, 'SPM');\n\n% Verify the contrast index is valid\nif spmContrastIndex &gt; numel(SPM.xCon)\n    error('Invalid contrast index. Ensure the index is within the range of defined contrasts.');\nend\n\n% Get the contrast name from SPM.xCon\ncontrastNameSPM = SPM.xCon(spmContrastIndex).name;\n\n% Iterate over thresholds to generate and save images\n% Prepare xSPM structure for results\ncontrastName = sprintf('%s_%s_%g_%s', subjectName, pipelineStr, thresholdValue, contrastNameSPM);\nxSPM = struct();\nxSPM.swd = outputPath; % Directory where SPM.mat is saved\nxSPM.title = contrastName;\nxSPM.Ic = spmContrastIndex; % Contrast index\nxSPM.Im = []; % Mask (empty means no mask)\nxSPM.pm = []; % P-value masking\nxSPM.Ex = []; % Mask exclusion\nxSPM.u = thresholdValue; % Threshold (uncorrected p-value)\nxSPM.k = 0; % Extent threshold (number of voxels)\nxSPM.STAT = 'T'; % Use T-statistics\nxSPM.thresDesc = 'none'; % No threshold description\n\n% Generate results without GUI\n[SPM, xSPM] = spm_getSPM(xSPM);\n\nxSPM.thresDesc = 'none'; % No threshold description\n\n% Display results\n[hReg, xSPM] = spm_results_ui('setup', xSPM);\n\n% Set crosshair coordinates\nspm_results_ui('SetCoords', crossCoords);\n\n% Overlay activations on anatomical image\nsectionImgPath = fullfile(fmriprepRoot, subjectName, 'anat', [subjectName, '_space-MNIPediatricAsym_cohort-1_res-2_desc-preproc_T1w.nii']);\nif exist(sectionImgPath, 'file')\n    fprintf('Overlaying activations for threshold %g...\\n', thresholdValue);\n    spm_sections(xSPM, hReg, sectionImgPath);\n\n    % Save the overlay image\n    overlayImgPath = fullfile(outputPath, sprintf('%s.png', contrastName));\n    spm_figure('GetWin', 'Graphics');\n    print('-dpng', overlayImgPath);\n    fprintf('Overlay saved as %s\\n', overlayImgPath);\nelse\n    warning('Anatomical image not found at %s. Skipping overlay.', sectionImgPath);\nend\n\n% Close graphics window\nspm_figure('Close', 'Graphics');\nend\n</code></pre> plotBoxcarAndHRFResponses.m <pre><code>function plotBoxcarAndHRFResponses(SPMstruct, outDir)\n% plotBoxcarAndHRFResponses Visualize boxcar functions and convolved HRF responses per condition and session.\n%\n% This function generates a comprehensive visualization of the boxcar functions\n% and their corresponding convolved hemodynamic response functions (HRFs) for\n% each condition across all sessions, as defined in the SPM.mat structure.\n%\n% Usage:\n%   plotBoxcarAndHRFResponses(SPM);\n%   plotBoxcarAndHRFResponses(SPM, outDir);\n%\n% Inputs:\n%   - SPM: A struct loaded from an SPM.mat file containing experimental design\n%          and statistical model parameters.\n%   - outDir: (Optional) A string specifying the directory to save the plot. If\n%             provided, the plot is saved as a PNG file in the specified directory.\n%\n% Output:\n%   - A figure is displayed with subplots representing each condition (row)\n%     and session (column). Each subplot contains the boxcar function and the\n%     convolved HRF for the corresponding condition and session.\n%\n% Example:\n%   % Load the SPM.mat file\n%   load('SPM.mat');\n%\n%   % Call the function to visualize\n%   plotBoxcarAndHRFResponses(SPM);\n%\n%   % Save the plot to a directory\n%   plotBoxcarAndHRFResponses(SPM, 'output_directory/');\n%\n% Notes:\n%   - This function assumes that the SPM structure contains the following:\n%     *SPM.Sess: Session-specific condition information.\n%* SPM.xY.RT: Repetition time (TR) in seconds.\n%     *SPM.nscan: Number of scans per session.\n%* SPM.xX.X: Design matrix containing the convolved regressors.\n%     * SPM.xX.name: Names of the columns in the design matrix.\n%   - Ensure that the SPM.mat file corresponds to your specific fMRI data analysis.\n%\n\n% Get the number of sessions\nSPM = SPMstruct.SPM;\nnum_sessions = length(SPM.Sess);\n\n% Get the repetition time (TR)\nTR = SPM.xY.RT;\n\n% Get the number of scans per session\nnscans = SPM.nscan;\n\n% Calculate the cumulative number of scans to determine session boundaries\nsession_boundaries = [0 cumsum(nscans)];\n\n% Determine the maximum number of conditions across all sessions\nmax_num_conditions = max(arrayfun(@(x) length(x.U), SPM.Sess));\n\n% Create a new figure for plotting\nfigure;\n\n% Adjust the figure size for better visibility\nset(gcf, 'Position', [100, 100, 1400, 800]);\n\n% Initialize subplot index\nsubplot_idx = 1;\n\n% Define line styles and colors for boxcar and convolved HRF\nboxcar_line_style = '-';\nboxcar_line_color = [0, 0.4470, 0.7410]; % MATLAB default blue\nboxcar_line_width = 1.5;\n\nhrf_line_style = '-';\nhrf_line_color = [0.8500, 0.3250, 0.0980]; % MATLAB default red\nhrf_line_width = 1.5;\n\n% Loop over each condition (regressor)\nfor cond_idx = 1:max_num_conditions\n    % Loop over each session\n    for sess_idx = 1:num_sessions\n        % Create a subplot for the current condition and session\n        subplot(max_num_conditions, num_sessions, subplot_idx);\n\n        % Check if the current session has the current condition\n        if length(SPM.Sess(sess_idx).U) &gt;= cond_idx\n            % Extract the condition structure\n            U = SPM.Sess(sess_idx).U(cond_idx);\n\n            % Get the condition name\n            condition_name = U.name{1};\n\n            % Get the onsets and durations of the events\n            onsets = U.ons;\n            durations = U.dur;\n\n            % Get the number of scans (time points) in the current session\n            num_scans = nscans(sess_idx);\n\n            % Create the time vector for the current session\n            time_vector = (0:num_scans - 1) * TR;\n\n            % Initialize the boxcar function for the current session\n            boxcar = zeros(1, num_scans);\n\n            % Build the boxcar function based on onsets and durations\n            for i = 1:length(onsets)\n                onset_idx = floor(onsets(i) / TR) + 1;\n                offset_idx = ceil((onsets(i) + durations(i)) / TR);\n                onset_idx = max(onset_idx, 1);\n                offset_idx = min(offset_idx, num_scans);\n                boxcar(onset_idx:offset_idx) = 1;\n            end\n\n            % Find the rows corresponding to the current session in the design matrix\n            session_row_start = session_boundaries(sess_idx) + 1;\n            session_row_end = session_boundaries(sess_idx + 1);\n            session_rows = session_row_start:session_row_end;\n\n            % Find the columns in the design matrix corresponding to the current condition\n            prefix = sprintf('Sn(%d) %s', sess_idx, condition_name);\n            column_indices = find(strncmp(SPM.xX.name, prefix, length(prefix)));\n\n            % Extract the convolved regressor(s) for the current condition and session\n            convolved_regressor = sum(SPM.xX.X(session_rows, column_indices), 2);\n\n            % Plot the boxcar function\n            plot(time_vector, boxcar, 'LineStyle', boxcar_line_style, 'Color', boxcar_line_color, 'LineWidth', boxcar_line_width);\n            hold on;\n\n            % Plot the convolved HRF response\n            plot(time_vector, convolved_regressor, 'LineStyle', hrf_line_style, 'Color', hrf_line_color, 'LineWidth', hrf_line_width);\n            hold off;\n\n            % Improve the appearance of the plot\n            grid on;\n            xlim([0, max(time_vector)]);\n            ylim_min = min(min(boxcar), min(convolved_regressor)) - 0.1;\n            ylim_max = max(max(boxcar), max(convolved_regressor)) + 0.1;\n            ylim([ylim_min, ylim_max]);\n            set(gca, 'FontSize', 8);\n\n            % Add condition names as y-labels on the first column\n            if sess_idx == 1\n                ylabel(condition_name, 'FontSize', 10, 'Interpreter', 'none');\n            else\n                set(gca, 'YTick', []);\n                set(gca, 'YTickLabel', []);\n            end\n\n            % Add x-labels on the bottom row\n            if cond_idx == max_num_conditions\n                xlabel('Time (s)', 'FontSize', 10);\n            else\n                set(gca, 'XTick', []);\n                set(gca, 'XTickLabel', []);\n            end\n\n            % Add session titles on the first row\n            if cond_idx == 1\n                title(sprintf('Session %d', sess_idx), 'FontSize', 12);\n            end\n        else\n            % If the condition is not present in the session\n            axis off;\n            text(0.5, 0.5, 'Not Present', 'HorizontalAlignment', 'center', 'FontSize', 12);\n\n            % Add condition names as y-labels on the first column\n            if sess_idx == 1\n                ylabel(['Condition: ' num2str(cond_idx)], 'FontSize', 10, 'Interpreter', 'none');\n            end\n\n            % Add session titles on the first row\n            if cond_idx == 1\n                title(sprintf('Session %d', sess_idx), 'FontSize', 12);\n            end\n        end\n\n        % Increment the subplot index\n        subplot_idx = subplot_idx + 1;\n    end\nend\n\n% Add an overall title for the figure\nsgtitle('Boxcar and Convolved HRF Responses per Condition and Session', 'FontSize', 16);\n\n% Save the plot as PNG if outDir is specified\nif nargin &gt; 1 &amp;&amp; ~isempty(outDir)\n    if ~isfolder(outDir)\n        mkdir(outDir); % Create the directory if it doesn't exist\n    end\n    % Create a file name based on the current date and time\n    timestamp = datestr(now, 'yyyy-mm-dd_HH-MM-SS');\n    file_name = fullfile(outDir, ['BoxcarHRFResponses_' timestamp '.png']);\n    saveas(gcf, file_name);\n    fprintf('Figure saved to: %s\\n', file_name);\nend\nclose(gcf);\n\nend\n</code></pre> saveSPMDesignMatrix.m <pre><code>function saveSPMDesignMatrix(SPMstruct, outDir)\n% saveSPMDesignMatrix Visualize and optionally save the design matrix from SPM.\n%\n% This function uses SPM's internal `spm_DesRep` function to display the design\n% matrix and optionally saves the resulting figure as a PNG file.\n%\n% Usage:\n%   saveSPMDesignMatrix(SPMstruct);\n%   saveSPMDesignMatrix(SPMstruct, outDir);\n%\n% Inputs:\n%   - SPMstruct: A struct loaded from an SPM.mat file containing experimental\n%                design and statistical model parameters.\n%   - outDir: (Optional) A string specifying the directory to save the figure.\n%             If provided, the design matrix is saved as a PNG file in the specified\n%             directory.\n%\n% Output:\n%   - A figure is displayed showing the design matrix as produced by SPM.\n%   - If `outDir` is provided, the design matrix is saved as a PNG file in the\n%     specified directory.\n%\n% Example:\n%   % Load the SPM.mat file\n%   load('SPM.mat');\n%\n%   % Display the design matrix\n%   saveSPMDesignMatrix(SPMstruct);\n%\n%   % Save the design matrix to a directory\n%   saveSPMDesignMatrix(SPMstruct, 'output_directory/');\n%\n% Notes:\n%   - Ensure that the SPM.mat file corresponds to your specific fMRI data analysis.\n%   - This function depends on the SPM toolbox being properly set up and initialized.\n%\n\nSPM = SPMstruct.SPM;\n\n% Check if the design matrix exists\nif ~isfield(SPM, 'xX') || ~isfield(SPM.xX, 'X') || isempty(SPM.xX.X)\n    error('The SPM structure does not contain a valid design matrix.');\nend\n\n% Use SPM's spm_DesRep function to display the design matrix\nspm_DesRep('DesMtx', SPM.xX);\n\n% Get the current figure handle (SPM's design matrix figure)\nfigHandle = gcf;\n\n% Save the figure as a PNG if outDir is specified\nif nargin &gt; 1 &amp;&amp; ~isempty(outDir)\n    if ~isfolder(outDir)\n        mkdir(outDir); % Create the directory if it doesn't exist\n    end\n    % Create a file name based on the current date and time\n    timestamp = datestr(now, 'yyyy-mm-dd_HH-MM-SS');\n    file_name = fullfile(outDir, ['SPMDesignMatrix_' timestamp '.png']);\n    saveas(figHandle, file_name);\n    fprintf('Design matrix saved to: %s\\n', file_name);\nend\n\n% Close the figure after saving\nclose(figHandle);\n\nend\n</code></pre> findRunSubstring.m <pre><code>function runSubstring = findRunSubstring(inputStr)\n%FINDRUNSUBSTRING Extracts a 'run-xx' substring from a given string\n%   This function takes an input string and searches for a substring that\n%   matches the pattern 'run-xx', where 'xx' can be any one or two digit number.\n%   If such a substring is found, it is returned; otherwise, an empty string\n%   is returned.\n\n% Regular expression to match 'run-' followed by one or two digits\npattern = 'run-\\d{1,2}';\n\n% Search for the pattern in the input string\nmatches = regexp(inputStr, pattern, 'match');\n\n% Check if any match was found\nif ~isempty(matches)\n    % If a match was found, return the first match\n    runSubstring = matches{1};\nelse\n    % If no match was found, return an empty string\n    runSubstring = '';\nend\nend\n</code></pre> filterRowsBySubstring.m <pre><code>function filteredRows = filterRowsBySubstring(data, substring)\n%FILTERROWSBYSUBSTRING Filters rows based on a substring in the first column\n%   This function takes a cell array 'data' and a 'substring' as inputs,\n%   and returns a new cell array 'filteredRows' containing only the rows\n%   from 'data' where the first column includes the specified 'substring'.\n\n% Initialize an empty cell array to store the filtered rows\nfilteredRows = {};\n\n% Iterate through each row in the data\nfor rowIndex = 1:size(data, 1)\n    % Fetch the first column of the current row\n    currentEntry = data(rowIndex).name;\n\n    % Check if the first column contains the specified substring\n    if contains(currentEntry, ['_' substring '_'])\n        % If it does, add the current row to the filteredRows array\n        filteredRows = [filteredRows; data(rowIndex, :)];\n    end\nend\nend\n</code></pre> adjust_contrasts.m <pre><code>function weight_vector = adjust_contrasts(spmMatPath, contrastWeights)\n% ADJUST_CONTRASTS Adjust contrast weights according to the design matrix in SPM.\n%\n% DESCRIPTION:\n% This function adjusts the specified contrast weights according to the design\n% matrix in SPM, and provides a visual representation of the weights applied to\n% the design matrix.\n%\n% INPUTS:\n% spmMatPath: String\n%   - Path to the SPM.mat file.\n%     Example: '/path/to/SPM.mat'\n%\n% contrastWeights: Struct\n%   - Specifies the weight of each condition in the contrast.\n%     For wildcard specification, use '*WILDCARD*'. E.g., 'condition_WILDCARD_': weight\n%     Example: struct('condition1', 1, 'condition2_WILDCARD_', -1)\n%\n% OUTPUTS:\n% weight_vector: Numeric Vector\n%   - A vector of weights for each regressor.\n%     Example: [0, 1, -1, 0, ...]\n%\n% The function also generates a visual representation of the design matrix with\n% the specified contrast weights.\n\n% Load the SPM.mat\nload(spmMatPath);\n% Extracting regressor names from the SPM structure\nregressor_names = SPM.xX.name;\n\n% Generate weight vector based on SPMs design matrix and specified weights for the single contrast\nweight_vector = generate_weight_vector_from_spm(contrastWeights, regressor_names);\n\n% % Plotting for visual verification\n% figure;\n% \n% % Display the design matrix\n% imagesc(SPM.xX.X);  % Display the design matrix\n% colormap('gray');   % Set base colormap to gray for design matrix\n% hold on;\n% \n% % Create a color overlay based on the weights\n% for i = 1:length(weight_vector)\n%     x = [i-0.5, i+0.5, i+0.5, i-0.5];\n%     y = [0.5, 0.5, length(SPM.xX.X) + 0.5, length(SPM.xX.X) + 0.5];\n%     if weight_vector(i) &gt; 0\n%         % Green for positive weights\n%         color = [0, weight_vector(i), 0];  % Green intensity based on weight value\n%         patch(x, y, color, 'EdgeColor', 'none', 'FaceAlpha', 0.3);  % Reduced transparency\n%     elseif weight_vector(i) &lt; 0\n%         % Red for negative weights\n%         color = [abs(weight_vector(i)), 0, 0];  % Red intensity based on absolute weight value\n%         patch(x, y, color, 'EdgeColor', 'none', 'FaceAlpha', 0.3);  % Reduced transparency\n%     end\n% end\n% \n% % Annotate with regressor names\n% xticks(1:length(regressor_names));\n% xticklabels('');  % Initially empty, to be replaced by colored text objects\n% xtickangle(45);  % Angle the text so it doesnt overlap\n% set(gca, 'TickLabelInterpreter', 'none');  % Ensure special characters in regressor names display correctly\n% \n% % Color code the regressor names using text objects\n% for i = 1:length(regressor_names)\n%     if weight_vector(i) &gt; 0\n%         textColor = [0, 0.6, 0];\n%     elseif weight_vector(i) &lt; 0\n%         textColor = [0.6, 0, 0];\n%     else\n%         textColor = [0, 0, 0];\n%     end\n%     text(i, length(SPM.xX.X) + 5, regressor_names{i}, 'Color', textColor, 'Rotation', 45, 'Interpreter', 'none', 'HorizontalAlignment', 'right', 'VerticalAlignment', 'bottom');\n% end\n% \n% title('Design Matrix with Contrast Weights');\n% xlabel('');\n% ylabel('Scans');\n% \n% % Add legends\n% legend({'Positive Weights', 'Negative Weights'}, 'Location', 'northoutside');\n% \n% % Optional: Add a dual color colorbar to represent positive and negative weight intensities\n% colorbar('Ticks', [-1, 0, 1], 'TickLabels', {'-Max Weight', '0', '+Max Weight'}, 'Direction', 'reverse');\n% \n% hold off;\nend\n</code></pre> generate_weight_vector_from_spm.m <pre><code>function weight_vector = generate_weight_vector_from_spm(contrastWeights, regressor_names)\n% GENERATE_WEIGHT_VECTOR_FROM_SPM Generates a weight vector from the SPM design matrix.\n%\n% This function constructs a weight vector based on the design matrix in SPM\n% and the user-specified contrast weights. Its equipped to handle wildcard matches\n% in condition names for flexibility in defining contrasts.\n%\n% USAGE:\n%   weight_vector = generate_weight_vector_from_spm(contrastWeights, regressor_names)\n%\n% INPUTS:\n%   contrastWeights : struct\n%       A struct specifying the weight of each condition in the contrast.\n%       Fields of the struct are condition names and the associated values are the contrast weights.\n%       Use '*WILDCARD*' in the condition name to denote a wildcard match.\n%       Example:\n%           contrastWeights = struct('Faces', 1, 'Objects_WILDCARD_', -1);\n%\n%   regressor_names : cell array of strings\n%       Names of the regressors extracted from the SPM.mat structure.\n%       Typically includes task conditions and confound regressors.\n%       Example:\n%           {'Sn(1) Faces*bf(1)', 'Sn(1) Objects*bf(1)', 'Sn(1) trans_x', ...}\n%\n% OUTPUTS:\n%   weight_vector : numeric vector\n%       A vector of weights for each regressor in the order they appear in the regressor_names.\n%       Example:\n%           [1, -1, 0, ...]\n%\n% Notes:\n%   This function assumes that task-related regressors in the SPM design matrix end with \"*bf(1)\".\n%   Confound regressors (e.g., motion parameters) do not have this suffix.\n\n% Initialize a weight vector of zeros\nweight_vector = zeros(1, length(regressor_names));\n\n% Extract field names from the contrastWeights structure\nfields = fieldnames(contrastWeights);\n\n% Iterate over the field names to match with regressor names\nfor i = 1:length(fields)\n    field = fields{i};\n\n    % If the field contains a wildcard, handle it\n    if contains(field, '_WILDCARD_')\n        % Convert the wildcard pattern to a regular expression pattern\n        pattern = ['Sn\\(\\d{1,2}\\) ' strrep(field, '_WILDCARD_', '.*')];\n\n        % Find indices of matching regressors using the regular expression pattern\n        idx = find(~cellfun('isempty', regexp(regressor_names, pattern)));\n\n        % Assign the weight from contrastWeights to the matching regressors\n        weight_vector(idx) = contrastWeights.(field);\n    else\n        % No need to extract the condition name, just append *bf(1) to match the SPM regressor pattern\n        pattern = ['Sn\\(\\d{1,2}\\) ' field];\n\n        idx = find(~cellfun('isempty', regexp(regressor_names, pattern)));\n\n        % Assign the weight from contrastWeights to the regressor\n        if ~isempty(idx)\n            weight_vector(idx) = contrastWeights.(field);\n        end\n    end\nend\nend\n</code></pre> eventsBIDS2SPM.m <pre><code>function new_df = eventsBIDS2SPM(tsv_file)\n    % eventsBIDS2SPM - Convert BIDS event files to SPM format\n    % This function reads a BIDS event file and converts it to the format required by SPM.\n    % It extracts the unique trial types and their onsets and durations and stores them in a\n    % Matlab structure.\n    %\n    % Author: Andrea Costantino\n    % Date: 23/1/2023\n    %\n    % Usage:\n    %   mat_dict = eventsBIDS2SPM(tsv_file, run_id)\n    %\n    % Inputs:\n    %   tsv_file - string, path to the tsv file containing the events\n    %\n    % Outputs:\n    %   mat_dict - struct, a Matlab structure containing the events in the format\n    %              required by SPM. The structure contains three fields:\n    %                - 'names': cell array of string, the names of the trial types\n    %                - 'onsets': cell array of double, onset times of the trials\n    %                - 'durations': cell array of double, duration of the trials\n    %\n    % This function reads a BIDS event file and converts it to the format required by SPM.\n    % It extracts the unique trial types and their onsets and durations and stores them in a\n    % Matlab structure\n\n    % read the tsv file\n    df = readtable(tsv_file,'FileType','text');\n    % Select unique trial type name\n    unique_names = unique(df.trial_type);\n    % Make new table in a form that SPM can read\n    new_df = table('Size',[length(unique_names),3],'VariableTypes',{'cellstr', 'cellstr', 'cellstr'},'VariableNames',{'names', 'onsets', 'durations'});\n    % For each trial type (i.e., condition)\n    for k = 1:length(unique_names)\n        % Select rows belonging to that condition\n        filtered = df(strcmp(df.trial_type,unique_names{k}),:);\n        % Copy trial name, onset and duration to the new table\n        new_df.names(k) = unique(filtered.trial_type);\n        new_df.onsets(k) = {filtered.onset};\n        new_df.durations(k) = {filtered.duration};\n    end\n    new_df = sortrows(new_df, 'names');\nend\n</code></pre> findSubjectsFolders.m <pre><code>function [filteredFolderStructure] = findSubjectsFolders(fmriprepRoot, selectedSubjectsList, excludedSubjectsList)\n% FINDSUBJECTSFOLDERS Locate subject folders based on a list or wildcard.\n%\n% USAGE:\n% sub_paths = findSubjectsFolders(fmriprepRoot, selectedSubjectsList)\n%\n% INPUTS:\n% fmriprepRoot          - The root directory where 'sub-*' folders are located.\n%\n% selectedSubjectsList  - Can be one of two things:\n%                         1) A list of integers, each representing a subject ID.\n%                            For example, [7,9] would search for folders 'sub-07'\n%                            and 'sub-09' respectively.\n%                         2) A single character string '*'. In this case, the function\n%                            will return all folders starting with 'sub-*'.\n%\n% OUTPUTS:\n% sub_paths             - A structure array corresponding to the found directories.\n%                         Each structure has fields: 'name', 'folder', 'date',\n%                         'bytes', 'isdir', and 'datenum'.\n%\n% EXAMPLES:\n% 1) To fetch directories for specific subjects:\n%    sub_paths = findSubjectsFolders('/path/to/fmriprepRoot', [7,9]);\n%\n% 2) To fetch all directories starting with 'sub-*':\n%    sub_paths = findSubjectsFolders('/path/to/fmriprepRoot', '*');\n%\n% Notes:\n% If a subject ID from the list does not match any directory, a warning is issued.\n\n% Start by fetching all directories with the 'sub-*' pattern.\nsub_paths = dir(fullfile(fmriprepRoot, 'sub-*'));\nsub_paths = sub_paths([sub_paths.isdir]); % Keep only directories.\n\n% Check the type of selectedSubjectsList\nif isnumeric(selectedSubjectsList(1))\n    % Case 1: selectedSubjectsList is a list of integers.\n\n    % Convert each integer in the list to a string of the form 'sub-XX'.\n    subIDs = cellfun(@(x) sprintf('sub-%02d', x), num2cell(selectedSubjectsList), 'UniformOutput', false);\n\n    % Filter the sub_paths to keep only those directories matching the subIDs.\n    sub_paths = sub_paths(ismember({sub_paths.name}, subIDs));\n\n    % Check and throw warnings for any missing subID.\n    foundSubIDs = {sub_paths.name};\n    for i = 1:length(subIDs)\n        if ~ismember(subIDs{i}, foundSubIDs)\n            warning(['The subID ', subIDs{i}, ' was not found in sub_paths.name.']);\n        end\n    end\n\nelseif ischar(selectedSubjectsList) &amp;&amp; strcmp(selectedSubjectsList, '*')\n    % Case 2: selectedSubjectsList is '*'. \n    % No further action required as we have already selected all 'sub-*' folders.\n\nelse\n    % Invalid input.\n    error('Invalid format for selectedSubjects. It should be either \"*\" or a list of integers.');\nend\n\n% Only process exclusion if the excludedSubjectsList is provided.\nif nargin == 3\n    % Create a list of excluded folder names\n    excludedNames = cellfun(@(x) sprintf('sub-%02d', x), num2cell(excludedSubjectsList), 'UniformOutput', false);\n\n    % Logical array of folders to exclude\n    excludeMask = arrayfun(@(x) ismember(x.name, excludedNames), sub_paths);\n\n    % Filtered structure\n    filteredFolderStructure = sub_paths(~excludeMask);\nelse\n    % If no excludedSubjectsList is provided, just return the sub_paths.\n    filteredFolderStructure = sub_paths;\nend\nend\n</code></pre> fMRIprepConfounds2SPM.m <pre><code>function confounds = fMRIprepConfounds2SPM(json_path, tsv_path, pipeline)\n% fMRIprepConfounds2SPM - Extracts and formats fMRI confounds for SPM analysis\n%\n% This function processes confound data from fMRIprep outputs, suitable for\n% Statistical Parametric Mapping (SPM) analysis. It reads a JSON file with\n% confound descriptions and a TSV file with confound values, then selects and\n% formats the required confounds based on the specified denoising pipeline.\n%\n% Usage:\n%   confounds = fMRIprepConfounds2SPM(json_path, tsv_path, pipeline)\n%\n% Inputs:\n%   json_path (string): Full path to the JSON file. This file contains metadata\n%                       about the confounds, such as their names and properties.\n%\n%   tsv_path (string):  Full path to the TSV file. This file holds the actual\n%                       confound values in a tabular format for each fMRI run.\n%\n%   pipeline (cell array of strings): Specifies the denoising strategies to be\n%                                     applied. Each element is a string in the\n%                                     format 'strategy-number'. For example,\n%                                     'HMP-6' indicates using 6 head motion\n%                                     parameters. Valid strategies include:\n%             'HMP': Head Motion Parameters, options: 6, 12, 24\n%             'GS': Global Signal, options: 1, 2, 4\n%             'CSF_WM': CSF and White Matter signals, options: 2, 4, 8\n%             'aCompCor': CompCor, options: 10, 50\n%             'MotionOutlier': Motion Outliers, options: FD &gt; 0.5, DVARS &gt; 1.5\n%             'Cosine': Discrete Cosine Transform based regressors for HPF\n%             'FD': Framewise Displacement, a raw non-binary value\n%             'Null': Returns an empty table if no confounds are to be applied\n%\n% Outputs:\n%   confounds (table): A table containing the selected confounds, formatted for\n%                      use in SPM. Each column represents a different confound,\n%                      and each row corresponds to a time point in the fMRI data.\n%\n% Author: Andrea Costantino\n% Date: 23/1/2023\n%\n% Example:\n%   confounds = fMRIprepConfounds2SPM('path/to/json', 'path/to/tsv', {'HMP-6', 'GS-4'});\n%\n% This example would extract and format 6 head motion parameters and the global\n% signal (with raw, derivative, and squared derivative) for SPM analysis.\n\n% Read the TSV file containing the confound values\ntsv_run = readtable(tsv_path, 'FileType', 'text');\n\n% Open and read the JSON file, then parse it into a MATLAB structure\nfid = fopen(json_path); \nraw = fread(fid, inf); \nstr = char(raw'); \nfclose(fid); \njson_run = jsondecode(str);\n\n% Initialize an empty cell array to store the keys of the selected confounds\nselected_keys = {};\n\n% If 'Null' is found in the pipeline, return an empty table and exit the function\nif any(strcmp(pipeline, 'Null'))\n    disp('\"Null\" found in the pipeline. Returning an empty table.')\n    return;\nelse\n    % Process each specified strategy in the pipeline\n\n    % Head Motion Parameters (HMP)\n    if any(contains(pipeline, 'HMP'))\n        % Extract and validate the specified number of head motion parameters\n        idx = find(contains(pipeline, 'HMP'));\n        conf_num_str = pipeline(idx(1)); \n        conf_num_str_split = strsplit(conf_num_str{1}, '-');\n        conf_num = str2double(conf_num_str_split(2));\n        if ~any([6, 12, 24] == conf_num)\n            error('HMP must be 6, 12, or 24.');\n        else\n            % Add the appropriate head motion parameters to selected_keys\n            hmp_id = floor(conf_num / 6);\n            if hmp_id &gt; 0\n                selected_keys = [selected_keys, {'rot_x', 'rot_y', 'rot_z', 'trans_x', 'trans_y', 'trans_z'}];\n            end\n            if hmp_id &gt; 1\n                selected_keys = [selected_keys, {'rot_x_derivative1', 'rot_y_derivative1', 'rot_z_derivative1', 'trans_x_derivative1', 'trans_y_derivative1', 'trans_z_derivative1'}];\n            end\n            if hmp_id &gt; 2\n                selected_keys = [selected_keys, {'rot_x_power2', 'rot_y_power2', 'rot_z_power2', 'trans_x_power2', 'trans_y_power2', 'trans_z_power2', 'rot_x_derivative1_power2', 'rot_y_derivative1_power2', 'rot_z_derivative1_power2', 'trans_x_derivative1_power2', 'trans_y_derivative1_power2', 'trans_z_derivative1_power2'}];\n            end\n        end\n    end\n\n    % Global Signal (GS)\n    if any(contains(pipeline, 'GS'))\n        % Extract and validate the specified level of global signal processing\n        idx = find(contains(pipeline, 'GS'));\n        conf_num_str = pipeline(idx(1)); \n        conf_num_str_split = strsplit(conf_num_str{1}, '-');\n        conf_num = str2double(conf_num_str_split(2));\n        if ~any([1, 2, 4] == conf_num)\n            error('GS must be 1, 2, or 4.');\n        else\n            % Add the global signal parameters to selected_keys based on the specified level\n            gs_id = conf_num;\n            if gs_id &gt; 0\n                selected_keys = [selected_keys, {'global_signal'}];\n            end\n            if gs_id &gt; 1\n                selected_keys = [selected_keys, {'global_signal_derivative1'}];\n            end\n            if gs_id &gt; 2\n                selected_keys = [selected_keys, {'global_signal_derivative1_power2', 'global_signal_power2'}];\n            end\n        end\n    end\n\n    % CSF and WM masks global signal (CSF_WM)\n    if any(contains(pipeline, 'CSF_WM'))\n        % Extract and validate the specified level of CSF/WM signal processing\n        idx = find(contains(pipeline, 'CSF_WM'));\n        conf_num_str = pipeline(idx(1)); \n        conf_num_str_split = strsplit(conf_num_str{1}, '-');\n        conf_num = str2double(conf_num_str_split(2));\n        if ~any([2, 4, 8] == conf_num)\n            error('CSF_WM must be 2, 4, or 8.');\n        else\n            % Add the CSF and WM parameters to selected_keys based on the specified level\n            phys_id = floor(conf_num / 2);\n            if phys_id &gt; 0\n                selected_keys = [selected_keys, {'white_matter', 'csf'}];\n            end\n            if phys_id &gt; 1\n                selected_keys = [selected_keys, {'white_matter_derivative1', 'csf_derivative1'}];\n            end\n            if phys_id &gt; 2\n                selected_keys = [selected_keys, {'white_matter_derivative1_power2', 'csf_derivative1_power2', 'white_matter_power2', 'csf_power2'}];\n            end\n        end\n    end\n\n    % aCompCor\n    if any(contains(pipeline, 'aCompCor'))\n        % Extract and format aCompCor confounds based on the specified number\n        csf_50_dict = json_run(ismember({json_run.Mask}, 'CSF') &amp; ismember({json_run.Method}, 'aCompCor') &amp; ~contains({json_run.key}, 'dropped'));\n        wm_50_dict = json_run(ismember({json_run.Mask}, 'WM') &amp; ismember({json_run.Method}, 'aCompCor') &amp; ~contains({json_run.key}, 'dropped'));\n        idx = find(contains(pipeline, 'aCompCor'));\n        conf_num_str = pipeline{idx(1)}; \n        conf_num_str_split = strsplit(conf_num_str{1}, '-');\n        conf_num = str2double(conf_num_str_split(2));\n        if ~any([10, 50] == conf_num)\n            error('aCompCor must be 10 or 50.');\n        else\n            % Select the appropriate aCompCor components and add them to selected_keys\n            if conf_num == 10\n                csf = sort(cell2mat(csf_50_dict.keys()));\n                csf_10 = csf(1:5);\n                wm = sort(cell2mat(wm_50_dict.keys()));\n                wm_10 = wm(1:5);\n                selected_keys = [selected_keys, csf_10, wm_10];\n            elseif conf_num == 50\n                csf_50 = cell2mat(csf_50_dict.keys());\n                wm_50 = cell2mat(wm_50_dict.keys());\n                selected_keys = [selected_keys, csf_50, wm_50];\n            end\n        end\n    end\n\n    % Cosine\n    if any(contains(pipeline, 'Cosine'))\n        % Extract cosine-based regressors for high-pass filtering\n        cosine_keys = tsv_run.Properties.VariableNames(contains(tsv_run.Properties.VariableNames, 'cosine'));\n        selected_keys = [selected_keys, cosine_keys];\n    end\n\n    % MotionOutlier\n    if any(contains(pipeline, 'MotionOutlier'))\n        % Process motion outliers, either using pre-computed values or calculating them\n        motion_outlier_keys = tsv_run.Properties.VariableNames(find(contains(tsv_run.Properties.VariableNames, {'non_steady_state_outlier', 'motion_outlier'})));\n        selected_keys = [selected_keys, motion_outlier_keys];\n    end\n\n    % Framewise Displacement (FD)\n    if any(contains(pipeline, 'FD'))\n        % Add raw framewise displacement values to selected_keys\n        % If the first row is 'n/a', replace it with 0\n        fd_values = tsv_run.framewise_displacement;\n        if isnan(fd_values(1))\n            fd_values(1) = 0;\n        end\n        tsv_run.framewise_displacement = fd_values;\n        selected_keys = [selected_keys, {'framewise_displacement'}];\n    end\n\n    % Retrieve the selected confounds and convert them into a table\n    confounds_table = tsv_run(:, ismember(tsv_run.Properties.VariableNames, selected_keys));\n    confounds = fillmissing(confounds_table, 'constant', 0);\nend\nend\n</code></pre> gunzipNiftiFile.m <pre><code>function gunzippedNii = gunzipNiftiFile(niiGzFile, outPath)\n    % gunzipNiftiFile - Decompress (gunzip) .nii.gz file and save it into a\n    % './BIDS/derivatives/pre-SPM/gunzipped/sub-xx' folder in the derivatives directory of a BIDS dataset\n    %\n    % Author: Andrea Costantino\n    % Date: 3/2/2023\n    %\n    % Usage:  \n    %   outPath = gunzipNiftiFile(niiGzFile, outPath)\n    %\n    % Inputs:\n    %    niiGzFile - String indicating the path to the input .nii file.\n    %    outPath - String indicating the root output directory.\n    %\n    % Outputs:\n    %    gunzippedNii - String indicating the new directory of the output file.\n    %\n\n    % Extract subject and task from nii file name\n    [~, niiGzName, niiGzExt] = fileparts(niiGzFile); % isolate the name of the nii.gz file\n    nameSplits = split(niiGzName, \"_\"); % split the nii file name into segments\n    selectedSub = nameSplits{1}; % subject is the first segment\n\n    % Infer the output path if not provided\n    if nargin &lt; 2\n        % get the BIDS root folder path\n        splitPath = strsplit(niiGzFile, '/'); % split the string into folders\n        idx = find(contains(splitPath, 'BIDS')); % find the index of the split that includes 'BIDS'\n        bidsPath = strjoin(splitPath(1:idx), '/'); % create the full folder path\n        outPath = fullfile(bidsPath, 'derivatives', 'pre-SPM', 'gunzipped', selectedSub); % build the output path\n    end\n\n    % Check if the output folder already exists\n    if exist(outPath, 'dir') == 7\n        fprintf('GUNZIP: Output directory already exists: %s.\\n', outPath);\n    else\n        mkdir(outPath); % create the output directory\n        fprintf('GUNZIP: Created output directory: %s.\\n', outPath);\n    end\n\n    % Check if the output file already exists\n    newFilePath = fullfile(outPath, niiGzName);\n\n    if exist(newFilePath, 'file') == 2\n        fprintf('GUNZIP: Gunzipped file already exists: %s\\n', newFilePath);\n        gunzippedNii = {newFilePath};\n    else\n        % gunzip them\n        fprintf('GUNZIP: decompressing file %s ...\\n', [niiGzName, niiGzExt])\n        gunzippedNii = gunzip(niiGzFile, outPath);\n        fprintf('GUNZIP: Created gunzipped file: %s\\n', newFilePath);\n\n        % Save a copy of this function in the output folder\n        if exist(fullfile(outPath, 'smoothNiftiFile.m'), 'file') ~= 2\n            copyfile([mfilename('fullpath'), '.m'], fullfile(outPath, 'gunzipNiftiFile.m'));\n        end\n    end\nend\n</code></pre> smoothNiftiFile.m <pre><code>function newFilePath = smoothNiftiFile(niiFile, outPath)\n    % smoothNiftiFile - Smooth a .nii file and save it into a\n    % './BIDS/derivatives/pre-SPM/smoothed/sub-xx' folder in the derivatives directory of a BIDS dataset\n    %\n    % Author: Andrea Costantino\n    % Date: 3/2/2023\n    %\n    % Usage:  \n    %   outRoot = smoothNiftiFile(niiFile, outPath)\n    %\n    % Inputs:\n    %    niiFile - String indicating the path to the input .nii file.\n    %    outRoot - String indicating the output directory.\n    %\n    % Outputs:\n    %    newFilePath - String indicating the new directory of the output file.\n    %\n\n    % Extract subject and task from nii file name\n    [niiFolder, niiName, niiExt] = fileparts(niiFile); % isolate the name of the nii file\n    subAndTask = split(niiName, \"_\"); % split the nii file name into segments\n    selectedSub = subAndTask{1}; % subject is the first segment\n\n    % Infer the output path if not provided\n    if nargin &lt; 2\n        % get the BIDS root folder path\n        splitPath = strsplit(niiFile, '/'); % split the string into folders\n        idx = find(contains(splitPath, 'BIDS')); % find the index of the split that includes 'BIDS'\n        bidsPath = strjoin(splitPath(1:idx), '/'); % create the full folder path\n        outPath = fullfile(bidsPath, 'derivatives', 'pre-SPM', 'smoothed', selectedSub); % build the output path\n    end\n\n    % Check if the output folder already exists\n    if exist(outPath, 'dir') == 7\n        fprintf('SMOOTH: Output directory %s already exists.\\n', outPath);\n    else\n        mkdir(outPath); % create the output directory\n        fprintf('SMOOTH: Created output directory %s.\\n', outPath);\n    end\n\n    % Check if the output file already exists\n    smoothFileName = strcat('smooth_', [niiName, niiExt]);\n    newFilePath = fullfile(outPath, strrep(smoothFileName, niiExt, ['_smooth', niiExt]));\n\n    if exist(newFilePath, 'file') == 2\n        fprintf('SMOOTH: Smoothed file already exists: %s\\n', newFilePath);\n    else\n        % Setup and run SPM smoothing job\n        matlabbatch{1}.spm.spatial.smooth.data = {niiFile};\n        matlabbatch{1}.spm.spatial.smooth.fwhm = [6 6 6];\n        matlabbatch{1}.spm.spatial.smooth.dtype = 0;\n        matlabbatch{1}.spm.spatial.smooth.im = 0;\n        matlabbatch{1}.spm.spatial.smooth.prefix = 'smooth_';\n\n        % Initialize SPM\n        spm_jobman('initcfg');\n        spm('defaults','fmri');\n\n        % Run batch job and suppress the SPM output\n        fprintf('SMOOTH: smoothing file %s ...\\n', [niiName, niiExt])\n        evalc('spm_jobman(''run'', matlabbatch);');\n\n        % Move file to correct folder\n        movefile(fullfile(niiFolder, smoothFileName), newFilePath);\n        fprintf('SMOOTH: Created smoothed file: %s\\n', newFilePath);\n\n        % Save a copy of this function in the output folder\n        if exist(fullfile(outPath, 'smoothNiftiFile.m'), 'file') ~= 2\n            copyfile([mfilename('fullpath'), '.m'], fullfile(outPath, 'smoothNiftiFile.m'));\n        end\n    end\nend\n</code></pre> <p>Continue to the next guide for instructions on setting up Regions of Interest (ROIs) to extract and analyze data from specific brain regions: \u2192 Regions of Interest</p>"},{"location":"research/fmri/analysis/fmri-glm.html","title":"General linear model in SPM","text":"<p>You should land on this page after having collected your fMRI data, converted it to BIDS and preprocessed it. Your goal now is to model the BOLD activity with a Generalised Linear Model (GLM), in order to obtain the beta values on which to apply further analyses.</p> <p>You should start here if you are not yet familiar with SPM and what it does. Before scripting your analysis pipeline, take your time to understand the different steps taken by SPM and what they do. Once you're confident with it, move ahead and write your analysis in code (see First-level analysis - scripting).</p> <p>In this section, we will use the Statistical Parametric Mapping (SPM) package to construct the GLM. Here\u2019s an overview of the steps:</p> <ol> <li>Data Preparation: Get your files ready for SPM.</li> <li>Design Matrix Setup: Define the model for your analysis.</li> <li>Model Estimation &amp; Results: Estimate your model and analyze contrasts.</li> </ol> <p>This page will accompany you through the steps and can be used as a guide to the input and output of each function of SPM. It is not a complete tutorial on SPM, however, and there are more extensive resources out there. It can be a good idea to cover the tutorials from Andy Jahn, for instance, as they go in depth into SPM and how it works. </p>"},{"location":"research/fmri/analysis/fmri-glm.html#step-1-data-preparation","title":"Step 1: Data Preparation","text":"<p>Before running the GLM, we need to make sure the data is compatible with SPM. There are two steps that need to be taken to bring your <code>.nii</code> files from fMRIPrep output to SPM input:</p> <ol> <li>gunzipping (de-compressing <code>.nii.gz</code> files, which SPM can't handle natively)</li> <li>smoothing (mostly for localizer runs).</li> </ol> <p>The suggested way of proceeding is to create a <code>derivatives/pre-SPM</code> folder where to store a <code>gunzipped</code> output folder and a <code>smoothed</code> output folder.</p>"},{"location":"research/fmri/analysis/fmri-glm.html#decompressing-nifti-files","title":"Decompressing NIfTI Files","text":"<p>SPM cannot process <code>.nii.gz</code> files directly, so we first need to decompress them:</p> <ol> <li>Create a directory for pre-processed files:</li> </ol> <pre><code>mkdir derivatives/pre-SPM\n</code></pre> <ol> <li>Decompress the files using <code>gunzip</code> in the terminal:</li> </ol> <pre><code>gunzip path/to/your/files/*.nii.gz\n</code></pre> <ul> <li>Store the decompressed files in a subdirectory called <code>gunzipped</code> inside <code>derivatives/pre-SPM</code>.</li> </ul> <p>Decompress with a right-click!</p> <p>Most Operating Systems will let you decompress the <code>nii.gz</code> files directly from the File Explorer. Right click on the files you want to decompress, and extract them like you would do with a compressed folder.</p>"},{"location":"research/fmri/analysis/fmri-glm.html#smoothing-functional-data","title":"Smoothing functional data","text":"<p>Smoothing is required to increase signal-to-noise ratio, especially for localizer runs. Follow these steps:</p> <ol> <li>Launch the SPM GUI with the command:</li> </ol> <pre><code>spm fmri\n</code></pre> <ol> <li>In the GUI, click on <code>Smooth</code>.</li> <li>Select the decompressed <code>.nii</code> files.</li> <li>Set the FWHM (Full Width at Half Maximum) to <code>[4 4 4]</code> or <code>[6 6 6]</code> for moderate smoothing.</li> <li>Save the smoothed output in <code>derivatives/pre-SPM/smoothed</code>.</li> </ol> <p>Automated Preprocessing</p> <p>You can integrate the decompression and smoothing steps into a script to streamline your workflow, avoiding manual steps (see the Analysis Workflow for an example).</p>"},{"location":"research/fmri/analysis/fmri-glm.html#step-2-design-matrix-setup","title":"Step 2: Design Matrix Setup","text":"<p>To run a GLM, you need to create a design matrix that links the timing of experimental conditions to the observed BOLD response. The design matrix is specified in SPM\u2019s <code>Specify 1st level</code> interface. While this can be done manually for simpler designs, for complex designs with many conditions, it\u2019s more efficient to use externally generated onset time and confounds files.</p>"},{"location":"research/fmri/analysis/fmri-glm.html#creating-onset-files","title":"Creating onset files","text":"<p>For complex designs, you should create one onset file per run per subject, containing information about event types, onsets, and durations. The <code>eventsBIDS2SPM</code> function can help convert BIDS-formatted event files into the onset files that SPM requires.</p> <ol> <li>Use the <code>eventsBIDS2SPM</code> function to convert event files.</li> <li> <p>Store the onset files in the following structure:</p> <ul> <li><code>BIDS/sub-xx/func/sub-xx_run-x_eventsspm.mat</code>.</li> </ul> <pre><code>BIDS/\n\u251c\u2500\u2500 sub-01/\n\u2502   \u2514\u2500\u2500 func/\n\u2502       \u251c\u2500\u2500 sub-01_run-1_eventsspm.mat\n\u2502       \u2514\u2500\u2500 sub-01_run-2_eventsspm.mat\n</code></pre> </li> </ol> eventsBIDS2SPM <pre><code>function new_df = eventsBIDS2SPM(tsv_file)\n    % eventsBIDS2SPM - Convert BIDS event files to SPM format\n    % This function reads a BIDS event file and converts it to the format required by SPM.\n    % It extracts the unique trial types and their onsets and durations and stores them in a\n    % Matlab structure.\n    %\n    % Author: Andrea Costantino\n    % Date: 23/1/2023\n    %\n    % Usage:\n    %   mat_dict = eventsBIDS2SPM(tsv_file, run_id)\n    %\n    % Inputs:\n    %   tsv_file - string, path to the tsv file containing the events\n    %\n    % Outputs:\n    %   mat_dict - struct, a Matlab structure containing the events in the format\n    %              required by SPM. The structure contains three fields:\n    %                - 'names': cell array of string, the names of the trial types\n    %                - 'onsets': cell array of double, onset times of the trials\n    %                - 'durations': cell array of double, duration of the trials\n    %\n    % This function reads a BIDS event file and converts it to the format required by SPM.\n    % It extracts the unique trial types and their onsets and durations and stores them in a\n    % Matlab structure\n\n    % read the tsv file\n    df = readtable(tsv_file,'FileType','text');\n    % Select unique trial type name\n    unique_names = unique(df.trial_type);\n    % Make new table in a form that SPM can read\n    new_df = table('Size',[length(unique_names),3],'VariableTypes',{'cellstr', 'cellstr', 'cellstr'},'VariableNames',{'names', 'onsets', 'durations'});\n    % For each trial type (i.e., condition)\n    for k = 1:length(unique_names)\n        % Select rows belonging to that condition\n        filtered = df(strcmp(df.trial_type,unique_names{k}),:);\n        % Copy trial name, onset and duration to the new table\n        new_df.names(k) = unique(filtered.trial_type);\n        new_df.onsets(k) = {filtered.onset};\n        new_df.durations(k) = {filtered.duration};\n    end\n    new_df = sortrows(new_df, 'names');\nend\n</code></pre>"},{"location":"research/fmri/analysis/fmri-glm.html#creating-confounds-files","title":"Creating confounds files","text":"<p>Head motion and other confound regressors from fMRIPrep need to be formatted to be compatible with SPM. The <code>fMRIprepConfounds2SPM</code> function can convert these files into the format required by SPM.</p> <ol> <li>Use the <code>fMRIprepConfounds2SPM</code> function to convert confounds from fMRIPrep.</li> <li>Store the confounds files in the BIDS structure with SPM-compatible names:<ul> <li><code>BIDS/sub-xx/func/sub-xx_run-x_confoundsspm.mat</code>.</li> </ul> </li> </ol> fMRIprepConfounds2SPM <pre><code>function confounds = fMRIprepConfounds2SPM(json_path, tsv_path, pipeline)\n    % fMRIprepConfounds2SPM - Extracts and formats fMRI confounds for SPM analysis\n    %\n    % This function processes confound data from fMRIprep outputs, suitable for\n    % Statistical Parametric Mapping (SPM) analysis. It reads a JSON file with\n    % confound descriptions and a TSV file with confound values, then selects and\n    % formats the required confounds based on the specified denoising pipeline.\n    %\n    % Usage:\n    %   confounds = fMRIprepConfounds2SPM(json_path, tsv_path, pipeline)\n    %\n    % Inputs:\n    %   json_path (string): Full path to the JSON file. This file contains metadata\n    %                       about the confounds, such as their names and properties.\n    %\n    %   tsv_path (string):  Full path to the TSV file. This file holds the actual\n    %                       confound values in a tabular format for each fMRI run.\n    %\n    %   pipeline (cell array of strings): Specifies the denoising strategies to be\n    %                                     applied. Each element is a string in the\n    %                                     format 'strategy-number'. For example,\n    %                                     'HMP-6' indicates using 6 head motion\n    %                                     parameters. Valid strategies include:\n    %             'HMP': Head Motion Parameters, options: 6, 12, 24\n    %             'GS': Global Signal, options: 1, 2, 4\n    %             'CSF_WM': CSF and White Matter signals, options: 2, 4, 8\n    %             'aCompCor': CompCor, options: 10, 50\n    %             'MotionOutlier': Motion Outliers, options: FD &gt; 0.5, DVARS &gt; 1.5\n    %             'Cosine': Discrete Cosine Transform based regressors for HPF\n    %             'FD': Framewise Displacement, a raw non-binary value\n    %             'Null': Returns an empty table if no confounds are to be applied\n    %\n    % Outputs:\n    %   confounds (table): A table containing the selected confounds, formatted for\n    %                      use in SPM. Each column represents a different confound,\n    %                      and each row corresponds to a time point in the fMRI data.\n    %\n    % Author: Andrea Costantino\n    % Date: 23/1/2023\n    %\n    % Example:\n    %   confounds = fMRIprepConfounds2SPM('path/to/json', 'path/to/tsv', {'HMP-6', 'GS-4'});\n    %\n    % This example would extract and format 6 head motion parameters and the global\n    % signal (with raw, derivative, and squared derivative) for SPM analysis.\n\n    % Read the TSV file containing the confound values\n    tsv_run = readtable(tsv_path, 'FileType', 'text');\n\n    % Open and read the JSON file, then parse it into a MATLAB structure\n    fid = fopen(json_path); \n    raw = fread(fid, inf); \n    str = char(raw'); \n    fclose(fid); \n    json_run = jsondecode(str);\n\n    % Initialize an empty cell array to store the keys of the selected confounds\n    selected_keys = {};\n\n    % If 'Null' is found in the pipeline, return an empty table and exit the function\n    if any(strcmp(pipeline, 'Null'))\n        disp('\"Null\" found in the pipeline. Returning an empty table.')\n        return;\n    else\n        % Process each specified strategy in the pipeline\n\n        % Head Motion Parameters (HMP)\n        if any(contains(pipeline, 'HMP'))\n            % Extract and validate the specified number of head motion parameters\n            idx = find(contains(pipeline, 'HMP'));\n            conf_num_str = pipeline(idx(1)); \n            conf_num_str_split = strsplit(conf_num_str{1}, '-');\n            conf_num = str2double(conf_num_str_split(2));\n            if ~any([6, 12, 24] == conf_num)\n                error('HMP must be 6, 12, or 24.');\n            else\n                % Add the appropriate head motion parameters to selected_keys\n                hmp_id = floor(conf_num / 6);\n                if hmp_id &gt; 0\n                    selected_keys = [selected_keys, {'rot_x', 'rot_y', 'rot_z', 'trans_x', 'trans_y', 'trans_z'}];\n                end\n                if hmp_id &gt; 1\n                    selected_keys = [selected_keys, {'rot_x_derivative1', 'rot_y_derivative1', 'rot_z_derivative1', 'trans_x_derivative1', 'trans_y_derivative1', 'trans_z_derivative1'}];\n                end\n                if hmp_id &gt; 2\n                    selected_keys = [selected_keys, {'rot_x_power2', 'rot_y_power2', 'rot_z_power2', 'trans_x_power2', 'trans_y_power2', 'trans_z_power2', 'rot_x_derivative1_power2', 'rot_y_derivative1_power2', 'rot_z_derivative1_power2', 'trans_x_derivative1_power2', 'trans_y_derivative1_power2', 'trans_z_derivative1_power2'}];\n                end\n            end\n        end\n\n        % Global Signal (GS)\n        if any(contains(pipeline, 'GS'))\n            % Extract and validate the specified level of global signal processing\n            idx = find(contains(pipeline, 'GS'));\n            conf_num_str = pipeline(idx(1)); \n            conf_num_str_split = strsplit(conf_num_str{1}, '-');\n            conf_num = str2double(conf_num_str_split(2));\n            if ~any([1, 2, 4] == conf_num)\n                error('GS must be 1, 2, or 4.');\n            else\n                % Add the global signal parameters to selected_keys based on the specified level\n                gs_id = conf_num;\n                if gs_id &gt; 0\n                    selected_keys = [selected_keys, {'global_signal'}];\n                end\n                if gs_id &gt; 1\n                    selected_keys = [selected_keys, {'global_signal_derivative1'}];\n                end\n                if gs_id &gt; 2\n                    selected_keys = [selected_keys, {'global_signal_derivative1_power2', 'global_signal_power2'}];\n                end\n            end\n        end\n\n        % CSF and WM masks global signal (CSF_WM)\n        if any(contains(pipeline, 'CSF_WM'))\n            % Extract and validate the specified level of CSF/WM signal processing\n            idx = find(contains(pipeline, 'CSF_WM'));\n            conf_num_str = pipeline(idx(1)); \n            conf_num_str_split = strsplit(conf_num_str{1}, '-');\n            conf_num = str2double(conf_num_str_split(2));\n            if ~any([2, 4, 8] == conf_num)\n                error('CSF_WM must be 2, 4, or 8.');\n            else\n                % Add the CSF and WM parameters to selected_keys based on the specified level\n                phys_id = floor(conf_num / 2);\n                if phys_id &gt; 0\n                    selected_keys = [selected_keys, {'white_matter', 'csf'}];\n                end\n                if phys_id &gt; 1\n                    selected_keys = [selected_keys, {'white_matter_derivative1', 'csf_derivative1'}];\n                end\n                if phys_id &gt; 2\n                    selected_keys = [selected_keys, {'white_matter_derivative1_power2', 'csf_derivative1_power2', 'white_matter_power2', 'csf_power2'}];\n                end\n            end\n        end\n\n        % aCompCor\n        if any(contains(pipeline, 'aCompCor'))\n            % Extract and format aCompCor confounds based on the specified number\n            csf_50_dict = json_run(ismember({json_run.Mask}, 'CSF') &amp; ismember({json_run.Method}, 'aCompCor') &amp; ~contains({json_run.key}, 'dropped'));\n            wm_50_dict = json_run(ismember({json_run.Mask}, 'WM') &amp; ismember({json_run.Method}, 'aCompCor') &amp; ~contains({json_run.key}, 'dropped'));\n            idx = find(contains(pipeline, 'aCompCor'));\n            conf_num_str = pipeline{idx(1)}; \n            conf_num_str_split = strsplit(conf_num_str{1}, '-');\n            conf_num = str2double(conf_num_str_split(2));\n            if ~any([10, 50] == conf_num)\n                error('aCompCor must be 10 or 50.');\n            else\n                % Select the appropriate aCompCor components and add them to selected_keys\n                if conf_num == 10\n                    csf = sort(cell2mat(csf_50_dict.keys()));\n                    csf_10 = csf(1:5);\n                    wm = sort(cell2mat(wm_50_dict.keys()));\n                    wm_10 = wm(1:5);\n                    selected_keys = [selected_keys, csf_10, wm_10];\n                elseif conf_num == 50\n                    csf_50 = cell2mat(csf_50_dict.keys());\n                    wm_50 = cell2mat(wm_50_dict.keys());\n                    selected_keys = [selected_keys, csf_50, wm_50];\n                end\n            end\n        end\n\n        % Cosine\n        if any(contains(pipeline, 'Cosine'))\n            % Extract cosine-based regressors for high-pass filtering\n            cosine_keys = tsv_run.Properties.VariableNames(contains(tsv_run.Properties.VariableNames, 'cosine'));\n            selected_keys = [selected_keys, cosine_keys];\n        end\n\n        % MotionOutlier\n        if any(contains(pipeline, 'MotionOutlier'))\n            % Process motion outliers, either using pre-computed values or calculating them\n            motion_outlier_keys = tsv_run.Properties.VariableNames(find(contains(tsv_run.Properties.VariableNames, {'non_steady_state_outlier', 'motion_outlier'})));\n            selected_keys = [selected_keys, motion_outlier_keys];\n        end\n\n        % Framewise Displacement (FD)\n        if any(contains(pipeline, 'FD'))\n            % Add raw framewise displacement values to selected_keys\n            % If the first row is 'n/a', replace it with 0\n            fd_values = tsv_run.framewise_displacement;\n            if isnan(fd_values(1))\n                fd_values(1) = 0;\n            end\n            tsv_run.framewise_displacement = fd_values;\n            selected_keys = [selected_keys, {'framewise_displacement'}];\n        end\n\n        % Retrieve the selected confounds and convert them into a table\n        confounds_table = tsv_run(:, ismember(tsv_run.Properties.VariableNames, selected_keys));\n        confounds = fillmissing(confounds_table, 'constant', 0);\nend\n</code></pre> <p>BIDS-Compliant Naming</p> <p>Ensure the files are saved with names that include:</p> <ul> <li><code>sub-xx</code>: subject identifier.</li> <li><code>run-x</code>: run identifier.</li> <li><code>confoundsspm</code> or <code>eventsspm</code> for confounds and timings, respectively.</li> </ul>"},{"location":"research/fmri/analysis/fmri-glm.html#using-the-functions-in-a-script","title":"Using the Functions in a Script","text":"<p>Both <code>eventsBIDS2SPM</code> and <code>fMRIprepConfounds2SPM</code> are MATLAB functions that can be integrated into scripts for batch processing in SPM. This allows you to automate the import of timing and confounds data or save them into SPM-compatible files for later use.</p> Directly Load Data into SPMSave SPM-Compatible Files <p>You can use these functions within a MATLAB script to load onset times and confounds directly into an SPM batch job. This is useful when you want to process multiple subjects and runs in one go:</p> <pre><code>% Example script to use eventsBIDS2SPM and fMRIprepConfounds2SPM in batch jobs\nsubject_id = 'sub-01';\nrun_id = 'run-01';\n\n% Paths to the BIDS event and confound files\ntsv_file = fullfile('BIDS', subject_id, 'func', [subject_id '_' run_id '_events.tsv']);\njson_file = fullfile('BIDS', subject_id, 'func', [subject_id '_' run_id '_desc-confounds_timeseries.json']);\nconfound_tsv = fullfile('BIDS', subject_id, 'func', [subject_id '_' run_id '_desc-confounds_timeseries.tsv']);\n\n% Convert event files to SPM-compatible format\nonset_data = eventsBIDS2SPM(tsv_file);\n\n% Convert confound files to SPM-compatible format with a chosen pipeline\npipeline = {'HMP-6', 'GS-1'}; % Example: 6 head motion parameters + 1 global signal\nconfound_data = fMRIprepConfounds2SPM(json_file, confound_tsv, pipeline);\n\n% Pass the resulting onset and confound data directly into SPM batch processing\nmatlabbatch{1}.spm.stats.fmri_spec.sess.multi = {onset_data};\nmatlabbatch{1}.spm.stats.fmri_spec.sess.multi_reg = {confound_data};\n\n% Run the SPM batch\nspm_jobman('run', matlabbatch);\n</code></pre> <p>This script converts the timing and confounds information, then immediately feeds them into an SPM batch, making it suitable for automated batch jobs over multiple runs or subject.</p> <p>If you want to save the converted files for later use, you can use the functions to write them into files with BIDS-compliant names. This is helpful if you need to inspect the files or share them with collaborators before running the GLM analysis:</p> <pre><code>% Example script to convert and save files in SPM-compatible format\nsubject_id = 'sub-01';\nrun_id = 'run-01';\n\n% Paths to the BIDS event and confound files\ntsv_file = fullfile('BIDS', subject_id, 'func', [subject_id '_' run_id '_events.tsv']);\njson_file = fullfile('BIDS', subject_id, 'func', [subject_id '_' run_id '_desc-confounds_timeseries.json']);\nconfound_tsv = fullfile('BIDS', subject_id, 'func', [subject_id '_' run_id '_desc-confounds_timeseries.tsv']);\n\n% Specify output paths\nonset_save_path = fullfile('BIDS', subject_id, 'func', [subject_id '_' run_id '_desc-events.mat']);\nconfound_save_path = fullfile('BIDS', subject_id, 'func', [subject_id '_' run_id '_desc-confoundsspm.mat']);\n\n% Convert and save onset data\nonset_data = eventsBIDS2SPM(tsv_file);\nsave(onset_save_path, 'onset_data');\n\n% Convert and save confound data with a chosen pipeline\npipeline = {'HMP-6', 'GS-1'}; % Example: 6 head motion parameters + 1 global signal\nconfound_data = fMRIprepConfounds2SPM(json_file, confound_tsv, pipeline);\nsave(confound_save_path, 'confound_data');\n\n% Now the onset and confound files can be loaded into SPM as needed.\n</code></pre> <p>This script allows you to convert the timing and confound data and save them to files with clear, BIDS-compliant names. These files can later be imported into SPM using the GUI or through further scripting.</p> <p>Automating Batch Processing</p> <p>By integrating these functions into a script, you can automate the entire process of setting up the design matrix for multiple subjects and runs. This approach is particularly useful for large datasets, allowing you to focus on refining the analysis rather than manual data preparation.</p>"},{"location":"research/fmri/analysis/fmri-glm.html#specifying-the-1st-level-model-in-spm","title":"Specifying the 1<sup>st</sup>-Level Model in SPM","text":"<p>Once you have your onset times and confound regressors files ready, you can set up the design matrix:</p> <ol> <li>Open SPM: Launch the SPM GUI with <code>spm fmri</code></li> <li>Specify 1<sup>st</sup>-Level: Go to <code>Specify 1st Level</code> in the SPM menu.</li> <li>Set Parameters:<ul> <li>Units for design: Set to <code>seconds</code>.</li> <li>Interscan interval (TR): Use your fMRI acquisition\u2019s TR value.</li> <li>Microtime resolution: This should be the number of slices acquired per TR (e.g., <code>64</code> for a 64-slice scan).</li> </ul> </li> <li>Input Onset Files:<ul> <li>Use the multiple conditions option to input onset time files (e.g., <code>sub-01_run-01_eventsspm.mat</code>).</li> </ul> </li> <li>Include Confound Regressors:<ul> <li>Select the confound regressors from the confound files (e.g., <code>sub-01_run-01_confoundsspm.mat</code>).</li> </ul> </li> </ol>"},{"location":"research/fmri/analysis/fmri-glm.html#reviewing-the-design-matrix","title":"Reviewing the Design Matrix","text":"<p>After specifying the design matrix:</p> <ol> <li>Click <code>Review</code>: This will open a visualization of the design matrix.</li> <li>Check for the following:<ul> <li>Clear separation between conditions.</li> <li>Proper alignment of conditions with the expected timing.</li> <li>Inclusion of nuisance regressors (e.g., head motion).</li> </ul> </li> </ol> <p>What to Look For</p> <ul> <li>Boxcar Patterns: Ensure that the regressors follow the expected patterns based on your design.</li> <li>Orthogonality: Verify that the conditions are not overly correlated, as this can impact the model\u2019s stability.</li> </ul> <p>Saving the Design Matrix</p> <p>To save the design matrix visualization for documentation: - Right-click on the matrix plot and select <code>Save as Image</code>. - Store the image in your documentation folder.</p>"},{"location":"research/fmri/analysis/fmri-glm.html#step-3-model-estimation-results","title":"Step 3: Model Estimation &amp; Results","text":"<p>With your design matrix ready, you can estimate the model and define contrasts to test your hypotheses. This step will produce the beta values and residuals necessary for further analysis.</p>"},{"location":"research/fmri/analysis/fmri-glm.html#estimating-the-model","title":"Estimating the Model","text":"<ol> <li>Review the Design Matrix: Before estimation, ensure the design matrix looks correct by clicking <code>Review</code>.</li> <li> <p>Estimate the Model: Select <code>Estimate</code> in the GUI and choose the <code>SPM.mat</code> file.</p> <ul> <li>Set <code>write residuals</code> to <code>yes</code> to save residual images.</li> <li> <p>Click <code>Run</code> to initiate the model estimation.</p> </li> <li> <p>This process will generate beta images (one per regressor) and residual variance estimates.</p> </li> </ul> </li> </ol> <p>Why Save Residuals?</p> <p>Saving residuals can help diagnose issues with model fit by checking for any systematic patterns in the residual images.</p>"},{"location":"research/fmri/analysis/fmri-glm.html#defining-and-evaluating-contrasts","title":"Defining and Evaluating Contrasts","text":"<p>Contrasts allow you to test specific hypotheses about the brain's response to different conditions, such as comparing activations between different task conditions or testing against a baseline.</p> <ol> <li> <p>Define New Contrasts:</p> <ul> <li>Go to <code>Results</code> &gt; <code>Define New Contrast</code>.</li> <li>Enter a name for the contrast (e.g., <code>Condition1 &gt; Condition2</code>).</li> <li>Specify the weights for each condition based on the order of the regressors (e.g., <code>[1 -1]</code>).</li> </ul> </li> <li> <p>Evaluate the Contrasts:</p> <ul> <li>After defining contrasts, set the desired threshold (e.g., <code>p &lt; 0.001</code>) and click <code>Run</code> to see the results.</li> <li>Review the statistical maps for significant clusters or activations.</li> </ul> </li> </ol> <p>Example contrast</p> <p>For a comparison between two conditions (e.g., <code>Faces</code> vs. <code>Objects</code>), use: <pre><code>[1 -1]\n</code></pre> This weights <code>Faces</code> positively and <code>Objects</code> negatively, highlighting brain regions more active during the <code>Faces</code> condition.</p>"},{"location":"research/fmri/analysis/fmri-glm.html#verifying-the-order-of-regressors","title":"Verifying the order of regressors","text":"<p>It\u2019s crucial to confirm the order of regressors in the design matrix before specifying contrasts to ensure that your weights align correctly with the conditions. Here\u2019s how to verify this using both the SPM GUI and by inspecting the <code>SPM.mat</code> file directly.</p> Option 1: Checking regressor order using the SPM GUIOption 2: Inspecting the <code>SPM.mat</code> File Directly <ol> <li> <p>Review the Design Matrix:</p> <ul> <li>Open SPM and select <code>Review</code> &gt; <code>SPM.mat</code>.</li> <li>This opens the design matrix in a new window.</li> <li>In the design matrix window, each column represents a different regressor (condition or nuisance variable).</li> </ul> </li> <li> <p>Interpret the Design Matrix:</p> <ul> <li>Hover over each column to see the name and description of the regressor.</li> <li>Typically, the first set of columns corresponds to your experimental conditions (e.g., <code>Faces</code>, <code>Objects</code>), followed by any nuisance regressors (e.g., motion parameters).</li> <li>Make a note of the order so that you can set your contrast weights accurately.</li> </ul> </li> </ol> <p>Use the design matrix visualization</p> <p>The design matrix visualization provides a graphical representation of each regressor. Patterns in the design matrix (e.g., boxcar shapes for task conditions) can help you verify the expected structure.</p> <ol> <li> <p>Load the <code>SPM.mat</code> File in MATLAB:</p> <p>In the MATLAB command window, navigate to the folder containing your <code>SPM.mat</code> file:  <pre><code>cd('/path/to/your/SPM_results_folder');\nload('SPM.mat');\n</code></pre> This loads the <code>SPM</code> structure into your workspace.</p> </li> <li> <p>Explore the Regressors:</p> <p>To see the names of the regressors, type:  <pre><code>SPM.xX.name\n</code></pre> This command will display a list of the regressor names in the order they appear in the design matrix.</p> </li> <li> <p>Interpret the Output:</p> <p>The output will look something like this:  <pre><code>'Sn(1) condition1*bf(1)'\n'Sn(1) condition2*bf(1)'\n'Sn(1) condition3*bf(1)'\n'Sn(1) motion_x'\n'Sn(1) motion_y'\n</code></pre> Each string corresponds to a regressor:</p> <ul> <li><code>Sn(1)</code>: Refers to session 1 (the run number).</li> <li><code>condition1*bf(1)</code>: Represents a condition convolved with the basis function (e.g., <code>Faces</code>).</li> <li><code>motion_x</code>, <code>motion_y</code>, etc.: These are motion parameters as nuisance regressors.</li> </ul> </li> <li> <p>Align the Contrast Weights:</p> <p>Based on this order, you can now set your contrast weights correctly. For instance, if <code>condition1</code> is the first regressor and <code>condition2</code> is the second, a contrast comparing them would be:  <pre><code>[1 -1 0 0 0 ...]\n</code></pre></p> </li> </ol> <p>Checking Regressors in a Script</p> <p>If you are scripting the process, you can automate this check by including: <pre><code>% Load the design matrix and display regressor names\nload('SPM.mat');\ndisp(SPM.xX.name);\n</code></pre> This will print the regressor names directly in the MATLAB command window, helping you confirm the correct order before specifying your contrasts.</p> <p>Why checking regressor order matters?</p> <ul> <li>Ensures Correct Contrast Specification: Mismatched contrasts can lead to incorrect interpretations of your data, as they might test unintended comparisons.</li> <li>Simplifies Troubleshooting: If the results look unexpected, double-checking the regressor order is one of the first steps to identify potential issues in the GLM setup.</li> <li>Consistency Across Sessions: If you\u2019re analyzing multiple runs or sessions, verifying regressor order ensures consistency across sessions, which is crucial for second-level analyses.</li> </ul>"},{"location":"research/fmri/analysis/fmri-glm.html#visualizing-and-saving-results","title":"Visualizing and Saving Results","text":"<ol> <li>Viewing Results: Use the SPM results viewer to explore significant clusters.</li> <li>Save the Statistical Maps:<ul> <li>Save thresholded activation maps as <code>.nii</code> files using the <code>Save</code> button in the results window.</li> </ul> </li> <li>Generate Figures:<ul> <li>Use the <code>Render</code> or <code>Surface</code> options to create visual summaries of your findings.</li> <li>Save these figures for inclusion in reports or presentations.</li> </ul> </li> </ol> <p>Exporting Images</p> <p>To include figures in publications or reports: - Use <code>Export</code> in SPM to save figures as high-resolution images. - For 3D brain renderings, adjust the orientation and threshold for a clear presentation.</p> <ul> <li>[TODO]: Add example directories showing how files are organized before and after preprocessing.</li> <li>[TODO]: Include screenshots or illustrations for key steps (e.g., setting up the design matrix in SPM).</li> <li>[PLACEHOLDER]: Add a screenshot of the SPM results interface to illustrate how to set thresholds.</li> <li>[TODO]: Include instructions on visualizing and saving the design matrix in SPM for documentation.</li> </ul>"},{"location":"research/fmri/analysis/fmri-mvpa.html","title":"Multi-variate analysis (MVPA/RSA) with CoSMoMVPA","text":""},{"location":"research/fmri/analysis/fmri-mvpa.html#overview","title":"Overview","text":"<p>This page shows how to use CoSMoMVPA for Multivariate Pattern Analysis (MVPA) and Representational Similarity Analysis (RSA) in fMRI research. It assumes that you understand basic fMRI concepts and want to learn how to go from raw data to advanced analyses that reveal the structure of neural representations.</p> <p>Download full code example</p> <p>The full code example, along with all the needed files to run this script, can be found in this repository (click on <code>&lt;&gt;Code</code> \u2192 <code>Download ZIP</code>). The only pre-requisite is that you have MATLAB and CoSMoMVPA installed on your system.</p> <p>Some of the code used here (e.g., for plotting or clustering data) is not shipped with CoSMoMVPA, and can be found in the <code>utils</code> folder of the repository.</p>"},{"location":"research/fmri/analysis/fmri-mvpa.html#typical-pipeline-for-real-fmri-data","title":"Typical Pipeline for Real fMRI Data","text":"<ol> <li>Data Acquisition: Collect fMRI data as participants perform tasks or view stimuli from different categories.</li> <li>BIDS Conversion: Convert raw data into the Brain Imaging Data Structure (BIDS) format. This standardization makes subsequent preprocessing and analysis much easier.</li> <li>Preprocessing (fMRIPrep): Use minimal preprocessing to preserve fine-grained voxel-level patterns. fMRIPrep performs standardized alignment and normalization without over-smoothing or aggressive denoising, both of which can destroy subtle patterns critical for MVPA.</li> <li>GLM Estimation (SPM):<ul> <li>Beta images: One per condition (category) and run.</li> <li>SPM.mat: Contains the experiment design and regression coefficients.</li> </ul> </li> <li>Defining Regions of Interest (ROIs):<ul> <li>Use anatomical or functional Regions of Interest (ROI) masks to focus on specific brain regions.</li> <li>Linear classifiers struggle with high-dimensional data \u2014 particularly when the number of features (voxels) are significantly higher than the number of observations (beta images), which is often the case with fMRI data. Therefore, restricting MVPA analyses to smaller ROIs is generally a good practice.</li> </ul> </li> </ol>"},{"location":"research/fmri/analysis/fmri-mvpa.html#purpose-of-this-tutorial","title":"Purpose of This Tutorial","text":"<p>In this example, we will investigate the representational structure of simulated synthetic data. We will illustrate the (few) steps required to perform a multi-variate analysis on post GLM data (i.e., beta images produces by SPM).</p> <p>The goal is to investigate the representational geometry of two ROIs, V1 and IT, to determine what properties of the stimuli are represented in these areas.</p>"},{"location":"research/fmri/analysis/fmri-mvpa.html#what-we-will-do","title":"What we will do","text":"<ol> <li>Generate synthetic fMRI-like data simulating responses in IT or V1.</li> <li>Perform MVPA (decoding) to see if the ROI can distinguish between multiple categories above chance.</li> <li>Conduct RSA to compare the observed representational geometry with theoretical model RDMs, revealing whether perceptual or categorical features dominate the neural patterns.</li> </ol>"},{"location":"research/fmri/analysis/fmri-mvpa.html#data-structure-synthetic","title":"Data Structure (Synthetic)","text":"<ul> <li>Subject(s): 1</li> <li>Runs: 10 runs/subject</li> <li>Categories (8 total): Human face, Human body, Animal face, Animal body, Natural round object, Natural spiky object, Artificial round object, Artificial spiky object</li> </ul>"},{"location":"research/fmri/analysis/fmri-mvpa.html#hypothesis","title":"Hypothesis","text":"<ul> <li>V1: Similarities are based on low-level perceptual features (e.g., round vs. spiky).</li> <li>IT: Similarities are based on higher-level conceptual distinctions (e.g., human vs. animal, animate vs. inanimate, natural vs. artificial).</li> </ul>"},{"location":"research/fmri/analysis/fmri-mvpa.html#code","title":"Code","text":""},{"location":"research/fmri/analysis/fmri-mvpa.html#1-setup","title":"1. Setup","text":"<p>We set up the MATLAB environment, ensuring reproducibility and proper paths.</p> <pre><code>% Clear workspace, close figures, and reset command window\nclear all;\nclose all;\nclc;\n\n% Set random seed for reproducibility\nseed = 42;\nrng(seed);\n\n% Add paths to required functions\naddpath(\"functions/\");\noutDir = fullfile(pwd, \"results\");\n</code></pre>"},{"location":"research/fmri/analysis/fmri-mvpa.html#2-optional-loading-real-data-from-spm","title":"2. (Optional) Loading Real Data from SPM","text":"<p>If we had real data, after preprocessing and GLM estimation, we could load it like this:</p> <pre><code>% Example (not run here):\nspm_path = 'path_to_SPM.mat';\nmask_path = 'path_to_ROI_mask.nii';\nds = cosmo_fmri_dataset(spm_path, 'mask', mask_path);\n</code></pre> <p>For this tutorial, we are skipping this step because we use synthetic data.</p>"},{"location":"research/fmri/analysis/fmri-mvpa.html#3-synthetic-data-generation","title":"3. Synthetic Data Generation","text":"<p>We generate synthetic data that mimics patterns in IT or V1. You can switch <code>roiName</code> between <code>\"V1\"</code> and <code>\"IT\"</code>.</p> <pre><code>% Parameters for synthetic data generation\nnumCategories = 8;\nnumRuns = 10;\nnumSubjects = 1;\nnumRepetitions = 1;\nsize = \"normal\";\n\n% Choose ROI\nroiName = \"IT\";  % For abstract, categorical representations\n% roiName = \"V1\"; % For low-level, perceptual representations\n\nsigma = 0.6; % Data variability\n\n% Generate synthetic dataset\nds = generate_clustered_dataset(numCategories, numSubjects, numRuns, ...\n    numRepetitions, sigma, seed, roiName, size);\n</code></pre>"},{"location":"research/fmri/analysis/fmri-mvpa.html#4-inspecting-the-data","title":"4. Inspecting the Data","text":"<p>Before proceeding, inspect the generated dataset:</p> <ul> <li>Target-to-Label Mapping: Understand which category corresponds to which integer label.</li> <li>Activation Heatmap: Visualize category-specific activity patterns.</li> </ul> <pre><code>% Extract and display unique labels\nlabelNames = unique(ds.sa.labels, 'stable');\ndisp('Target to Label Mapping:');\ndisp(table(ds.sa.targets, ds.sa.labels));\n\n% Plot activation patterns\nplot_activation_heatmap(ds, labelNames, roiName, outDir);\n</code></pre> <p></p>"},{"location":"research/fmri/analysis/fmri-mvpa.html#5-mvpa-classification-analysis","title":"5. MVPA: Classification Analysis","text":"<p>We use a simple Linear Discriminant Analysis (LDA) classifier with cross-validation to decode category information.</p> <p>The central idea is that if differences between categories are present, the classifier will be able to pick that signal up. When we use an MVPA classifier to brain data, we are testing whether the neural patterns (represented as voxel activity) for different categories contain distinct, systematic differences. If the brain region being analyzed encodes information about the categories in its activity patterns, the classifier will achieve accuracy above chance.</p> <p></p> <p>In other words, the classifier looks at the activity from multiple voxels and tries to find consistent differences between patterns for different categories (e.g., cats vs. dogs). If those patterns are different enough, the classifier can \"learn\" to predict the category based on the brain activity by identifying separable representations in a high-dimensional space of neural activity.</p> <p>This works because the brain encodes information about what we see in these patterns\u2014like a unique \"signature\" for each category. If the classifier succeeds, it shows that the brain region we're analyzing contains information relevant to distinguishing those categories.</p>"},{"location":"research/fmri/analysis/fmri-mvpa.html#steps","title":"Steps","text":"<ol> <li>Partitioning: Divide the data into folds based on runs (e.g., leave-one-run-out).</li> <li>Training &amp; Testing: Train on some runs, test on another run.</li> <li>Evaluate Accuracy: Compare accuracy to chance (\u215b = 12.5%).</li> </ol> <pre><code>% Define the classifier\nclassifier = @cosmo_classify_lda;\n\n% Set up cross-validation partitions\npartitions = cosmo_nfold_partitioner(ds);\n\n% Perform cross-validation decoding\n[predictedLabels, accuracy] = cosmo_crossvalidate(ds, classifier, partitions);\n\n% Compute chance level\nchanceLevel = 1 / numCategories;\n\n% Display the decoding results\nfprintf('Classification accuracy: %.2f%% (chance level: %.2f%%)\\n', ...\n    accuracy * 100, chanceLevel * 100);\n\n% Plot confusion matrix\nplot_confusion_matrix(ds.sa.targets, predictedLabels, numCategories, accuracy, ...\n    labelNames, roiName, outDir);\n</code></pre> <p>A confusion matrix helps identify which categories are easily distinguishable and which are often confounded.</p> <p></p>"},{"location":"research/fmri/analysis/fmri-mvpa.html#6-representational-dissimilarity-matrices-rdms","title":"6. Representational Dissimilarity Matrices (RDMs)","text":"<p>RSA involves comparing the RDMs from brain data to RDM for our theoretical models. We do this by computing RDMs:</p> <ol> <li>Mean Patterns: Average the data across runs for each category.</li> <li>Compute Distances: Calculate pairwise distances (1-Correlation) between categories to form an RDM.</li> <li>Compare to Models: Generate theoretical RDMs and see which best fits the data.</li> </ol> <pre><code>% Compute mean dataset across runs per category\ndsMean = cosmo_fx(ds, @(x) mean(x, 1), 'targets');\n\n% Visualize categories in a 2D space using MDS\nplot_mds(ds, roiName, outDir);\n</code></pre> <p></p> <pre><code>% Compute the data RDM\nmeasureDsm = @cosmo_dissimilarity_matrix_measure;\nargsDsm.metric = 'correlation';\nargsDsm.center_data = true;\n\ndataDsm = measureDsm(dsMean, argsDsm);\ndataRdm = squareform(dataDsm.samples);\n\n% Generate model RDMs (theoretical structures)\nrdms = generate_model_rdms(numCategories);\n</code></pre> <p>To visualize the RDMs:</p> <pre><code>plot_rdm_full(dataRdm, labelNames, 'Brain RDM');\nplot_rdm_full(rdms(1).dsm, labelNames, rdms(1).description);\nplot_rdm_full(rdms(2).dsm, labelNames, rdms(2).description);\nplot_rdm_full(rdms(3).dsm, labelNames, rdms(3).description);\n</code></pre>"},{"location":"research/fmri/analysis/fmri-mvpa.html#7-rsa-regression","title":"7. RSA Regression","text":"<p>We now fit a linear model that predicts the data RDM from these model RDMs.</p> <p>The regression coefficients indicate how strongly each model explains the observed data's representational geometry.</p> <pre><code>% Set up RSA regression\nmeasure = @cosmo_target_dsm_corr_measure;\nmeasureArgs = struct();\nmeasureArgs.center_data = true;\n\n% Prepare model RDM regressors\nregressorRDMs = cellfun(@(x) squareform(x)', {rdms.dsm}, 'UniformOutput', false);\nmeasureArgs.glm_dsm = regressorRDMs;\n\n% Run RSA regression\nresult = measure(dsMean, measureArgs);\n\n% Extract category labels\nlabels = dsMean.sa.labels;\n\n% Plot results: observed RDM, model RDMs, and regression coefficients\nplot_rdms_and_coefficients(dataRdm, rdms, labels, result.samples, roiName, outDir);\n</code></pre> <p></p>"},{"location":"research/fmri/analysis/fmri-mvpa.html#full-code-example-comparison-with-v1","title":"Full code example: comparison with V1","text":"<p>Now that we have the full picture, let's re-run our full code, but this time we only see the main results for the other synthetic ROI: V1.</p> <pre><code>%% Setup\n% Clear workspace, close figures, and reset command window\nclear all;\nclose all;\nclc;\n\n% Set random seed for reproducibility\nseed = 42;\nrng(seed);\n\n% Add paths to required functions\naddpath(\"functions/\");\noutDir = fullfile(pwd, \"results\");\n\n%% Synthetic data generation\n% Parameters for synthetic data generation\nnumCategories = 8;\nnumRuns = 10;\nnumSubjects = 1;\nnumRepetitions = 1;\nsize = \"normal\";\n\n% Choose ROI\nroiName = \"V1\"; % For low-level, perceptual representations\n\nsigma = 0.6; % Data variability\n\n% Generate synthetic dataset\nds = generate_clustered_dataset(numCategories, numSubjects, numRuns, ...\n    numRepetitions, sigma, seed, roiName, size);\n\n%% Inspecting the data\n% Extract and display unique labels\nlabelNames = unique(ds.sa.labels, 'stable');\ndisp('Target to Label Mapping:');\ndisp(table(ds.sa.targets, ds.sa.labels));\n\n% Plot activation patterns\nplot_activation_heatmap(ds, labelNames, roiName, outDir);\n\n%% MVPA: Classification Analysis\n% Define the classifier\nclassifier = @cosmo_classify_lda;\n\n% Set up cross-validation partitions\npartitions = cosmo_nfold_partitioner(ds);\n\n% Perform cross-validation decoding\n[predictedLabels, accuracy] = cosmo_crossvalidate(ds, classifier, partitions);\n\n% Compute chance level\nchanceLevel = 1 / numCategories;\n\n% Display the decoding results\nfprintf('Classification accuracy: %.2f%% (chance level: %.2f%%)\\n', ...\n    accuracy * 100, chanceLevel * 100);\n\n% Plot confusion matrix\nplot_confusion_matrix(ds.sa.targets, predictedLabels, numCategories, accuracy, ...\n    labelNames, roiName, outDir);\n\n%% Representational Dissimilarity Matrices (RDMs)\n% Compute mean dataset across runs per category\ndsMean = cosmo_fx(ds, @(x) mean(x, 1), 'targets');\n\n% Visualize categories in a 2D space using MDS\nplot_mds(dsMean, roiName, outDir);\n\n% Compute the data RDM\nmeasureDsm = @cosmo_dissimilarity_matrix_measure;\nargsDsm.metric = 'correlation';\nargsDsm.center_data = true;\n\ndataDsm = measureDsm(dsMean, argsDsm);\ndataRdm = squareform(dataDsm.samples);\n\n% Generate model RDMs (theoretical structures)\nrdms = generate_model_rdms(numCategories);\n\n%% RSA Regression\n% Set up RSA regression\nmeasure = @cosmo_target_dsm_corr_measure;\nmeasureArgs = struct();\nmeasureArgs.center_data = true;\n\n% Prepare model RDM regressors\nregressorRDMs = cellfun(@(x) squareform(x)', {rdms.dsm}, 'UniformOutput', false);\nmeasureArgs.glm_dsm = regressorRDMs;\n\n% Run RSA regression\nresult = measure(dsMean, measureArgs);\n\n% Extract category labels\nlabels = dsMean.sa.labels;\n\n% Plot results: observed RDM, model RDMs, and regression coefficients\nplot_rdms_and_coefficients(dataRdm, rdms, labels, result.samples, roiName, o\n</code></pre> <p>And the results:</p> <p> </p>"},{"location":"research/fmri/analysis/fmri-mvpa.html#conclusion","title":"Conclusion","text":"<p>What we learned:</p> <ul> <li>How to structure and preprocess fMRI data (BIDS, fMRIPrep, SPM) for MVPA and RSA.</li> <li>How to decode category information using MVPA.</li> <li>How to apply RSA to compare observed brain patterns to theoretical representational models.</li> </ul>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html","title":"fMRI Preprocessing and Quality Assessment","text":"<p>You should land on this page after having collected your (f)MRI data and converted it to BIDS.</p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#preprocessing-quality-assessment-overview","title":"Preprocessing &amp; Quality Assessment Overview","text":"<p>In this page, you will learn how to preprocess fMRI data using fMRIPrep and perform quality assessment with MRIQC. We will cover:</p> <ul> <li> <p> Running fMRIPrep   Step-by-step guide to run fMRIPrep, including the required command structure, key options, and output directory organization.</p> </li> <li> <p> Performing Quality Control with MRIQC   Use MRIQC to assess the quality of your MRI data. Identify potential artifacts and ensure data suitability for further analysis.</p> </li> <li> <p> Interpreting fMRIPrep Outputs   Understand the content of the fMRIPrep HTML report, including motion parameters, anatomical alignment, and other key quality checks.</p> </li> <li> <p> Reviewing MRIQC Reports   Learn how to interpret MRIQC's visual reports and quality metrics, such as SNR and temporal SNR, to evaluate the data's integrity.</p> </li> <li> <p> Troubleshooting Common Issues   Find solutions to common challenges with fMRIPrep and MRIQC, including memory management and output interpretation.</p> </li> <li> <p> Next Steps: GLM Analysis   Once your data is preprocessed and quality-checked, move on to first-level analysis with the General Linear Model.</p> </li> </ul> <ul> <li> <p>fMRIPrep Documentation   Get detailed insights into the preprocessing steps, output formats, and recommended practices.</p> </li> <li> <p>MRIQC Documentation   Explore MRIQC's metrics and recommendations for improving MRI data quality.</p> </li> <li> <p>NeuroStars Community   A valuable resource for troubleshooting and community discussions related to fMRIPrep and MRIQC.</p> </li> <li> <p>YouTube: Reviewing fMRIPrep Outputs</p> </li> </ul> <p>Tip</p> <p>Before proceeding, ensure that your fMRI data is converted into BIDS format. Refer to the BIDS Conversion Guide for more details.</p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#preprocessing-with-fmriprep","title":"Preprocessing with fMRIPrep","text":""},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#1-setting-up-fmriprep","title":"1. Setting Up fMRIPrep","text":"<p>To use fMRIPrep, ensure that you have:</p> <ul> <li>Docker (or Singularity for HPC environments).</li> <li>Installed the <code>fmriprep-docker</code> wrapper for easier command-line usage:</li> </ul> <pre><code>pip install fmriprep-docker\n</code></pre> <ul> <li>A valid FreeSurfer license (<code>license.txt</code>) saved in a path accessible by fMRIPrep. This is needed for surface-based preprocessing.</li> </ul> <p>System Requirements</p> <p>fMRIPrep is resource-intensive. For optimal performance, allocate:</p> <ul> <li>At least 16 GB RAM and 4 CPUs.</li> <li>A high-speed SSD for the working directory to improve I/O performance.</li> </ul> <p>For detailed instructions, visit the fMRIPrep Installation Guide.</p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#2-running-fmriprep","title":"2. Running fMRIPrep","text":"<p>Once your environment is ready, you can run fMRIPrep using the following command:</p> <pre><code>fmriprep-docker /path/to/BIDS /path/to/derivatives/fmriprep participant \\\n    --work-dir /path/to/temp_fmriprep \\\n    --fs-license-file /path/to/.license \\\n    --output-spaces MNI152NLin2009cAsym:res-2 anat fsnative \\\n    --participant-label &lt;SUBJECT_ID&gt; \\\n    --n-cpus 8 --mem-mb 16000 --notrack\n</code></pre> <p>Replace:</p> <ul> <li><code>/path/to/BIDS</code> with the path to your BIDS directory.</li> <li><code>/path/to/derivatives/fmriprep</code> with where you want to store fMRIPrep outputs.</li> <li><code>&lt;SUBJECT_ID&gt;</code> with the ID of the subject being processed.</li> </ul> Why specify output spaces? <p><code>--output-spaces</code> defines the spaces in which your data will be resampled. Common options include:</p> <ul> <li>MNI152NLin2009cAsym: Standard volumetric template.</li> <li>anat: Subject\u2019s native T1w space.</li> <li>fsnative: FreeSurfer's subject-specific surface space.</li> </ul>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#3-output-structure-and-files","title":"3. Output Structure and Files","text":"<p>After running fMRIPrep, the output will be in the <code>derivatives/fmriprep</code> folder. This includes:</p> <ul> <li>Preprocessed anatomical images (<code>T1w</code>, <code>T2w</code>).</li> <li>Preprocessed functional images (BOLD series).</li> <li>Confounds: <code>.tsv</code> files containing motion parameters and other potential noise regressors.</li> <li>Reports: <code>sub-xx.html</code> files with a summary of the preprocessing.</li> </ul> <p>Refer to the fMRIPrep Output Documentation for more information.</p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#quality-assessment-with-mriqc","title":"Quality Assessment with MRIQC","text":""},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#1-running-mriqc","title":"1. Running MRIQC","text":"<p>MRIQC helps identify potential issues in your data by generating quality metrics. Run MRIQC using Docker with the following command:</p> <pre><code>docker run -it --rm \\\n    -v /path/to/BIDS:/data:ro \\\n    -v /path/to/derivatives/mriqc:/out \\\n    nipreps/mriqc:latest /data /out participant \\\n    --participant-label &lt;SUBJECT_ID&gt; --nprocs 8 --mem-gb 16 --verbose-reports\n</code></pre> <p>This command will analyze individual subjects and save the results in the specified output directory. Replace the paths as appropriate.</p> <p>Running Group-Level Analysis</p> <p>After processing individual subjects, you can run a group-level analysis to compare metrics across subjects:</p> <pre><code>docker run -it --rm \\\n    -v /path/to/BIDS:/data:ro \\\n    -v /path/to/derivatives/mriqc:/out \\\n    nipreps/mriqc:latest /data /out group \\\n    --nprocs 8 --mem-gb 16 --verbose-reports\n</code></pre>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#2-understanding-mriqc-outputs","title":"2. Understanding MRIQC Outputs","text":"<p>MRIQC generates:</p> <ul> <li>Visual reports (<code>sub-xx.html</code>) for each subject.</li> <li>CSV files with quality metrics.</li> <li>Group-level metrics for overall dataset quality.</li> </ul> <p>Refer to the MRIQC Documentation for a detailed explanation of each metric.</p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#interpreting-fmriprep-and-mriqc-reports","title":"Interpreting fMRIPrep and MRIQC Reports","text":""},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#fmriprep-html-report","title":"fMRIPrep HTML Report","text":"<p>After running fMRIPrep, the outputs will be stored in the <code>derivatives/fmriprep</code> directory, with each subject's data organized into subfolders like <code>sub-01</code>. These folders contain both the preprocessed functional and anatomical data, alongside JSON files with metadata.</p> <p>Each subject\u2019s report (<code>sub-xx.html</code>) includes:</p> <ul> <li>Registration Plots: Check the alignment of functional and anatomical images.</li> <li>Field Map Corrections: Review the effect of susceptibility distortion corrections.</li> <li>Motion Correction: Look for high motion frames using Framewise Displacement (FD) plots.</li> </ul> What is Framewise Displacement (FD)? <p>FD is a measure of head movement between frames. High FD values indicate potential motion artifacts.</p> <p>Let\u2019s walk through the key components of the output and how to interpret the HTML summary reports.</p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#1-output-directory-structure","title":"1. Output Directory Structure","text":"<p>Within each subject's directory (<code>sub-01</code>):</p> <ul> <li><code>anat/</code> folder: Contains anatomical images, including normalized versions (e.g., <code>MNI152</code> template) and images in native space.</li> <li><code>func/</code> folder: Contains functional data for each run, including:</li> <li>Confound Regressors (<code>.tsv</code>): Time series of noise estimates like white matter and cerebrospinal fluid (CSF).</li> <li>Preprocessed Functional Images: Aligned to templates like <code>MNI152</code>.</li> <li>Brain Masks: Estimated masks for the brain, used in further analyses.</li> </ul> <p>These files will be referenced in the HTML summary report, which provides an overview of the preprocessing steps and quality metrics.</p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#2-opening-the-html-summary-report","title":"2. Opening the HTML Summary Report","text":"<p>To view the HTML report, navigate to <code>derivatives/fmriprep/sub-01/</code> and open <code>sub-01.html</code> by double-clicking it or using the terminal:</p> <p>The report contains the following sections: Summary, Anatomical, Functional, About, Methods, and Errors. Use the tabs at the top of the report to navigate these sections.</p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#3-understanding-the-summary-section","title":"3. Understanding the Summary Section","text":"<p>The Summary tab includes:</p> <ul> <li>Number of Structural and Functional Images: Lists the number of anatomical and functional images processed.</li> <li>Normalization Template: Shows the template used for alignment (e.g., <code>MNI152NLin2009cAsym</code>).</li> <li>FreeSurfer: Indicates whether surface-based preprocessing was performed.</li> </ul> <p>Make sure these details match the parameters specified in your fMRIPrep command.</p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#4-anatomical-quality-checks","title":"4. Anatomical Quality Checks","text":"<p>The Anatomical section provides:</p> <ul> <li>Brain Mask Overlay: Displays the brain mask (red outline), gray matter (magenta), and white matter boundaries (blue) overlaid on the anatomical image in sagittal, axial, and coronal views.</li> </ul> <p></p> <ul> <li> <p>Normalization Check: A GIF compares the subject\u2019s anatomical image with the MNI template. Ensure that:</p> </li> <li> <p>The outlines of the brain and internal structures (e.g., ventricles) align well.</p> </li> <li>Any misalignment could indicate poor normalization, which may need further inspection.</li> </ul> <p></p> <p>Tip</p> <p>Hover over the GIF to see the back-and-forth comparison between the subject's brain and the template. Look closely at the alignment of internal brain structures.</p> <ul> <li>Surface Reconstruction if you ran the <code>recon-all</code> routine in fMRIprep</li> </ul> <p></p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#5-functional-quality-checks","title":"5. Functional Quality Checks","text":"<p>In the Functional section, you\u2019ll find:</p> <ul> <li>Functional-to-Anatomical Alignment: A GIF shows how well the preprocessed functional images align with the anatomical image.</li> </ul> <p>Check for alignment</p> <p>Check for alignment between internal structures like ventricles in the functional and anatomical images. Open the image in a new tab (Right Click on the image -&gt; Open in a new tab) and hover to see the dynamic image.</p> <p></p> <ul> <li>CompCor Masks: Displays masks used for Anatomical Component Correction (aCompCor):</li> <li>White Matter and CSF (Magenta): Masks used to extract noise components.</li> <li>High-Variance Voxels (Blue): Used for Functional Component Correction (fCompCor).</li> </ul> <p>Assessing Alignment</p> <p>Good alignment between functional and anatomical images is crucial for accurate analysis. Pay special attention to lighter fluid-filled regions in the functional image, which should correspond with dark CSF areas in the anatomical image.</p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#6-bold-summary-and-carpet-plot","title":"6. BOLD Summary and Carpet plot","text":"<p>The report includes time series plots for various confounds:</p> <ul> <li>Global Signal (GS): Measures signal fluctuations across the entire brain.</li> <li>CSF Signal (GSCSF) and White Matter Signal: Represent fluctuations in specific tissue types.</li> <li>Motion Metrics (DVARS, Framewise Displacement):</li> <li>DVARS: Shows changes in BOLD signal intensity from one time point to the next.</li> <li>Framewise Displacement (FD): Tracks the amount of head movement between frames.</li> <li>Use DVARS and FD to identify frames with high motion that could affect data quality.</li> </ul> <p>Tip</p> <p>High motion values often correlate with changes in global signal. Consider including these regressors in your GLM to account for motion-related noise.</p> <p>The carpet plot displays time series of BOLD signals across different brain regions:</p> <ul> <li>Cortex (blue), Subcortex (orange), Gray Matter (green), and White Matter/CSF (red).</li> <li>Look for sudden changes across a column, which may indicate motion artifacts affecting the entire brain at a particular time point.</li> </ul> <p></p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#7-correlation-matrix-of-confound-regressors","title":"7. Correlation Matrix of Confound Regressors","text":"<p>The report also includes a correlation matrix showing relationships between confound regressors:</p> <ul> <li>High correlations between CSF and motion regressors may indicate that motion affects CSF signals.</li> <li>Use this matrix to decide which regressors to include in your GLM for better noise correction.</li> </ul> <p>High Correlations</p> <p>High correlation values may suggest redundancy among some regressors. Consider removing or combining them to avoid overfitting when building your GLM.</p> <p></p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#8-making-decisions-for-further-analysis","title":"8. Making Decisions for Further Analysis","text":"<p>After reviewing the report:</p> <ul> <li>Identify Good Quality Runs: Look for well-aligned images and minimal motion artifacts.</li> <li>Decide on Regressors: Choose confounds like DVARS, FD, and CompCor components to include in your GLM.</li> </ul> <p>What confound regressors should I use in my GLM?</p> <p>A common choice is to include at least the 6 Head Motion parameters, and optionally FD and Global Signal ad nuisance regressors in your GLM.  </p> <p>See this awesome NeuroStars conversation with advice on choosing regressors and relevant resources.</p> <p>For more details on interpreting fMRIPrep reports, see the fMRIPrep Outputs Documentation and discussions on NeuroStars.</p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#mriqc-html-report","title":"MRIQC HTML Report","text":"<p>The MRIQC report highlights:</p> <ul> <li>Summary Image: A visual overview of key metrics, including signal-to-noise ratio (SNR) and temporal SNR (tSNR).</li> <li>Detailed Metrics: Click through different tabs to examine metrics like Mean Framewise Displacement, EPI-to-T1w registration quality, and artifact presence.</li> </ul> <p>Interpreting tSNR</p> <p>Higher temporal SNR (tSNR) values indicate better data quality. Typical values range from 30-60 for fMRI. Low tSNR may suggest issues like excessive noise or scanner artifacts. Review the group-level metrics to identify subjects with unusually high motion or low tSNR.</p> <p>For more information on understanding these metrics, check out the MRIQC interpretation guide on NeuroStars.</p>"},{"location":"research/fmri/analysis/fmri-prepocessing-qa.html#common-issues-with-fmriprep-and-mriqc","title":"Common Issues with fMRIPrep and MRIQC","text":"Memory Errors: Out of Memory (OOM) or Crash <ul> <li>Problem: fMRIPrep crashes or terminates unexpectedly due to insufficient memory.</li> <li>Solution: Reduce the <code>--mem-mb</code> parameter to allocate less memory or increase the swap space available on your system. This can help prevent OOM errors.</li> <li>Tip: Monitor your memory usage during processing using tools like <code>htop</code> (Linux) or Activity Monitor (Mac). Aim to use around 80-90% of your available RAM without exceeding it.</li> </ul> Docker File Permissions Error <ul> <li>Problem: fMRIPrep cannot access input or output directories due to file permissions.</li> <li>Solution: Ensure that Docker has read and write permissions to the directories being mounted. Adjust permissions using:   <pre><code>chmod -R 755 /path/to/BIDS /path/to/derivatives\n</code></pre></li> <li>Tip: On Windows, ensure that Shared Drives are enabled in Docker Desktop settings.</li> </ul> Missing Fields in JSON Files <ul> <li>Problem: fMRIPrep fails due to missing <code>SliceTiming</code> or <code>PhaseEncodingDirection</code> fields in the JSON sidecar files.</li> <li>Solution: Verify that all required metadata fields are present using the BIDS Validator. For guidance on JSON sidecar fields, see the BIDS Specification.</li> <li>Tip: If using custom acquisition parameters, manually edit JSON files to include the missing fields.</li> </ul> RuntimeError: Fieldmap Issues <ul> <li>Problem: fMRIPrep throws a <code>RuntimeError</code> related to fieldmaps, such as missing or improperly specified fieldmaps.</li> <li>Solution: Ensure that fieldmaps are correctly specified in your BIDS dataset according to the BIDS Fieldmap documentation.</li> <li>Tip: If your study does not require fieldmap correction, you can skip this step by specifying <code>--ignore fieldmaps</code> in your fMRIPrep command.</li> </ul> MRIQC: NaN Values in JSON Files <ul> <li>Problem: MRIQC fails when encountering <code>NaN</code> values in JSON metadata files.</li> <li>Solution: Use a script like <code>sanitize_json.py</code> to replace <code>NaN</code> values with valid placeholders before running MRIQC.</li> <li>Tip: Validate your JSON files before running MRIQC to avoid processing interruptions.</li> </ul> Docker: Cannot Allocate Memory <ul> <li>Problem: fMRIPrep crashes with the error <code>cannot allocate memory</code> when using Docker.</li> <li>Solution: Restart the Docker service or allocate more memory and CPUs through the Docker Desktop settings under Resources.</li> <li>Tip: Increase memory allocation gradually (e.g., 2-4 GB increments) until fMRIPrep runs smoothly.</li> </ul> Slow Processing: fMRIPrep Takes Too Long <ul> <li>Problem: fMRIPrep runs slowly, taking an excessively long time for each subject.</li> <li>Solution: Use a faster SSD for the <code>--work-dir</code> to improve read/write speeds and reduce processing time. Also, ensure <code>--n-cpus</code> is set to the majority of available cores, but not all, to avoid system slowdowns.</li> <li>Tip: Consider running fMRIPrep on a high-performance computing (HPC) cluster if available.</li> </ul> Missing or Corrupted Output Files <ul> <li>Problem: After running fMRIPrep or MRIQC, certain output files (e.g., <code>sub-xx.html</code> reports) are missing or corrupted.</li> <li>Solution: Check for errors in the log files generated during the run. Often, disk space issues or interruptions during processing can cause missing files. Re-run the affected subjects with sufficient disk space.</li> <li>Tip: Use a dedicated work directory and ensure it has at least 100 GB of free space to accommodate intermediate files.</li> </ul> MRIQC: No Group Report Generated <ul> <li>Problem: Group-level analysis in MRIQC does not produce a report.</li> <li>Solution: Ensure that MRIQC was run in group mode using the correct <code>group</code> argument. Check if all individual reports are present in the output directory before running the group-level command.</li> <li>Tip: Verify that the <code>derivatives/mriqc</code> directory has read and write access for Docker.</li> </ul> fMRIPrep output: empty surf files <ul> <li> <p>Problem: Some files in <code>freesurfer/sub-xx/surf</code> are empty (0 KB), namely:  </p> <ul> <li><code>*h.fsaverage.sphere.reg</code></li> <li><code>*h.pial</code></li> <li><code>*h.white.H</code></li> <li><code>*h.white.K</code></li> </ul> <p>These files are supposed to be symbolic links pointing to other outputs in the folder. A 0 KB size indicates that the link is broken. This often happens if preprocessing was done on Windows, since Windows does not fully preserve these link-type files.</p> <p>Even if the symbolic link is broken, the files to which the links originally pointed are likely still present in your <code>surf/</code> folder, so you do not need to re-run <code>recon-all</code> or <code>fmriprep</code>. </p> </li> <li> <p>Solution: If you need any of these files, you can either use the corresponding \u201coriginal\u201d file directly, or recreate the symbolic link (or a duplicate file) so external tools can see it under the expected filename. Below are the relevant file mappings:</p> Broken (link) file Original (target) file <code>*h.fsaverage.sphere.reg</code> <code>*h.sphere.reg</code> <code>*h.pial</code> <code>*h.pial.T1</code> <code>*h.white.K</code> <code>*h.white.preaparc.K</code> <code>*h.white.H</code> <code>*h.white.preaparc.H</code> <p>For instance, if you need <code>lh.pial</code> and it\u2019s empty, you can create it by copying <code>lh.pial.T1</code> with the following command:</p> <pre><code>cp lh.pial.T1 lh.pial\n</code></pre> <p>To fix these links automatically across multiple subjects (on Windows, use the WSL terminal, not in the native PowerShell / Windows terminal)):</p> <ol> <li>Set your <code>FREESURFER_PATH</code> (the folder containing your pre-existing <code>recon-all</code> or output):      <pre><code>export FREESURFER_PATH=/BIDE/derivatives/freesurfer\n</code></pre></li> <li>Copy and paste the script below into an empty file and save it as <code>fix_surf_files.sh</code>.  </li> <li>Open a terminal and navigate to the folder where you saved the file (e.g., <code>cd ~/Documents</code>).</li> <li>Make the script executable: <code>chmod +x fix_surf_files.sh</code></li> <li>Run the script: <code>./fix_surf_files.sh</code></li> </ol> <p>Here is the full script:</p> <pre><code>#!/bin/bash\n\n# ==============================================================================\n# Fix Broken Files in FreeSurfer Directories\n#\n# This script checks for specific broken or empty files in a FreeSurfer directory.\n# If a broken file is found, it creates a symbolic link to its corresponding\n# original file.\n#\n# Usage:\n#   ./fix_freesurfer_links.sh         # Process all subjects in $FREESURFER_PATH\n#   ./fix_freesurfer_links.sh sub-01  # Process only the given subject(s)\n#\n# Requirements:\n#   - The environment variable $FREESURFER_PATH must be set and point to the \n#     directory containing the subject folders.\n#   - FreeSurfer outputs must exist for the fix to work.\n# ==============================================================================\n\n# Check if FREESURFER_PATH is set\nif [[ -z \"$FREESURFER_PATH\" ]]; then\n    echo \"\u274c Error: FREESURFER_PATH is not set. Please export FREESURFER_PATH first.\"\n    exit 1\nfi\n\n# If no subject is provided, process all subjects in the FreeSurfer directory\nif [[ $# -eq 0 ]]; then\n    SUBJS=($(ls \"$FREESURFER_PATH\"))  # Get all subjects in the directory\nelse\n    SUBJS=(\"$@\")  # Use provided subjects\nfi\n\n# Define file mappings: (broken file \u2192 target file)\ndeclare -A FILE_MAP=(\n    [\"lh.fsaverage.sphere.reg\"]=\"lh.sphere.reg\"\n    [\"rh.fsaverage.sphere.reg\"]=\"rh.sphere.reg\"\n    [\"lh.pial\"]=\"lh.pial.T1\"\n    [\"rh.pial\"]=\"rh.pial.T1\"\n    [\"lh.white.K\"]=\"lh.white.preaparc.K\"\n    [\"rh.white.K\"]=\"rh.white.preaparc.K\"\n    [\"lh.white.H\"]=\"lh.white.preaparc.H\"\n    [\"rh.white.H\"]=\"rh.white.preaparc.H\"\n)\n\n# Loop over subjects\nfor SUBJ in \"${SUBJS[@]}\"; do\n    SURF_PATH=\"${FREESURFER_PATH}/${SUBJ}/surf\"\n\n    # Check if the subject directory exists\n    if [[ ! -d \"$SURF_PATH\" ]]; then\n        echo \"\u26a0\ufe0f Warning: Subject directory not found for $SUBJ. Skipping...\"\n        continue\n    fi\n\n    # Loop over each broken file type\n    for BROKEN_FILE in \"${!FILE_MAP[@]}\"; do\n        TARGET_FILE=\"${FILE_MAP[$BROKEN_FILE]}\"\n        BROKEN_PATH=\"${SURF_PATH}/${BROKEN_FILE}\"\n        TARGET_PATH=\"${SURF_PATH}/${TARGET_FILE}\"\n\n        # Check if the broken file exists and is empty\n        if [[ -e \"$BROKEN_PATH\" &amp;&amp; ! -s \"$BROKEN_PATH\" ]]; then\n            echo \"\ud83d\udee0 Fixing $BROKEN_FILE for $SUBJ...\"\n\n            # Check if the corresponding target file exists before creating the link\n            if [[ -e \"$TARGET_PATH\" ]]; then\n                ln -sf \"$TARGET_PATH\" \"$BROKEN_PATH\"\n                echo \"\u2714 Created symbolic link: $BROKEN_FILE \u2192 $TARGET_FILE\"\n            else\n                echo \"\u26a0\ufe0f Warning: $TARGET_FILE not found for $SUBJ. Cannot create link.\"\n            fi\n        else\n            echo \"\u2705 $BROKEN_FILE for $SUBJ is fine. No action needed.\"\n        fi\n    done\n\ndone\n\necho \"\u2705 Done.\"\n</code></pre> </li> </ul> <p>With these quality checks complete, you're ready to proceed to the General Linear Model (GLM) analysis. See the next guide for instructions on setting up your GLM. \u2192 Go to GLM</p>"},{"location":"research/fmri/analysis/fmri-rois.html","title":"Regions of Interest (ROIs)","text":"<p>When conducting fMRI analyses, we often focus on specific brain regions based on theoretical or empirical questions. These Regions of Interest (ROIs) are defined areas in the brain that we hypothesize to be relevant to a particular cognitive function or task.</p> <p>By restricting the analysis to ROIs, researchers can improve statistical power, focus on hypothesized brain areas, and extrat data for multivariate pattern analyses (MVPA).</p>"},{"location":"research/fmri/analysis/fmri-rois.html#commonly-used-roi-types","title":"Commonly Used ROI Types","text":"<p>The following types of ROIs are commonly used in fMRI research:</p> <ol> <li>Anatomical ROIs: These are based on anatomical landmarks, often derived from standard brain atlases.</li> <li>Functional ROIs: Defined based on brain activation patterns observed in functional localizer tasks.</li> <li>Spherical ROIs: Spheres around specific MNI coordinates, offering a quick, automated way to generate ROIs.</li> </ol>"},{"location":"research/fmri/analysis/fmri-rois.html#example-creating-spherical-rois-with-the-gui","title":"Example: Creating Spherical ROIs with the GUI","text":""},{"location":"research/fmri/analysis/fmri-rois.html#example-creating-spherical-rois-with-a-script","title":"Example: Creating Spherical ROIs with a Script","text":"<p>Below is an example MATLAB script designed to creare bilateral ROIs. This script leverages MarsBaR and SPM to generate spherical ROIs around given MNI coordinates. The ROIs are saved as NIfTI files, which can be further used in analyses such as multivariate decoding.</p> Script for Creating Spherical ROIs in MATLAB makeROISpheres.m<pre><code>% MATLAB Script for Generating Spherical ROIs\n% ================================================\n% This script creates spherical ROIs with specified MNI coordinates, radii, and a reference image.\n% Dependencies: MarsBaR, SPM, hop_roi_sphere function.\n%\n% Author: Andrea Costantino\n% Date: October 2024\n\n% Define output paths and options\noutRoot = './chess-expertise-2024/results/test';\nopt = struct();\nopt.space = 'MNI152NLin2009cAsym_res-2'; % Coordinate space\nopt.dir.output = fullfile(outRoot, 'rois'); % Output directory for ROIs\n\n% Define the reference image path (any BOLD or beta image for this subject)\nm.referencePath = './BIDS/derivatives/fmriprep/sub-01/func/sub-01_task-exp_run-2_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz';\n\n% Define ROI parameters\nm.radii = [5, 10]; % Radii for the spherical ROIs in mm\nm.roisToCreate = struct(...\n    'area', {'FFA', 'LOC', 'PPA', 'TPJ', 'PCC1', ...\n             'CoS_PPA1', 'pMTL_OTJ', 'OTJ', 'pMTG', 'SMG1', ...\n             'CoS_PPA2', 'RSC_PCC', 'SMG2', 'pMTG_OTJ', 'Caudatus'}, ...\n    'coordsL', {[-38, -58, -14], [-44, -77, -12], [-30, -50, -10], [-56, -47, 33], [2, -30, 34], ...\n                [33, 39, 12], [-47, -69, 8], [-47, -69, 8], [-60, -54, -3], [-60, -36, 36], ...\n                [-32, -43, -11], [-10, -75, 16], [-63, -31, 33], [-35, -80, 25], [-15, 13, 11]}, ...\n    'coordsR', {[40, -55, -12], [44, -78, -13], [30, -54, -12], [56, -47, 33], [], ...\n                [30, 42, 9], [48, -69, 15], [55, -69, 14], [58, -52, 1], [63, -27, 42], ...\n                [18, -52, 5], [38, -36, -13], [], [51, -69, 16], [11, 18, 10]} ...\n);\n\n% Display ROI parameters for verification\nfprintf('Preparing to create ROIs with the following parameters:\\n');\nfor i = 1:length(m.roisToCreate)\n    currROI = m.roisToCreate(i);\n    fprintf('  ROI: %s\\n', currROI.area);\n    fprintf('    MNI coordinates (left): [%s]\\n', num2str(currROI.coordsL));\n    fprintf('    MNI coordinates (right): [%s]\\n', num2str(currROI.coordsR));\nend\n\n% Generate the ROIs\nhop_roi_sphere(opt, m);\n\n%% Helper function to create and save spherical ROIs\nfunction hop_roi_sphere(opt, m)\n    % Generates spherical ROIs and saves them as NIfTI files.\n    % Parameters:\n    %   opt - Contains output directory and coordinate space information.\n    %   m - Contains reference image path, radii, and ROI coordinates.\n\n    for radius = m.radii\n        fprintf('\\nCreating ROIs with radius: %d mm\\n', radius);\n\n        % Set output folder for current radius\n        outputFolder = fullfile(opt.dir.output, sprintf('radius_%dmm', radius));\n        if ~exist(outputFolder, 'dir'), mkdir(outputFolder); end\n\n        % Load reference space using SPM functions\n        refSPM = spm_vol(m.referencePath);\n        referenceSpace = mars_space(refSPM);\n\n        % Iterate through each ROI definition\n        for i = 1:length(m.roisToCreate)\n            currROI = m.roisToCreate(i);\n            fprintf('\\nProcessing ROI: %s\\n', currROI.area);\n\n            % Left hemisphere ROI\n            if ~isempty(currROI.coordsL)\n                create_and_save_roi(currROI.coordsL, 'L', currROI.area, radius, opt, referenceSpace, outputFolder);\n            end\n\n            % Right hemisphere ROI\n            if ~isempty(currROI.coordsR)\n                create_and_save_roi(currROI.coordsR, 'R', currROI.area, radius, opt, referenceSpace, outputFolder);\n            end\n\n            % Bilateral ROI\n            if ~isempty(currROI.coordsL) &amp;&amp; ~isempty(currROI.coordsR)\n                create_and_save_roi_bilateral(currROI.coordsL, currROI.coordsR, currROI.area, radius, opt, referenceSpace, outputFolder);\n            end\n        end\n    end\nend\n\nfunction create_and_save_roi(coords, hemi, area, radius, opt, referenceSpace, outputFolder)\n    % Creates and saves a sphere ROI for given coordinates.\n    % Parameters:\n    %   coords - MNI coordinates for the ROI center.\n    %   hemi - 'L' or 'R' indicating hemisphere.\n    %   area - Label for the ROI.\n    %   radius - Sphere radius in mm.\n    %   opt - Options struct with output and space information.\n    %   referenceSpace - MarsBaR space object for the brain reference.\n    %   outputFolder - Directory to save the NIfTI file.\n\n    roiLabel = sprintf('hemi-%s_label-%s', hemi, area);\n    fprintf('  Creating %s ROI with radius %d mm\\n', roiLabel, radius);\n\n    % Create sphere ROI with MarsBaR\n    sphereROI = maroi_sphere(struct('centre', coords, 'radius', radius, 'label', roiLabel, 'reference', referenceSpace));\n\n    % Save ROI as NIfTI file\n    filename = fullfile(outputFolder, sprintf('hemi-%s_space-%s_radius-%dmm_label-%s.nii', hemi, opt.space, radius, area));\n    save_as_image(sphereROI, filename);\n    fprintf('  Saved ROI: %s\\n', filename);\nend\n\nfunction create_and_save_roi_bilateral(coordsL, coordsR, area, radius, opt, referenceSpace, outputFolder)\n    % Creates and saves a bilateral ROI by combining left and right coordinates.\n    % Parameters are identical to create_and_save_roi, but it combines two sets of coordinates.\n\n    fprintf('  Creating bilateral ROI with radius %d mm for area %s\\n', radius, area);\n\n    % Create left and right hemisphere ROIs\n    leftROI = maroi_sphere(struct('centre', coordsL, 'radius', radius, 'label', sprintf('hemi-L_label-%s', area)));\n    rightROI = maroi_sphere(struct('centre', coordsR, 'radius', radius, 'label', sprintf('hemi-R_label-%s', area)));\n\n    % Resample and combine hemispheres\n    resampledLeftROI = maroi_matrix(leftROI, referenceSpace);\n    resampledRightROI = maroi_matrix(rightROI, referenceSpace);\n    combinedData = (struct(resampledLeftROI).dat &gt; 0.5) | (struct(resampledRightROI).dat &gt; 0.5);\n    bilateralROI = maroi_matrix(struct('dat', combinedData, 'mat', referenceSpace.mat, 'label', sprintf('hemi-B_label-%s', area)));\n\n    % Save as bilateral NIfTI file\n    filename = fullfile(outputFolder, sprintf('hemi-B_space-%s_radius-%dmm_label-%s.nii', opt.space, radius, area));\n    save_as_image(bilateralROI, filename);\n    fprintf('  Saved bilateral ROI: %s\\n', filename);\nend\n</code></pre> <p>This script will output a folder as follow:     <pre><code>rois\n\u2514\u2500\u2500 radius_5mm\n    \u251c\u2500\u2500 hemi-B_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-Caudatus.nii\n    \u251c\u2500\u2500 hemi-B_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-CoS_PPA1.nii\n    \u251c\u2500\u2500 hemi-B_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-CoS_PPA2.nii\n    \u251c\u2500\u2500 hemi-B_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-FFA.nii\n    \u251c\u2500\u2500 hemi-B_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-LOC.nii\n    \u251c\u2500\u2500 hemi-B_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-OTJ.nii\n    \u251c\u2500\u2500 hemi-B_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-pMTG.nii\n    \u251c\u2500\u2500 hemi-B_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-pMTG_OTJ.nii\n    \u251c\u2500\u2500 hemi-B_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-pMTL_OTJ.nii\n    \u251c\u2500\u2500 hemi-B_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-PPA.nii\n    \u251c\u2500\u2500 hemi-B_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-RSC_PCC.nii\n    \u251c\u2500\u2500 hemi-B_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-SMG1.nii\n    \u251c\u2500\u2500 hemi-B_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-TPJ.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-Caudatus.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-CoS_PPA1.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-CoS_PPA2.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-FFA.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-LOC.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-OTJ.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-PCC1.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-pMTG.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-pMTG_OTJ.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-pMTL_OTJ.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-PPA.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-RSC_PCC.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-SMG1.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-SMG2.nii\n    \u251c\u2500\u2500 hemi-L_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-TPJ.nii\n    \u251c\u2500\u2500 hemi-R_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-Caudatus.nii\n    \u251c\u2500\u2500 hemi-R_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-CoS_PPA1.nii\n    \u251c\u2500\u2500 hemi-R_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-CoS_PPA2.nii\n    \u251c\u2500\u2500 hemi-R_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-FFA.nii\n    \u251c\u2500\u2500 hemi-R_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-LOC.nii\n    \u251c\u2500\u2500 hemi-R_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-OTJ.nii\n    \u251c\u2500\u2500 hemi-R_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-pMTG.nii\n    \u251c\u2500\u2500 hemi-R_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-pMTG_OTJ.nii\n    \u251c\u2500\u2500 hemi-R_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-pMTL_OTJ.nii\n    \u251c\u2500\u2500 hemi-R_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-PPA.nii\n    \u251c\u2500\u2500 hemi-R_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-RSC_PCC.nii\n    \u251c\u2500\u2500 hemi-R_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-SMG1.nii\n    \u2514\u2500\u2500 hemi-R_space-MNI152NLin2009cAsym_res-2_radius-5mm_label-TPJ.nii\n</code></pre></p> <p>With right (<code>hemi-R</code>), left (<code>hemi-L</code>) and bilateral (<code>hemi-B</code>) masks.</p> <p>The size of the ROIs is defined in millimeters, and is a list of radii such as <code>m.radii = [5, 10];</code> (see line 19 of the <code>makeROISpheres.m</code> script).</p> <p>Warning</p> <p>The bilateral masks will be generated automatically only when both R and L coordinates are provided for a given ROI. If only R or L coordinates are provided, only the R or L mask will be saved.</p>"},{"location":"research/fmri/analysis/fmri-rois.html#intersecting-roi-masks-with-glm-results","title":"Intersecting ROI Masks with GLM Results","text":"<p>In our lab, we apply an additional refinement step to Region of Interest (ROI) masks to precisely target the most relevant brain voxels. This involves intersecting the mask (e.g., a spherical or anatomical ROI) with the significant activation of a specific contrast. This approach is particularly beneficial for analyses like multivariate pattern analysis (MVPA), where targeted voxel selection is crucial for decoding tasks.</p> <p>For instance, suppose we want to perform an MVPA to determine if we can distinguish between Female and Male faces in the Fusiform Face Area (FFA). Here\u2019s how we might set up the analysis:</p> <ol> <li> <p>Create an Initial ROI Mask:</p> <ul> <li>First, we create a NIfTI file with values of 1 in the FFA region and 0 elsewhere. This mask can be created either anatomically (using an atlas) or by defining a spherical mask centered on FFA coordinates.</li> </ul> </li> <li> <p>Run a First-Level Analysis to Identify Activation in the ROI:</p> <ul> <li>In SPM, perform a First-Level analysis to obtain significant activation for a relevant contrast, such as Faces vs. Objects. This will produce a t-map that indicates the t values of all voxels for the given contrast.</li> </ul> <p>What contrasts should I use?</p> <ul> <li>If we use a localizer run (i.e., a run where participants are shown categories for functional localization), then we can set up contrasts based on well-established literature. For example, for the FFA, the Faces &gt; Objects contrast is commonly used; for LOC, Objects &gt; Scrambled is typical.</li> <li>If we use the experimental task run (i.e., when participants perform the main task), we generally choose contrasts that reflect overall activity in the region. For example, we might use an All &gt; Rest contrast where all experimental conditions are positive and rest blocks are negative, capturing the regions most active during the task overall.</li> </ul> </li> <li> <p>Threshold the Activation Map </p> <ul> <li>Apply a statistical threshold to the t-map, setting a significance level (e.g., p &lt; .001) to identify the voxels significantly active for the contrast of interest.</li> </ul> </li> <li> <p>Intersect Masked and Activated Voxels </p> <ul> <li>Generate a new ROI that includes only the voxels both significantly active in the contrast and within the initial mask (e.g., sphere or anatomical region).</li> </ul> </li> <li> <p>Extract beta values from selected voxels </p> <ul> <li>We use the generated ROI to filter voxels in the beta images (the <code>beta_00*.nii</code> images in the SPM GLM output folder) for further MVPA.</li> </ul> </li> </ol> <p>This approach has two key benefits:</p> <ol> <li>Selective Targeting of Relevant Voxels: It ensures that the ROI captures only the voxels relevant for the cognitive function of interest, maximizing information and minimizing potential noise from uninformative voxels.</li> <li>Feature Reduction: Reducing the number of features (voxels) helps improve the classifier's performance by mitigating the Curse of Dimensionality (see this article on dimensionality reduction for more info).</li> </ol>"},{"location":"research/fmri/analysis/fmri-rois.html#example-generating-refined-rois-by-intersecting-masks-with-activation-maps","title":"Example: Generating Refined ROIs by Intersecting Masks with Activation Maps","text":"<p>The following MATLAB script refines ROIs by intersecting an initial mask (e.g., anatomical or spherical) with a subject-specific activation map from an SPM contrast. This approach is particularly valuable for targeted analyses, such as multivariate pattern analysis (MVPA).</p>"},{"location":"research/fmri/analysis/fmri-rois.html#script-workflow","title":"Script Workflow","text":"<ol> <li>Thresholding: Applies a statistical threshold to the contrast map, identifying only voxels with significant activation.</li> <li>Intersection: Intersects these significant voxels with the ROI mask to focus on relevant areas within the predefined region.</li> <li>Voxel Count Check: Ensures the resulting ROI contains a minimum number of significant voxels (default: 25). If the voxel count is too low, the significance threshold is incrementally relaxed until the minimum count is met.</li> <li>Saving Results: Exports the final ROI in both NIfTI (<code>.nii</code>) and MATLAB (<code>.mat</code>) formats, making it ready for further analysis.</li> </ol> <p>Finding Contrast Names in SPM</p> <p>To find the exact names of contrasts in an SPM model, load the SPM.mat file and check <code>SPM.xCon.name</code>. This allows you to confirm the contrast names required for the <code>contrastName</code> field in <code>roisStruct</code>.</p> Script to Create ROIs from an Image Mask and Statistical Activation Map intersectROIandGLM.m<pre><code>% MATLAB Script for Creating ROI Files from Activation Maps and Image Masks\n% =========================================================================\n% This script generates an ROI from an existing ROI mask and an activation contrast map.\n%\n% Dependencies: Requires MarsBaR and SPM to be installed.\n%\n% Author: Andrea Costantino\n% Date: 7 July 2023\n\nclc;\nclear;\n\n%% Set Parameters\n% Define paths for data directories, GLM results, MarsBaR, and ROI storage.\nderivativesDir = '/data/projects/chess/data/BIDS/derivatives';\nGLMroot = fullfile(derivativesDir, 'SPM/GLM/');\nmarsabPath = fullfile(derivativesDir, 'marsbar');\nroisRoot = fullfile(marsabPath, 'rois-noloc');\noutRoot = fullfile(marsabPath, 'rois+loc');\n\n%% Define ROI Details\n\n% Example ROIs: Specify ROI name, path to the image file, task, and contrast name.\nroisStruct(1).roiName = 'FFA';\nroisStruct(1).roiImgPath = fullfile(roisRoot, 'radius_10/ROI-FFA_radius-10_space-MNI_resampled-to-sub_binary.nii');\nroisStruct(1).taskName = 'exp';\nroisStruct(1).contrastName = 'All &gt; Rest';\nroisStruct(1).outFolder = 'radius_10+exp';\n\n% LOC - Objects &gt; Scrambled\nroisStruct(2).roiName = 'LOC';\nroisStruct(2).roiImgPath = fullfile(roisRoot,'radius_10/ROI-LOC_radius-10_space-MNI_resampled-to-sub_binary.nii');\nroisStruct(2).taskName = 'loc';\nroisStruct(2).contrastName = 'Objects &gt; Scrambled';\nroisStruct(2).outFolder = 'radius_10+loc';\n\n% Define here more ROIs if needed\n\n%% Select Subjects\n% Define which subjects to process. Use '*' for all subjects.\nselectedSubjectsList = '*';\nsubPaths = findSubjectsFolders(GLMroot, selectedSubjectsList);\n\n%% Initialize MarsBaR and SPM\n% Start the MarsBaR toolbox and set SPM defaults for fMRI.\nmarsbar('on');\nspm('defaults', 'fmri');\n\n%% Process Each Subject\nfor subRow = 1:length(subPaths)\n    % Subject name and preparation\n    subName = subPaths(subRow).name;\n    fprintf('\\n###### Processing %s ######\\n', subName);\n\n    for roiNum = 1:length(roisStruct)\n        % Output directory setup\n        outDir = createOutputDir(outRoot, roisStruct(roiNum).outFolder, subName);\n\n        % Ensure subject has the specified task, or skip this ROI\n        subPath = fullfile(subPaths(subRow).folder, subPaths(subRow).name);\n        subHasTask = checkSubjectHasTask(subPath, roisStruct(roiNum).taskName, subName);\n        if ~subHasTask\n            continue;\n        end\n\n        % Load ROI image data\n        [roiImg, roiStruct] = loadROIImageData(roisStruct(roiNum).roiImgPath);\n\n        % Load SPM model for the subject and task\n        GLMdir = fullfile(GLMroot, subName, roisStruct(roiNum).taskName);\n        [model, loadSuccess] = loadSPMModel(GLMdir, subName, roisStruct(roiNum).taskName);\n        if ~loadSuccess\n            continue;\n        end\n\n        % Load and threshold the contrast map (p &lt; 0.001)\n        [tConImg, tStruct] = loadTContrastImage(model, roisStruct(roiNum).contrastName, GLMdir);\n        [thresholdedImage, finalPThresh] = calculateAndApplyThreshold(tConImg, model);\n\n        % Check alignment between ROI and contrast map\n        assert(isequal(roiStruct.mat, tStruct.mat), 'Affine matrices do not match.');\n        assert(isequal(roiStruct.dim, tStruct.dim), 'Image dimensions do not match.');\n\n        % Intersect thresholded T map and ROI mask\n        finalRoi = createIntersectedROI(thresholdedImage, roiImg, tStruct, roisStruct(roiNum).roiName, roisStruct(roiNum).contrastName);\n\n        % Save final ROI to file\n        saveROI(finalRoi, roisStruct(roiNum).roiImgPath, finalPThresh, roisStruct(roiNum).taskName, roisStruct(roiNum).contrastName, outDir);\n    end\n\n    % Save a copy of the script for replicability\n    saveScriptForReplicability(outDir);\nend\n\n%% Helper functions\nfunction saveScriptForReplicability(outDir)\n    % Copies the currently running script to the specified output directory for replicability.\n    %\n    % This function identifies the full path of the currently executing script and copies it\n    % to the given output directory. This process aids in ensuring that analyses can be\n    % replicated or reviewed in the future with the exact code version used.\n    %\n    % Parameters:\n    %   outDir: The directory where the script should be copied for future reference.\n    %\n    % Example usage:\n    %   saveScriptForReplicability('/path/to/output/dir');\n\n    fprintf('Saving current script for replicability...\\n');\n\n    % Get the full path of the currently executing script\n    fileNameAndLocation = mfilename('fullpath');\n\n    % Extract the directory, file name, and extension of the current script\n    [path, filename, ~] = fileparts(fileNameAndLocation);\n\n    % Construct the output file name and location\n    outputFileNameAndLocation = fullfile(outDir, strcat(filename, '.m'));\n\n    % Define the current script's file path\n    currentfile = strcat(fileNameAndLocation, '.m');\n\n    % Copy the script file to the output directory\n    try\n        copyfile(currentfile, outputFileNameAndLocation);\n        fprintf('Script copied to output folder: %s\\n', outputFileNameAndLocation);\n    catch ME\n        warning('Failed to copy script to output folder: %s', ME.message);\n    end\nend\n\nfunction saveROI(finalRoi, roiImgPath, pThresh, taskName, contrastName, outDir)\n    % Saves the ROI to the specified output directory in both NIfTI and .mat formats.\n    %\n    % This function constructs the output file name using the base ROI name, task name,\n    % a simplified contrast name, and the p-value threshold used. It saves the ROI object\n    % as an image and as a MATLAB file, logging each step.\n    %\n    % Parameters:\n    %   finalRoi: The ROI object to be saved.\n    %   roiImgPath: The file path of the original ROI, used to extract the base file name.\n    %   pThresh: The p-value threshold used, for inclusion in the file name.\n    %   taskName: The name of the task associated with the ROI.\n    %   contrastName: The original contrast name, to be simplified for file naming.\n    %   outDir: The directory where the ROI files should be saved.\n    %\n    % Example usage:\n    %   saveROI(finalRoi, '/path/to/original/roi.nii', 0.001, 'task1', 'Contrast 1', '/path/to/output/dir');\n\n    fprintf('STEP: Saving ROI...\\n');\n\n    % Extract the base file name of the original ROI for use in the output file names\n    [~, roiFileName, ~] = fileparts(roiImgPath);\n\n    % Simplify the contrast name for use in the file name\n    contrastNameSimple = simplifyContrastName(contrastName);\n\n    % Prepare the p-value string for inclusion in the file name\n    pThreshStringSplit = split(string(pThresh), '.');\n    pThreshString = pThreshStringSplit(2); % Extracting decimal part for file naming\n\n    % Construct the output file name\n    outFileName = strcat(roiFileName, '_task-', taskName, '_contrast-', contrastNameSimple, '_p', pThreshString);\n\n    % Define full paths for the NIfTI and .mat files\n    niiFilePath = char(fullfile(outDir, strcat(outFileName, '.nii')));\n    matFilePath = char(fullfile(outDir, strcat(outFileName, '.mat')));\n\n    % Save the ROI as an image file\n    try\n        save_as_image(finalRoi, niiFilePath);\n        fprintf('ROI image saved to: %s\\n', niiFilePath);\n    catch ME\n        warning('Failed to save ROI image: %s', ME.message);\n    end\n\n    % Save the ROI as a MATLAB .mat file\n    try\n        saveroi(finalRoi, matFilePath);\n        fprintf('ROI MATLAB file saved to: %s\\n', matFilePath);\n    catch ME\n        warning('Failed to save ROI MATLAB file: %s', ME.message);\n    end\nend\n\nfunction finalRoi = createIntersectedROI(thresholdedImage, roiImg, tStruct, roiName, contrastName)\n    % Creates a new ROI object from the intersection of a thresholded T image and an ROI image.\n    %\n    % This function performs a logical AND operation between a binary thresholded T image\n    % and a binary ROI image, marking voxels included in both images. It then creates a new\n    % ROI object with a label derived from the ROI name and a simplified contrast name.\n    %\n    % Parameters:\n    %   thresholdedImage: Binary image where voxels above a certain threshold are marked as 1.\n    %   roiImg: The ROI image data.\n    %   tStruct: The structure returned by spm_vol, containing the T image's affine matrix.\n    %   roiName: The name of the ROI.\n    %   contrastName: The name of the contrast used for thresholding.\n    %\n    % Returns:\n    %   finalRoi: The newly created ROI object.\n    %\n    % Example usage:\n    %   finalRoi = createIntersectedROI(thresholdedTImg, roiImgData, tImageStruct, 'ROI_Name', 'Contrast_Name');\n\n    fprintf('STEP: Intersecting thresholded T image with ROI...\\n');\n\n    % Perform intersection by applying a logical AND operation\n    intersectedImage = thresholdedImage &amp; (roiImg &gt; 0.5);\n\n    % Optionally, count the number of voxels in the intersection for reporting or further analysis\n    numIntersectedVoxels = sum(intersectedImage(:));\n    fprintf('Number of intersected voxels: %d\\n', numIntersectedVoxels);\n\n    fprintf('STEP: Creating new ROI from intersected data...\\n');\n\n    % Simplify the contrast name for labeling the ROI\n    contrastNameSimple = simplifyContrastName(contrastName);\n\n    % Create the new ROI object with specified parameters\n    finalRoi = maroi_matrix(struct('dat', intersectedImage, 'mat', tStruct.mat, 'label', strcat([roiName, ' ', contrastNameSimple]), 'binarize', 1, 'roithresh', 1e-10));\n\n    fprintf('DONE: New ROI created successfully.\\n');\nend\n\nfunction contrastNameSimple = simplifyContrastName(contrastName)\n    % Simplifies the contrast name by removing non-alphabetic characters and parentheses.\n    %\n    % Parameters:\n    %   contrastName: The original contrast name as a string.\n    %\n    % Returns:\n    %   contrastNameSimple: The simplified contrast name, with only lowercase alphabetic characters.\n    %\n    % Example usage:\n    %   contrastNameSimple = simplifyContrastName('Contrast 1 (Session 1)');\n\n    contrastNameSimple = regexprep(lower(contrastName), '[^a-z]+|\\([^)]*\\)', '');\nend\n\nfunction hasTask = checkSubjectHasTask(subPath, taskName, subName)\n    % Checks if the specified subject directory contains files related to the specified task.\n    %\n    % Parameters:\n    %   subPath: The full path to the subject's directory.\n    %   taskName: The name of the task to check for.\n    %   subName: The name of the subject being checked.\n    %\n    % Returns:\n    %   hasTask: A boolean indicating whether the task is present for the subject.\n    %\n    % Example usage:\n    %   hasTask = checkSubjectHasTask('/path/to/subject/directory', 'task_name', 'subject_name');\n    %\n    % This function searches the subject's directory for files containing the task name,\n    % and provides a warning if the task is not found, suggesting the iteration should be skipped.\n\n    fprintf('Checking for task %s in subject %s directory...\\n', taskName, subName);\n\n    % Construct the full path and list files\n    files = dir(subPath);\n    fileNames = {files.name};\n\n    % Check for the presence of the task name in any file names\n    hasTask = any(contains(fileNames, taskName));\n\n    if ~hasTask\n        % If the task is not found, issue a warning\n        warning('Task %s not found for %s in %s. Skipping...', taskName, subName, subPath);\n    else\n        fprintf('Task %s found for subject %s.\\n', taskName, subName);\n    end\nend\n\nfunction [tConImg, tStruct] = loadTContrastImage(model, contrastName, GLMdir)\n    % Loads T contrast image voxel data for a given contrast name and GLM directory.\n    %\n    % Parameters:\n    %   model: The loaded SPM model object for the current subject and task.\n    %   contrastName: The name of the contrast to load.\n    %   GLMdir: Directory containing the GLM results for the subject.\n    %\n    % Returns:\n    %   tConImg: The voxel data of the T contrast image.\n    %   tStruct: The structure of the T contrast image.\n    %\n    % Example usage:\n    %   [tConImg, tStruct] = loadTContrastImage(model, 'contrastName', '/path/to/GLMdir');\n    %\n    % This function first attempts to find the specified contrast within the model.\n    % If not found, it appends ' - All Sessions' to the name and retries.\n    % It then checks for the existence of the T contrast image file and loads its voxel data.\n\n    try\n        fprintf('Attempting to load T contrast image for contrast: %s\\n', contrastName);\n\n        % Attempt to find the specified contrast by name\n        t_con = get_contrast_by_name(model, contrastName);\n        if isempty(t_con)\n            % If not found, try appending ' - All Sessions' and search again\n            fprintf('Contrast %s not found, trying with \" - All Sessions\" suffix.\\n', contrastName);\n            contrastName = strcat(contrastName, ' - All Sessions');\n            t_con = get_contrast_by_name(model, contrastName);\n            if isempty(t_con)\n                error('ContrastNotFound', 'Cannot find the contrast %s in the design; has it been estimated?', contrastName);\n            end\n        end\n\n        % Construct the full path to the T contrast image file\n        tConFname = fullfile(GLMdir, t_con.Vspm.fname);\n\n        % Verify the existence of the T contrast image file\n        if ~exist(tConFname, 'file')\n            error('FileNotFound', 'Cannot find T image %s; has it been estimated?', tConFname);\n        else\n            fprintf('T contrast image found: %s\\n', tConFname);\n        end\n\n        % Load the voxel data from the T contrast image\n        fprintf('Loading voxel data from T contrast image...\\n');\n        tStruct = spm_vol(tConFname);\n        tConImg = spm_read_vols(tStruct);\n        fprintf('Voxel data loaded successfully.\\n');\n\n    catch ME\n        switch ME.identifier\n            case 'ContrastNotFound'\n                fprintf('Error: %s\\n', ME.message);\n            case 'FileNotFound'\n                fprintf('Error: %s\\n', ME.message);\n            otherwise\n                fprintf('An unexpected error occurred: %s\\n', ME.message);\n        end\n        tConImg = []; % Return empty in case of error\n        tConFname = '';\n    end\nend\n\nfunction [model, loadSuccess] = loadSPMModel(GLMdir, subName, taskName)\n    % Attempts to load the SPM model for a given subject and task.\n    %\n    % Parameters:\n    %   GLMdir: The directory containing the GLM results for the subject.\n    %   subName: The name of the current subject being processed.\n    %   taskName: The name of the task for which the model is being loaded.\n    %\n    % Returns:\n    %   model: The loaded SPM model object, or empty if loading failed.\n    %   loadSuccess: A boolean indicating whether the model was successfully loaded.\n    %\n    % Example usage:\n    %   [model, loadSuccess] = loadSPMModel('/path/to/GLMdir', 'subject1', 'task1');\n    %\n    % This function tries to load the SPM.mat file and handles any errors that occur,\n    % logging appropriate messages and returning a status indicator.\n\n    fprintf('Attempting to load SPM model for subject %s, task %s...\\n', subName, taskName);\n    model = []; % Initialize model as empty\n    loadSuccess = false; % Initialize success status as false\n\n    try\n        % Construct the path to the SPM.mat file\n        spmPath = fullfile(GLMdir, 'SPM.mat');\n\n        % Attempt to load the SPM model\n        model = mardo(spmPath);\n\n        % If successful, set the success status to true\n        loadSuccess = true;\n        fprintf('SPM model loaded successfully.\\n');\n\n    catch ME\n        % Handle errors that occur during model loading\n        fprintf('WARNING: Error loading SPM model for %s, task %s: %s\\n', subName, taskName, ME.message);\n        fprintf('Skipping to the next iteration.\\n');\n        % No need to set model or loadSuccess as they are already initialized to their failure states\n    end\n\n    % Return the model (empty if failed) and the success status\n    return;\nend\n\nfunction [roiImg, roiStruct] = loadROIImageData(roiImgPath)\n    % Loads the ROI image and its voxel data from a specified path.\n    %\n    % Parameters:\n    %   roiImgPath: The file path to the ROI image.\n    %\n    % Returns:\n    %   roiImg: The voxel data of the ROI image.\n    %   roiStruct: The structure returned by spm_vol, containing image volume information.\n    %\n    % Example usage:\n    %   [roiImg, roiStruct] = loadROIImageData('/path/to/roi/image.nii');\n    %\n    % This function logs the process of loading ROI data, loads the ROI image,\n    % and reads its voxel data, ensuring to handle any errors gracefully.\n\n    fprintf('Loading ROI data from: %s\\n', roiImgPath);\n\n    try\n        % Load the ROI image structure\n        roiStruct = spm_vol(roiImgPath);\n\n        % Read the voxel data from the ROI image\n        roiImg = spm_read_vols(roiStruct);\n\n        fprintf('ROI data successfully loaded.\\n');\n    catch ME\n        error('Failed to load ROI data: %s', ME.message);\n    end\nend\n\nfunction outDir = createOutputDir(outRoot, outFolder, subName)\n    % Creates an output directory for a given subject and ROI if it doesn't already exist.\n    %\n    % Parameters:\n    %   outRoot: The root directory under which all outputs are saved.\n    %   outFolder: The folder name specific to the ROI or processing step.\n    %   subName: The name of the subject being processed.\n    %\n    % Example usage:\n    %   createOutputDir('/path/to/output/root', 'roi_specific_folder', 'subject_name');\n    %\n    % This function constructs the output directory path, checks if it exists,\n    % and creates it if not, with logging at each step for clarity.\n\n    % Construct the full path to the output directory\n    outDir = fullfile(outRoot, outFolder, subName);\n\n    % Check if the output directory already exists\n    if ~exist(outDir, 'dir')\n        fprintf('Creating output folder: %s\\n', outDir);\n        % Attempt to create the directory\n        [mkdirSuccess, msg, msgID] = mkdir(outDir);\n        if mkdirSuccess\n            fprintf('Output folder created successfully: %s\\n', outDir);\n        else\n            error('Failed to create output folder: %s\\nMessage ID: %s\\n%s', outDir, msgID, msg);\n        end\n    else\n        fprintf('Output folder already exists: %s. No action needed.\\n', outDir);\n    end\nend\n\nfunction [thresholdedImage, finalPThresh] = calculateAndApplyThreshold(tConImg, model, initialPThresh, adjustmentPThreshs, minVoxels)\n    % Calculates and applies a threshold to T images based on uncorrected p-values,\n    % adjusting the threshold if necessary to meet a minimum voxel count.\n    %\n    % Parameters:\n    %   tConImg: The voxel data of the T contrast image.\n    %   model: The loaded SPM model object for obtaining error degrees of freedom.\n    %   initialPThresh (optional): The initial p-value threshold. Default is 0.001.\n    %   adjustmentPThreshs (optional): Array of p-value thresholds to try if initial thresholding is insufficient. Default is [0.01, 0.05].\n    %   minVoxels (optional): Minimum number of voxels required after thresholding. Default is 25.\n    %\n    % Returns:\n    %   thresholdedImage: The T image after applying the final threshold.\n    %   finalPThresh: The final p-value threshold used.\n\n    % Set default values if not provided\n    if nargin &lt; 3 || isempty(initialPThresh)\n        initialPThresh = 0.001;\n    end\n    if nargin &lt; 4 || isempty(adjustmentPThreshs)\n        adjustmentPThreshs = [0.01, 0.05];\n    end\n    if nargin &lt; 5 || isempty(minVoxels)\n        minVoxels = 25;\n    end\n\n    erdf = error_df(model);\n    finalPThresh = initialPThresh;\n    i = 1; % Start with the first adjustment threshold if needed\n\n    % Initially apply threshold\n    [thresholdedImage, numCells] = applyThreshold(tConImg, finalPThresh, erdf);\n\n    % Adjust threshold if necessary\n    while numCells &lt; minVoxels &amp;&amp; i &lt;= length(adjustmentPThreshs)\n        finalPThresh = adjustmentPThreshs(i);\n        [thresholdedImage, numCells] = applyThreshold(tConImg, finalPThresh, erdf);\n        i = i + 1; % Move to the next threshold for adjustment\n    end\n\n    if numCells &lt; minVoxels\n        warning('Final voxel count below %d even after adjusting thresholds. Final count: %d', minVoxels, numCells);\n    else\n        fprintf('Thresholding complete. Final p-value threshold: %f, Voxel count: %d\\n', finalPThresh, numCells);\n    end\nend\n\nfunction [thresholdedImage, numCells] = applyThreshold(tConImg, pThresh, erdf)\n    % Helper function to apply threshold and count voxels\n    tThresh = spm_invTcdf(1-pThresh, erdf); % Calculate T threshold\n    thresholdedImage = tConImg &gt; tThresh; % Apply threshold\n    numCells = sum(thresholdedImage(:)); % Count voxels above threshold\nend\n\nfunction [filteredFolderStructure] = findSubjectsFolders(fmriprepRoot, selectedSubjectsList, excludedSubjectsList)\n% FINDSUBJECTSFOLDERS Locate subject folders based on a list or wildcard.\n%\n% USAGE:\n% sub_paths = findSubjectsFolders(fmriprepRoot, selectedSubjectsList)\n%\n% INPUTS:\n% fmriprepRoot          - The root directory where 'sub-*' folders are located.\n%\n% selectedSubjectsList  - Can be one of two things:\n%                         1) A list of integers, each representing a subject ID.\n%                            For example, [7,9] would search for folders 'sub-07' \n%                            and 'sub-09' respectively.\n%                         2) A single character string '*'. In this case, the function\n%                            will return all folders starting with 'sub-*'.\n%\n% OUTPUTS:\n% sub_paths             - A structure array corresponding to the found directories.\n%                         Each structure has fields: 'name', 'folder', 'date', \n%                         'bytes', 'isdir', and 'datenum'.\n%\n% EXAMPLES:\n% 1) To fetch directories for specific subjects:\n%    sub_paths = findSubjectsFolders('/path/to/fmriprepRoot', [7,9]);\n%\n% 2) To fetch all directories starting with 'sub-*':\n%    sub_paths = findSubjectsFolders('/path/to/fmriprepRoot', '*');\n%\n% note:\n% If a subject ID from the list does not match any directory, a warning is issued.\n\n% Start by fetching all directories with the 'sub-*' pattern.\nsub_paths = dir(fullfile(fmriprepRoot, 'sub-*'));\nsub_paths = sub_paths([sub_paths.isdir]); % Keep only directories.\n\n% Check the type of selectedSubjectsList\nif isnumeric(selectedSubjectsList(1))\n    % Case 1: selectedSubjectsList is a list of integers.\n\n    % Convert each integer in the list to a string of the form 'sub-XX'.\n    subIDs = cellfun(@(x) sprintf('sub-%02d', x), num2cell(selectedSubjectsList), 'UniformOutput', false);\n\n    % Filter the sub_paths to keep only those directories matching the subIDs.\n    sub_paths = sub_paths(ismember({sub_paths.name}, subIDs));\n\n    % Check and throw warnings for any missing subID.\n    foundSubIDs = {sub_paths.name};\n    for i = 1:length(subIDs)\n        if ~ismember(subIDs{i}, foundSubIDs)\n            warning(['The subID ', subIDs{i}, ' was not found in sub_paths.name.']);\n        end\n    end\n\nelseif ischar(selectedSubjectsList) &amp;&amp; strcmp(selectedSubjectsList, '*')\n    % Case 2: selectedSubjectsList is '*'. \n    % No further action required as we've already selected all 'sub-*' folders.\n\nelse\n    % Invalid input.\n    error('Invalid format for selectedSubjects. It should be either \"*\" or a list of integers.');\nend\n\n% Only process exclusion if the excludedSubjectsList is provided.\nif nargin == 3\n    % Create a list of excluded folder names\n    excludedNames = cellfun(@(x) sprintf('sub-%02d', x), num2cell(excludedSubjectsList), 'UniformOutput', false);\n\n    % Logical array of folders to exclude\n    excludeMask = arrayfun(@(x) ismember(x.name, excludedNames), sub_paths);\n\n    % Filtered structure\n    filteredFolderStructure = sub_paths(~excludeMask);\nelse\n    % If no excludedSubjectsList is provided, just return the sub_paths.\n    filteredFolderStructure = sub_paths;\nend\nend\n</code></pre> <p>This code generates a new folder containing subject-specific ROIs. While previous spherical or anatomical masks were likely generic\u2014created in a standard space (e.g., MNI) and therefore applicable to any subject within that space \u2014 these new ROIs are specific for each subject, since they are created by intersecting the generic mask with the subject's unique pattern of activation from the GLM t-map.</p> <p>Now that you have your beta images (from the GLM) and your ROIs, you have everything you need to run your multi-variate analysis. \u2192 MVPA</p> <p>https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/atlases.html#id4 https://neurosynth.org/ https://openneuro.org/</p>"},{"location":"research/fmri/analysis/fmri-setup-env.html","title":"Setting up your fMRI analysis environment","text":"<p>Welcome to the fMRI analysis environment setup guide. This walkthrough will help you install and configure all necessary tools for our fMRI analysis workflow across Windows, macOS, and Linux platforms.</p> <p>Suggested System Specifications</p> <ul> <li>Operating System: Windows 10/11, macOS 10.14+, or Linux (Ubuntu 18.04+)</li> <li>RAM: 16GB (32GB+ recommended)</li> <li>Storage: 200GB+ free space (SSD preferred)</li> <li>CPU: Multi-core processor (4+ cores)</li> <li>GPU: NVIDIA GPU with CUDA support (optional, but beneficial)</li> </ul>"},{"location":"research/fmri/analysis/fmri-setup-env.html#folder-structure","title":"Folder Structure","text":"<p>Our lab uses a specific folder structure for fMRI projects. Here's an overview:</p> <pre><code>Project_Name/\n\u251c\u2500\u2500 sourcedata/\n\u2502   \u2514\u2500\u2500 sub-xx/\n\u2502       \u251c\u2500\u2500 dicom/\n\u2502       \u251c\u2500\u2500 dicom_anon/\n\u2502       \u251c\u2500\u2500 bh/\n\u2502       \u251c\u2500\u2500 et/\n\u2502       \u2514\u2500\u2500 nifti/\n\u251c\u2500\u2500 BIDS/\n\u2502   \u251c\u2500\u2500 derivatives/\n\u2502   \u2502   \u251c\u2500\u2500 deepmreye/\n\u2502   \u2502   \u251c\u2500\u2500 fastsurfer/\n\u2502   \u2502   \u251c\u2500\u2500 fmriprep/\n\u2502   \u2502   \u251c\u2500\u2500 fmriprep-mriqc/\n\u2502   \u2502   \u251c\u2500\u2500 fmriprep-spm/\n\u2502   \u2502   \u251c\u2500\u2500 fmriprep-spm-cosmomvpa/\n\u2502   \u2502   \u2514\u2500\u2500 rois/\n\u2502   \u2514\u2500\u2500 sub-xx/\n\u2502       \u251c\u2500\u2500 anat/\n\u2502       \u2514\u2500\u2500 func/\n\u251c\u2500\u2500 code/\n\u2502   \u251c\u2500\u2500 misc/\n\u2502   \u2514\u2500\u2500 utils/\n\u2514\u2500\u2500 temp/\n    \u251c\u2500\u2500 temp_fmriprep/\n    \u251c\u2500\u2500 temp_spm/\n    \u251c\u2500\u2500 temp_deepmreye/\n    \u2514\u2500\u2500 temp_mriqc/\n</code></pre> <ul> <li><code>sourcedata/</code>: Contains raw data for each subject</li> <li><code>BIDS/</code>: Organized according to BIDS specification</li> <li><code>derivatives/</code>: Stores processed data</li> <li><code>code/</code>: Contains analysis scripts and utilities</li> <li><code>temp/</code>: Temporary directories for various processing steps</li> </ul> <p>Create the folder structure</p> <p>To create this folder structure, you can use the following bash script:</p> create_fmri_structure.sh<pre><code># Create main project directory\nmkdir -p Project_Name\n\n# Navigate to the project directory\ncd Project_Name\n\n# Create sourcedata structure\nmkdir -p sourcedata/sub-xx/{dicom,dicom_anon,nifti,bh,et}\n\n# Create BIDS structure\nmkdir -p BIDS/sub-xx/{anat,func}\nmkdir -p BIDS/derivatives/{deepmreye,fastsurfer,fmriprep,fmriprep-spm,fmriprep-spm-cosmomvpa,fmriprep-mriqc,rois}\n\n# Create code structure\nmkdir -p code/{misc,utils}\n\n# Create temp structure\nmkdir -p temp/{temp_fmriprep,temp_spm,temp_deepmreye,temp_mriqc}\n\necho \"Folder structure created successfully!\"\n</code></pre> <p>Save this script as <code>create_fmri_structure.sh</code> and run it using:</p> <pre><code>bash create_fmri_structure.sh\n</code></pre>"},{"location":"research/fmri/analysis/fmri-setup-env.html#installing-core-tools","title":"Installing core tools","text":""},{"location":"research/fmri/analysis/fmri-setup-env.html#docker-desktop","title":"Docker Desktop","text":"<p>Docker is crucial for running containerized applications like fMRIPrep. Althoug Docker can be installed as a command-line tool, we strongly advise installing the GUI version (Docker Desktop).</p> <p>For up-to-date installation info, please consult the Docker Desktop installation pages for Mac, Windows or Linux.</p> <p>After installation, configure Docker resources:</p> <ol> <li>Open Docker Desktop settings</li> <li>Go to \"Resources\" section</li> <li>Allocate resources:<ul> <li>CPUs: Set to total CPUs - 2 (e.g., if you have 8 cores, set to 6)</li> <li>Memory: Set to 80% of total RAM (e.g., if you have 32GB, set to 25GB)</li> <li>Disk image size: Set to a reasonable amount (e.g., 100GB)</li> </ul> </li> <li>In the \"File sharing\" or \"Resources &gt; File sharing\" section, add your project folder (e.g., <code>~/fMRI_Projects</code>)</li> </ol>"},{"location":"research/fmri/analysis/fmri-setup-env.html#installing-docker-tools","title":"Installing Docker tools","text":"<p>We use several tools via Docker for our fMRI analysis pipeline. Docker installations are strongly encouraged over \"bare metal\" setups for several reasons:</p> <ol> <li>Docker containers come bundled with all necessary dependencies, ensuring compatibility across different systems.</li> <li>They provide a consistent environment, reducing \"it works on my machine\" issues.</li> <li>Docker simplifies the installation process and manages complex software interactions.</li> </ol> <p>Many of these tools are BIDS-apps, which are container images designed to work with BIDS-formatted datasets. BIDS (Brain Imaging Data Structure) is a standard for organizing and describing neuroimaging datasets. BIDS Apps have consistent command-line arguments, making them easy to run and integrate into automated platforms.</p> <p>Important</p> <p>Before running any BIDS-app, ensure your input folder is correctly structured according to BIDS standards. Validate your BIDS dataset using the BIDS Validator to avoid potential issues.</p> <p>For detailed installation and usage instructions, please refer to each tool's respective documentation. For examples of how to run these tools using Docker, refer to the usage notes in their respective documentation or check our fMRI workflow example in this same folder.</p> <p>Below are the basic Docker pull commands for the main tools we use:</p> <ul> <li> <p>fMRIprep:</p> <p>fMRIPrep is a tool for minimal pre-processing of structural and anatomical MRI images.</p> <p>To get the Docker image:</p> <pre><code>python -m pip install fmriprep-docker\n</code></pre> </li> <li> <p>MRIQC:</p> <p>MRIQC is a tool to perform Quality Check on your raw and pre-processed MRI images.</p> <p>To get the Docker image:</p> <pre><code>docker pull nipreps/mriqc:latest\n</code></pre> </li> <li> <p>FastSurfer:</p> <p>FastSurfer is a self-contained, faster (it uses the NVIDIA GPU processing) alternative to FreeSurfer. It can save quite some time when performing surface processing pipelines (e.g., <code>recon-all</code>).</p> <p>To get the Docker image:</p> <pre><code>docker pull deepmi/fastsurfer:latest\n</code></pre> <p>Note</p> <p>FastSurfer can save you time if you have a CUDA-compatible GPU. In short, this means that your machine should have a dedicated NVIDIA GPU with CUDA installed. You can check whether CUDA is correctly installed on you machine by typing <code>nvidia-smi</code> on your terminal. If this command does not return a list of active GPUs, you either need to install and configure CUDA, or you can avoid installing this tool and rely on the <code>recon-all</code> pipeline performed with the anatomical workflow of fMRIPrep.</p> </li> <li> <p>DeepMReye:</p> <p>DeepMReye is a tool to perform eye-tracking data analysis when you have no eye-tracking data. It estimates eye-movements from the eyes position in your functional images. This will of course results in a (very) much lower temporal resolution than real eye-tracking data, but we found results to be good enough for some experimental paradigms.</p> <p>To get the Docker image:</p> <pre><code>docker pull deepmreye/deepmreye\n</code></pre> </li> </ul>"},{"location":"research/fmri/analysis/fmri-setup-env.html#dcm2niix","title":"dcm2niix","text":"<p>dcm2niix is a powerful tool used for DICOM to NIfTI conversion. It can be used as a command-line tool or through a Graphical User Interface (GUI) when shipped with MRIcroGL (see this for more information).</p> <p>There are several ways to install dcm2niix, depending on your operating system and preferences:</p> WindowsmacOSLinux <ol> <li> <p>Download pre-compiled executable: <pre><code>curl -fLO https://github.com/rordenlab/dcm2niix/releases/latest/download/dcm2niix_win.zip\n</code></pre>    Unzip the file and add the executable to your system PATH:    <pre><code>$env:Path += \";C:\\path\\to\\dcm2niix\"\n</code></pre>    Replace <code>C:\\path\\to\\dcm2niix</code> with the actual path where you unzipped dcm2niix.</p> </li> <li> <p>Install with Conda: <pre><code>conda install -c conda-forge dcm2niix\n</code></pre></p> </li> <li> <p>Install with pip: <pre><code>python -m pip install dcm2niix\n</code></pre></p> </li> <li> <p>Download MRIcroGL: Download MRIcroGL which includes dcm2niix with a GUI.</p> </li> </ol> <ol> <li> <p>Download pre-compiled package: <pre><code>curl -fLO https://github.com/rordenlab/dcm2niix/releases/latest/download/macos_dcm2niix.pkg\n</code></pre>    Open the downloaded package to install.</p> </li> <li> <p>Install with Homebrew: <pre><code>brew install dcm2niix\n</code></pre></p> </li> <li> <p>Install with MacPorts: <pre><code>sudo port install dcm2niix\n</code></pre></p> </li> <li> <p>Install with Conda: <pre><code>conda install -c conda-forge dcm2niix\n</code></pre></p> </li> <li> <p>Install with pip: <pre><code>python -m pip install dcm2niix\n</code></pre></p> </li> </ol> <ol> <li> <p>Download pre-compiled executable: <pre><code>curl -fLO https://github.com/rordenlab/dcm2niix/releases/latest/download/dcm2niix_lnx.zip\n</code></pre>    Unzip the file and add the executable to your system PATH:    <pre><code>echo 'export PATH=$PATH:/path/to/dcm2niix' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>    Replace <code>/path/to/dcm2niix</code> with the actual path where you unzipped dcm2niix.</p> </li> <li> <p>Install on Debian-based systems: <pre><code>sudo apt-get install dcm2niix\n</code></pre></p> </li> <li> <p>Install with Conda: <pre><code>conda install -c conda-forge dcm2niix\n</code></pre></p> </li> <li> <p>Install with pip: <pre><code>python -m pip install dcm2niix\n</code></pre></p> </li> </ol> Older Linux versions compatibility <p>The pre-compiled Linux executable requires a recent version of Linux (e.g., Ubuntu 14.04 or later) with Glibc 2.19 (from 2014) or later. Users of older systems can compile their own copy of dcm2niix or download the compiled version included with MRIcroGL, which is compatible with Glibc 2.12 (from 2011).</p>"},{"location":"research/fmri/analysis/fmri-setup-env.html#python-and-conda","title":"Python and Conda","text":"<p>We use Conda to manage our Python environment.</p> <ol> <li>Install Miniconda</li> <li> <p>Create and activate the environment:</p> <pre><code>conda create -n fmri_env python=3.9 spyder numpy scipy matplotlib nibabel nilearn scikit-learn\n</code></pre> </li> <li> <p>Activate the environment:</p> <pre><code>conda activate fmri_env\n</code></pre> </li> </ol> <p>Warning</p> <p>It's crucial to create a new conda environment for each new project you start. Installing new packages into the base conda environment is a very bad practice that will eventually lead to a bloated, brittle environment with broken packages and compatibility issues. Uninstalling or re-installing Python on some machines can be a very painful (sometimes impossible) process!</p>"},{"location":"research/fmri/analysis/fmri-setup-env.html#setting-up-spyder-ide","title":"Setting up Spyder IDE","text":"<ol> <li> <p>Launch Spyder:</p> <pre><code>conda activate fmri_env\nspyder\n</code></pre> </li> <li> <p>Create a new project:</p> <ul> <li>Go to \"Projects\" &gt; \"New Project\"</li> <li>Choose \"Existing directory\"</li> <li>Select your project folder (e.g., <code>~/fMRI_Projects/Project_Name</code>)</li> <li>Name your project and click \"Create\"</li> </ul> </li> </ol>"},{"location":"research/fmri/analysis/fmri-setup-env.html#matlab","title":"MATLAB","text":"<p>For MATLAB installation and licensing, please refer to the Installing MATLAB section in our computer setup guide.</p> <p>Install the following MATLAB toolboxes:</p> <ul> <li> <p>SPM:</p> <p>SPM (Statistical Parametric Mapping) is used for GLM analysis.</p> <ol> <li>Download SPM12</li> <li>Unzip to a location of your choice</li> <li>Add SPM to MATLAB path:</li> </ol> <pre><code>addpath('path/to/spm12')\nsavepath\n</code></pre> <p>Mac installtion</p> <p>For mac users, potential installation issues can be tackled with the instructions for mac on the SPM wiki. Make sure Xcode is installed on your computer before installing SPM.</p> </li> <li> <p>CoSMoMVPA</p> <p>CoSMoMVPA is used for multivariate pattern analysis.</p> <ol> <li>Download from the official website</li> <li>Add to MATLAB path:</li> </ol> <pre><code>addpath(genpath('path/to/CoSMoMVPA'))\nsavepath\n</code></pre> </li> <li> <p>MarsBaR:</p> <p>MarsBaR is a region of interest toolbox for SPM.</p> <ol> <li>Download MarsBaR</li> <li>Unzip to a location of your choice, such as <code>/home/myhome/marsbar-0.42/</code></li> <li> <p>Copy the MarsBaR distribution into the SPM directory with:</p> <pre><code>mkdir /path-to-spm/toolbox/marsbar\ncp -r /home/myhome/marsbar-0.42/* /path-to-spm/toolbox/marsbar\n</code></pre> </li> </ol> <p>Change <code>/path-to-spm/</code> with your SPM path (e.g., <code>/usr/local/spm/spm12/</code>).</p> <p>The next time you start spm you should be able to start the toolbox by selecting \u2018marsbar\u2019 from the toolbox button on the SPM interface.  </p> </li> </ul>"},{"location":"research/fmri/analysis/fmri-setup-env.html#installing-additional-tools","title":"Installing additional tools","text":"<p>These tools are not mandatory -- they can be installed if needed.</p>"},{"location":"research/fmri/analysis/fmri-setup-env.html#freesurfer","title":"FreeSurfer","text":"<p>FreeSurfer is used for cortical surface reconstruction. The main surface reconstruction pipeline of FreeSurfer (<code>recon-all</code>) is bundled in the fmriprep docker image, and it is performed during the fmriprep anatomical workflow. This means that this tool is not strictly necessary, unless you plan on running additional surface processing steps (e.g., additional surface projections, such as the Glasser volumetric projection from fsaverage that is performed in the fMRI workflow example.</p> <p>To install:</p> <ol> <li>Download from the official website</li> <li>Set up environment variables:</li> </ol> <pre><code>export FREESURFER_HOME=/path/to/freesurfer\nsource $FREESURFER_HOME/SetUpFreeSurfer.sh\n</code></pre> FreeSurfer on Windows <p>FreeSurfer is not natively compatible with Windows. To use FreeSurfer on a Windows system, you have a few options:</p> <ol> <li> <p>Use Windows Subsystem for Linux (WSL):</p> <ul> <li>Install WSL 2 on your Windows machine</li> <li>Install a Linux distribution like Ubuntu through WSL</li> <li>Install FreeSurfer within the Linux environment</li> </ul> </li> <li> <p>Use a virtual machine:</p> <ul> <li>Install virtualization software like VirtualBox or VMware</li> <li>Set up a Linux virtual machine </li> <li>Install FreeSurfer in the Linux VM</li> </ul> </li> <li> <p>Use a Docker container:</p> <ul> <li>Install Docker Desktop for Windows</li> <li>Pull and run a FreeSurfer Docker image</li> </ul> </li> <li> <p>Remote access:</p> <ul> <li>Use a remote Linux server or cluster with FreeSurfer installed</li> <li>Connect via SSH or remote desktop</li> </ul> </li> </ol> <p>The WSL or Docker options are generally recommended as they have less overhead than a full VM. Whichever method you choose, ensure you have adequate disk space and RAM allocated for FreeSurfer to run efficiently.</p>"},{"location":"research/fmri/analysis/fmri-setup-env.html#ants","title":"ANTs","text":"<p>ANTs is used for image registration and normalization. As for FreeSurfer, this tool is not strictly necessary, unless you want to generate the Glasser volumetric projection from fsaverage described here</p> <ol> <li>Download from GitHub</li> <li>Add to system PATH:</li> </ol> <pre><code>export ANTSPATH=/path/to/ANTs/bin\nexport PATH=$ANTSPATH:$PATH\n</code></pre>"},{"location":"research/fmri/analysis/fmri-setup-env.html#common-issues","title":"Common Issues","text":"Docker: WSL2 Configuration Errors on Windows <p>Problem: Docker fails to start or displays errors related to WSL2.</p> <p>Solution: Ensure WSL2 is properly installed and configured. Open Docker Desktop settings and verify that WSL2 is selected as the backend. Restart Docker Desktop after making changes. If issues persist, run the following command in PowerShell: <pre><code>wsl --update\n</code></pre></p> Docker: Service Issues on Linux <p>Problem: Docker service fails to start or stops unexpectedly on Linux systems.</p> <p>Solution: Restart the Docker service and check the logs for more details: <pre><code>sudo systemctl restart docker\nsudo journalctl -u docker.service\n</code></pre> Ensure Docker is set to start on boot using: <pre><code>sudo systemctl enable docker\n</code></pre></p> MATLAB: Not Recognized in PATH <p>Problem: MATLAB is not found in the system PATH, leading to command not found errors.</p> <p>Solution: Add the MATLAB installation directory to your system PATH. For a temporary fix, run: <pre><code>export PATH=$PATH:/path/to/matlab/bin\n</code></pre> To make this change permanent, add the above line to your <code>~/.bashrc</code> or <code>~/.zshrc</code> file and restart the terminal.</p> SPM: Missing Toolboxes <p>Problem: Errors occur due to missing SPM toolboxes in MATLAB.</p> <p>Solution: Ensure the required toolboxes (e.g., SPM, CoSMoMVPA, MarsBaR) are installed and added to the MATLAB path. Use the following in MATLAB: <pre><code>addpath('path/to/spm12')\nsavepath\n</code></pre></p> FreeSurfer: License Not Found <p>Problem: FreeSurfer cannot locate the <code>license.txt</code> file, leading to startup errors.</p> <p>Solution: Place the <code>license.txt</code> file in the FreeSurfer home directory and set the path correctly: <pre><code>export FS_LICENSE=/path/to/license.txt\n</code></pre> Add this line to your <code>~/.bashrc</code> or <code>~/.zshrc</code> file to ensure the license path is set on each terminal start.</p> Python: Package Conflicts <p>Problem: Conflicting package versions cause Python environments to break.</p> <p>Solution: Create a new conda environment for each project to avoid conflicts. Use: <pre><code>conda create -n new_env python=3.9\nconda activate new_env\n</code></pre> For existing environments, try resolving conflicts by specifying package versions during installation: <pre><code>conda install package_name=version\n</code></pre></p> FreeSurfer: Incompatible with Native Windows <p>Problem: FreeSurfer is not compatible with Windows and cannot be installed directly.</p> <p>Solution: Use one of these methods: - WSL2: Install WSL2 and a Linux distribution like Ubuntu, then install FreeSurfer in this environment. - Virtual Machine: Use VirtualBox or VMware to set up a Linux virtual machine, then install FreeSurfer. - Docker: Install Docker Desktop for Windows and run a FreeSurfer Docker image for compatibility.</p> <p>For more specific issues, consult tool documentation or seek help on NeuroStars.</p> <p>Remember, setting up an fMRI analysis environment can be complex. Take your time, and don't hesitate to ask for help when needed. Good luck with your research!</p> <p>Now you\u2019re ready to proceed convert your data into BIDS format. See the next guide for instructions on setting up your BIDS folder. \u2192 BIDS conversion</p>"}]}